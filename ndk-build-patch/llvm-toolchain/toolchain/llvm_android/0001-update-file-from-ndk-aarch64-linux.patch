From dce6f707f9b23e94d4504adf50c8a2d9f72f2132 Mon Sep 17 00:00:00 2001
From: pngcui <pngcui1224@163.com>
Date: Fri, 19 Sep 2025 17:34:11 +0800
Subject: [PATCH 1/2] update file from ndk-aarch64-linux

Change-Id: Ic1dcbac8fb1c32224c715ebc7334b7e0649ee498
---
 MODULE_LICENSE_APACHE2                        |    0
 MODULE_LICENSE_BSD_LIKE                       |    0
 MODULE_LICENSE_MIT                            |    0
 OWNERS                                        |    3 +-
 PREUPLOAD.cfg                                 |    4 +-
 README.md                                     |  136 ++-
 __init__.py                                   |    0
 android_llvm_next_flags.go                    |    0
 android_version.py                            |    8 +-
 base_builders.py                              |  214 ++--
 bisect_build.py                               |  145 ---
 bisect_driver.py                              |    0
 builder_registry.py                           |    0
 builders.py                                   |  686 +++--------
 cherrypick_cl.py                              |  180 +--
 cherrypick_cl_hook.py                         |   38 -
 configs.py                                    |  189 +--
 constants.py                                  |    4 +-
 do_build.py                                   |  550 ++++-----
 do_kythe_xref.py                              |   19 +-
 do_test_compiler.py                           |  102 +-
 docker/Dockerfile                             |   14 +-
 docker/README                                 |   13 +-
 docker/prod_env.sh                            |    4 +-
 docker/requirements.txt                       | 1013 ++++++++---------
 docs/README.md                                |    5 -
 docs/TODO.md                                  |    7 -
 docs/compiler-errors.md                       |   59 -
 docs/toolchain-errors.md                      |   18 -
 fetch_kokoro_prebuilts.py                     |  218 ----
 hosts.py                                      |   22 -
 kernel-boot-tests/.gitignore                  |    0
 kokoro/aosp-master-plus-llvm.cfg              |    7 +
 kokoro/aosp-master-plus-llvm_build.sh         |   33 +-
 kokoro/aosp_arm64.cfg                         |   10 -
 kokoro/aosp_riscv64.cfg                       |   10 -
 kokoro/aosp_x86_64.cfg                        |   10 -
 kokoro/common.cfg                             |   24 +-
 kokoro/darwin.cfg                             |   18 +
 kokoro/linux-tot.cfg                          |    7 +
 kokoro/linux.cfg                              |    7 +
 kokoro/llvm_build.sh                          |   17 +-
 kokoro/windows.cfg                            |   15 +
 kythe_vnames.json                             |    0
 orderfiles/README.md                          |  150 ---
 orderfiles/scripts/README.md                  |  127 ---
 orderfiles/scripts/create_orderfile.py        |  128 ---
 orderfiles/scripts/merge_orderfile.py         |  408 -------
 orderfiles/scripts/orderfile_unittest.py      |  763 -------------
 orderfiles/scripts/orderfile_utils.py         |  103 --
 orderfiles/scripts/validate_orderfile.py      |  128 ---
 orderfiles/test/allowlistv.txt                |    1 -
 orderfiles/test/denylist.txt                  |    1 -
 orderfiles/test/example-mapping.txt           |    6 -
 orderfiles/test/example.orderfile             |    5 -
 orderfiles/test/example.prof                  |    6 -
 orderfiles/test/merge-test/1.orderfile        |    4 -
 orderfiles/test/merge-test/2.orderfile        |    3 -
 orderfiles/test/merge-test/3.orderfile        |    3 -
 orderfiles/test/merge-test/4.orderfile        |    2 -
 orderfiles/test/merge-test/5.orderfile        |    2 -
 orderfiles/test/merge-test/6.orderfile        |    4 -
 orderfiles/test/merge-test/merge.txt          |    4 -
 orderfiles/test/partial.txt                   |    2 -
 orderfiles/test/partial_bad.txt               |    2 -
 ...ride-containing-.-in-executable-name.patch |   14 -
 paths.py                                      |   62 +-
 py3_utils.py                                  |   11 +-
 pylintrc                                      |    0
 remove-prebuilts.py                           |   12 -
 source_manager.py                             |  118 +-
 test/scripts/ab_client.py                     |    0
 test/scripts/cluster_info.yaml                |    0
 test/scripts/data.py                          |    0
 test/scripts/forrest.py                       |    0
 test/scripts/gerrit.py                        |    0
 test/scripts/test_configs.yaml                |    0
 test/scripts/test_paths.py                    |    0
 timer.py                                      |    7 +-
 toolchain_errors.py                           |   41 -
 toolchains.py                                 |    6 +-
 trim_patch_data.py                            |   46 +-
 update-clang-stable.py                        |    4 +-
 update-prebuilts.py                           |  139 +--
 ...toolchain.py => update_kernel_toolchain.py |    0
 utils.py                                      |   82 +-
 version.py                                    |    0
 win_sdk.py                                    |    0
 88 files changed, 1394 insertions(+), 4809 deletions(-)
 mode change 100644 => 100755 MODULE_LICENSE_APACHE2
 mode change 100644 => 100755 MODULE_LICENSE_BSD_LIKE
 mode change 100644 => 100755 MODULE_LICENSE_MIT
 mode change 100644 => 100755 OWNERS
 mode change 100644 => 100755 PREUPLOAD.cfg
 mode change 100644 => 100755 README.md
 mode change 100644 => 100755 __init__.py
 mode change 100644 => 100755 android_llvm_next_flags.go
 mode change 100644 => 100755 android_version.py
 mode change 100644 => 100755 base_builders.py
 delete mode 100755 bisect_build.py
 mode change 100644 => 100755 bisect_driver.py
 mode change 100644 => 100755 builder_registry.py
 mode change 100644 => 100755 builders.py
 delete mode 100755 cherrypick_cl_hook.py
 mode change 100644 => 100755 configs.py
 mode change 100644 => 100755 constants.py
 mode change 100644 => 100755 do_kythe_xref.py
 mode change 100644 => 100755 docker/Dockerfile
 mode change 100644 => 100755 docker/README
 mode change 100644 => 100755 docker/requirements.txt
 delete mode 100644 docs/README.md
 delete mode 100644 docs/TODO.md
 delete mode 100644 docs/compiler-errors.md
 delete mode 100644 docs/toolchain-errors.md
 delete mode 100755 fetch_kokoro_prebuilts.py
 mode change 100644 => 100755 hosts.py
 mode change 100644 => 100755 kernel-boot-tests/.gitignore
 create mode 100755 kokoro/aosp-master-plus-llvm.cfg
 delete mode 100644 kokoro/aosp_arm64.cfg
 delete mode 100644 kokoro/aosp_riscv64.cfg
 delete mode 100644 kokoro/aosp_x86_64.cfg
 mode change 100644 => 100755 kokoro/common.cfg
 create mode 100755 kokoro/darwin.cfg
 mode change 100644 => 100755 kokoro/linux-tot.cfg
 mode change 100644 => 100755 kokoro/linux.cfg
 create mode 100755 kokoro/windows.cfg
 mode change 100644 => 100755 kythe_vnames.json
 delete mode 100644 orderfiles/README.md
 delete mode 100644 orderfiles/scripts/README.md
 delete mode 100644 orderfiles/scripts/create_orderfile.py
 delete mode 100644 orderfiles/scripts/merge_orderfile.py
 delete mode 100644 orderfiles/scripts/orderfile_unittest.py
 delete mode 100644 orderfiles/scripts/orderfile_utils.py
 delete mode 100644 orderfiles/scripts/validate_orderfile.py
 delete mode 100644 orderfiles/test/allowlistv.txt
 delete mode 100644 orderfiles/test/denylist.txt
 delete mode 100644 orderfiles/test/example-mapping.txt
 delete mode 100644 orderfiles/test/example.orderfile
 delete mode 100644 orderfiles/test/example.prof
 delete mode 100644 orderfiles/test/merge-test/1.orderfile
 delete mode 100644 orderfiles/test/merge-test/2.orderfile
 delete mode 100644 orderfiles/test/merge-test/3.orderfile
 delete mode 100644 orderfiles/test/merge-test/4.orderfile
 delete mode 100644 orderfiles/test/merge-test/5.orderfile
 delete mode 100644 orderfiles/test/merge-test/6.orderfile
 delete mode 100644 orderfiles/test/merge-test/merge.txt
 delete mode 100644 orderfiles/test/partial.txt
 delete mode 100644 orderfiles/test/partial_bad.txt
 mode change 100644 => 100755 paths.py
 mode change 100644 => 100755 py3_utils.py
 mode change 100644 => 100755 pylintrc
 mode change 100644 => 100755 source_manager.py
 mode change 100644 => 100755 test/scripts/ab_client.py
 mode change 100644 => 100755 test/scripts/cluster_info.yaml
 mode change 100644 => 100755 test/scripts/data.py
 mode change 100644 => 100755 test/scripts/forrest.py
 mode change 100644 => 100755 test/scripts/gerrit.py
 mode change 100644 => 100755 test/scripts/test_configs.yaml
 mode change 100644 => 100755 test/scripts/test_paths.py
 mode change 100644 => 100755 timer.py
 delete mode 100644 toolchain_errors.py
 mode change 100644 => 100755 toolchains.py
 rename update-kernel-toolchain.py => update_kernel_toolchain.py (100%)
 mode change 100644 => 100755 utils.py
 mode change 100644 => 100755 version.py
 mode change 100644 => 100755 win_sdk.py

diff --git a/MODULE_LICENSE_APACHE2 b/MODULE_LICENSE_APACHE2
old mode 100644
new mode 100755
diff --git a/MODULE_LICENSE_BSD_LIKE b/MODULE_LICENSE_BSD_LIKE
old mode 100644
new mode 100755
diff --git a/MODULE_LICENSE_MIT b/MODULE_LICENSE_MIT
old mode 100644
new mode 100755
diff --git a/OWNERS b/OWNERS
old mode 100644
new mode 100755
index 86e3dd5..f53cc7d
--- a/OWNERS
+++ b/OWNERS
@@ -2,8 +2,9 @@
 # ../clang ../clang-tools-extra ../compiler-rt ../libcxx
 # ../libcxxabi ../lld ../llvm ../openmp_llvm
 appujee@google.com
+chh@google.com
+ndesaulniers@google.com
 pirama@google.com
 srhines@google.com
 yabinc@google.com
 yikong@google.com
-zijunzhao@google.com
diff --git a/PREUPLOAD.cfg b/PREUPLOAD.cfg
old mode 100644
new mode 100755
index 3e51044..734a94d
--- a/PREUPLOAD.cfg
+++ b/PREUPLOAD.cfg
@@ -1,2 +1,2 @@
-[Hook Scripts]
-cherrypick_cl_hook = ${REPO_ROOT}/toolchain/llvm_android/cherrypick_cl_hook.py ${PREUPLOAD_COMMIT_MESSAGE} ${PREUPLOAD_FILES}
+[Builtin Hooks]
+pylint = true
diff --git a/README.md b/README.md
old mode 100644
new mode 100755
index a17806b..73e9307
--- a/README.md
+++ b/README.md
@@ -1,11 +1,101 @@
+Ndk for aarch64 linux
+============================
+
+a modified python script to build ndk for aarch64-linux-gnu 
+with system library
+
+>This project is still in develop,
+building steps may occur unexpected errors.
+
+Usage and Build
+------------------
+### Step 0: Prepare two empy dir.
+````sh
+ export llvm-toolchain=path/to/empty_a
+ export aarch64ndk=path/to/empty_b
+````
+
+### Step 1: Download source code
+1. Download ndk source code.
+````sh
+ cd ${llvm-toolchain} 
+ repo init -u https://android.googlesource.com/platform/manifest -b llvm-toolchain
+ repo sync -c
+````
+2. Clone this repository.
+````sh
+ cd ${aarch64ndk}
+ git clone https://github.com/SnowNF/ndk-aarch64-linux
+````
+### Step 2: Replace source code
+````sh
+ cd ${llvm-toolchain}/toolchain/llvm_android
+ mv patches ../patches #backup patches.
+ rm -rf *
+ cp -rf ${aarch64ndk}/ndk-aarch64-linux/* . #replace this repository's code
+ mv ../patchss patches #restore patches
+````
+### Step 3: Try to build
+>To build the core clang binary,
+you need to install essential packages and try several times.
+>>for Debian based
+>>````sh
+>> sudo apt install clang bison llvm llvm-dev python3 lld ninja-build cmake crossbuild-essential-arm64
+>>````
+try to build
+````sh
+ cd ${llvm-toolchain} 
+ python3 toolchain/llvm_android/build.py --no-build windows --skip-tests --single-stage --no-musl
+````
+>To not always copy llvm source,you can try
+>````sh
+> cd ${llvm-toolchain} 
+> python3 toolchain/llvm_android/build.py --no-build windows --skip-tests --single-stage --no-musl --skip-source-setup 
+>````
+
+if building end with
+````
+/usr/include/limits.h:26:10: fatal error: 'bits/libc-header-start.h' file not found
+#include <bits/libc-header-start.h>
+         ^~~~~~~~~~~~~~~~~~~~~~~~~~
+1 error generated.
+````
+This is mean that the clang core binary are built.
+
+### Step 4: Replace android-ndk-rXX-linux binary files
+```sh
+ export NDK=/path/to/amd64/linux/ndk
+ cp -fr ${llvm-toolchain}/out/stage2/bin/clang* ${NDK}/toolchains/llvm/prebuilt/linux-x86_64/bin/
+ cp -fr ${llvm-toolchain}/out/stage2/lib/llvm-* ${NDK}/toolchains/llvm/prebuilt/linux-x86_64/bin/
+ cp -fr ${llvm-toolchain}/out/stage2/lib/lld* ${NDK}/toolchains/llvm/prebuilt/linux-x86_64/bin/
+ cp -fr ${llvm-toolchain}/out/stage2/lib/clang ${NDK}/toolchains/llvm/prebuilt/linux-x86_64/lib/
+ cp -fr /usr/bin/make ${NDK}/prebuilt/linux-x86_64/bin/
+ cp -fr /usr/bin/yasm ${NDK}/prebuilt/linux-x86_64/bin/
+ cp -fr /usr/bin/ytasm ${NDK}/prebuilt/linux-x86_64/bin/
+```
+### Step 5: Test and enjoy
+```shell
+ $ ./aarch64-linux-android33-clang test.c
+ $ file ./a.out
+ ./a.out: ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /system/bin/linker64, not stripped
+```
+
+#
+#
+#
+#
+#
+# The Original Android ReadMe
+
+
 Android Clang/LLVM Toolchain
 ============================
 
 For the latest version of this doc, please make sure to visit:
-[Android Clang/LLVM Toolchain Readme Doc](https://android.googlesource.com/toolchain/llvm_android/+/main/README.md)
+[Android Clang/LLVM Toolchain Readme Doc](https://android.googlesource.com/toolchain/llvm_android/+/master/README.md)
 
 You can also visit the
-[Android Clang/LLVM Prebuilts Readme Doc](https://android.googlesource.com/platform/prebuilts/clang/host/linux-x86/+/main/README.md)
+[Android Clang/LLVM Prebuilts Readme Doc](https://android.googlesource.com/platform/prebuilts/clang/host/linux-x86/+/master/README.md)
 for more information about our prebuilt toolchains (and what versions they are based upon).
 
 Build Instructions
@@ -20,38 +110,13 @@ $ python toolchain/llvm_android/build.py
 
 The built toolchain will be installed to `out/install/$HOST/clang-dev`.
 
-#### MLGO
-
-The Android LLVM team has an internal docker image with dependencies required to
-build LLVM with MLGO.  External users can create a docker image with this configuration from
-scratch:
+If building on Linux, pass `--no-build windows` to `build.py` to skip
+building Clang for Windows.
 
-```
-# Googlers
-$ toolchain/llvm_android/docker/prod_env.sh
-# Other users
-$ toolchain/llvm_android/docker/test_env.sh
-
-# Build toolchain with mlgo
-$ toolchain/llvm_android/build.py --mlgo
-```
-
-> The default option is `--no-mlgo`, which builds the toolchain without MLGO.
-> Building Android with this toolchain will fail with `Requested regalloc
-> eviction advisor analysis could not be created.`.  Use
-> `m THINLTO_USE_MLGO=false` to bypass this error.
-
-#### Convenience Options
-
-Some common options to `build.py`.  Use `build.py --help` for an updated list
-of convenience options:
-- `--no-build windows` skips building clang for Windows (relevant when building
-on Linux).
-- `--skip-tests` skips running tests in the LLVM projects.
-- `--lto` to build with ThinLTO.  LTO is enabled in official Android Clang/LLVM
-prebuilts but the flag is off by default.  This option only affects the second
-stage Clang toolchain and not the on-device targets (compiler-rt, lldb-server etc).
-- `--no-mlgo` to disable MLGO support (see [MLGO](#MLGO)).
+Pass the `--lto` option to `build.py` to build the toolchain with LTO.  LTO is
+enabled in official Android Clang/LLVM prebuilts but the flag is off by default.
+This option only affects the second stage Clang toolchain and not the on-device
+targets (compiler-rt, lldb-server etc).
 
 If you have an additional llvm tree built and present in your `$PATH`, then
 `build.py` might fail during the Windows build of libcxxabi with the error
@@ -89,9 +154,6 @@ $ python toolchain/llvm_android/build.py
 Compiler Update Steps
 ---------------------
 
-This section is out-of-date. The source of truth is moved to
-http://go/android-llvm-update-process
-
 ### Step 1: Update source code
 
 1. Download source code.
@@ -235,7 +297,7 @@ by RBE.
 ### Step 7: Switch to the new compiler
 
 All places need to switch to the new compiler are listed in
-https://android.googlesource.com/platform/prebuilts/clang/host/linux-x86/+/main/README.md.
+https://android.googlesource.com/platform/prebuilts/clang/host/linux-x86/+/master/README.md.
 
 The updates in the kernel and NDK are done separately.
 
diff --git a/__init__.py b/__init__.py
old mode 100644
new mode 100755
diff --git a/android_llvm_next_flags.go b/android_llvm_next_flags.go
old mode 100644
new mode 100755
diff --git a/android_version.py b/android_version.py
old mode 100644
new mode 100755
index edb0cb0..e01ea21
--- a/android_version.py
+++ b/android_version.py
@@ -19,9 +19,9 @@ import re
 _llvm_next = False
 _version_read = False
 
-_patch_level = '1'
-_svn_revision = 'r522817'
-_git_sha = '3c92011b600bdf70424e2547594dd461fe411a41'
+_patch_level = '0'
+_svn_revision = 'r487747'
+_git_sha = 'c4c5e79dd4b4c78eee7cffd9b0d7394b5bedcf12'
 
 # Psudo revision for top of trunk LLVM.
 _svn_revision_next = 'r99999999'
@@ -49,7 +49,7 @@ def get_svn_revision():
 def get_git_sha():
     _version_read = True
     if _llvm_next:
-        return "refs/for/main"
+        return "refs/for/master"
     return _git_sha
 
 
diff --git a/base_builders.py b/base_builders.py
old mode 100644
new mode 100755
index ee99c4c..5f835e0
--- a/base_builders.py
+++ b/base_builders.py
@@ -262,11 +262,9 @@ class AutoconfBuilder(Builder):
         cflags = super().cflags
         cflags.append('-fPIC')
         cflags.append('-Wno-unused-command-line-argument')
-        if self._config.sysroot:
-            cflags.append(f'--sysroot={self._config.sysroot}')
         if self._config.target_os.is_darwin:
             sdk_path = self._get_mac_sdk_path()
-            cflags.append(f'-mmacos-version-min={constants.MAC_MIN_VERSION}')
+            cflags.append(f'-mmacosx-version-min={constants.MAC_MIN_VERSION}')
             cflags.append(f'-DMACOSX_DEPLOYMENT_TARGET={constants.MAC_MIN_VERSION}')
             cflags.append(f'-isysroot{sdk_path}')
             cflags.append(f'-Wl,-syslibroot,{sdk_path}')
@@ -275,7 +273,6 @@ class AutoconfBuilder(Builder):
     @property
     def cxxflags(self) -> List[str]:
         cxxflags = super().cxxflags
-        cxxflags.append('-stdlib=libc++')
         return cxxflags
 
     @property
@@ -371,10 +368,6 @@ class CMakeBuilder(Builder):
         cflags = self._config.cflags + self.cflags
         cxxflags = self._config.cxxflags + self.cxxflags
         ldflags = self._config.ldflags + self.ldflags
-        if self._config.sysroot:
-            cflags.append(f'--sysroot={self._config.sysroot}')
-            cxxflags.append(f'--sysroot={self._config.sysroot}')
-            ldflags.append(f'--sysroot={self._config.sysroot}')
         cflags_str = ' '.join(cflags)
         cxxflags_str = ' '.join(cxxflags)
         ldflags_str = ' '.join(ldflags)
@@ -402,7 +395,6 @@ class CMakeBuilder(Builder):
 
             'CMAKE_BUILD_TYPE': 'Release',
             'CMAKE_INSTALL_PREFIX': str(self.install_dir),
-            'CMAKE_INSTALL_LIBDIR': 'lib',
 
             'CMAKE_MAKE_PROGRAM': str(paths.NINJA_BIN_PATH),
 
@@ -416,14 +408,12 @@ class CMakeBuilder(Builder):
         linker = self._config.get_linker(self.toolchain)
         if linker:
             defines['CMAKE_LINKER'] = str(linker)
-        if self._config.sysroot:
-            defines['CMAKE_SYSROOT'] = str(self._config.sysroot)
         if self._config.target_os == hosts.Host.Android:
             defines['ANDROID'] = '1'
             # Inhibit all of CMake's own NDK handling code.
             defines['CMAKE_SYSTEM_VERSION'] = '1'
         if self._config.target_os.is_darwin:
-            # This will be used to set -mmacos-version-min. And helps to choose SDK.
+            # This will be used to set -mmacosx-version-min. And helps to choose SDK.
             # To specify a SDK, set CMAKE_OSX_SYSROOT or SDKROOT environment variable.
             defines['CMAKE_OSX_DEPLOYMENT_TARGET'] = constants.MAC_MIN_VERSION
             # Build universal binaries.
@@ -483,9 +473,7 @@ class CMakeBuilder(Builder):
 
         env = self.env
         utils.create_script(self.output_dir / 'cmake_invocation.sh', cmake_cmd, env)
-
-        with timer.Timer(f'cmake_{self.name}_{self._config}'):
-          utils.check_call(cmake_cmd, cwd=self.output_dir, env=env)
+        utils.check_call(cmake_cmd, cwd=self.output_dir, env=env)
 
         self._ninja(self.ninja_targets)
         self.install_config()
@@ -512,7 +500,7 @@ class LLVMBaseBuilder(CMakeBuilder):  # pylint: disable=abstract-method
 
         # https://github.com/android-ndk/ndk/issues/574 - Don't depend on libtinfo.
         defines['LLVM_ENABLE_TERMINFO'] = 'OFF'
-        defines['LLVM_ENABLE_PLUGINS'] = 'OFF'
+        defines['LLVM_ENABLE_THREADS'] = 'ON'
         if patch_level := android_version.get_patch_level():
             defines['LLVM_VERSION_PATCH'] = patch_level
         defines['LLVM_VERSION_SUFFIX'] = ""
@@ -522,7 +510,6 @@ class LLVMBaseBuilder(CMakeBuilder):  # pylint: disable=abstract-method
 
         # To prevent cmake from checking libstdcxx version.
         defines['LLVM_ENABLE_LIBCXX'] = 'ON'
-        defines['LLVM_STATIC_LINK_CXX_STDLIB'] = 'ON'
 
         if self._config.target_os.is_darwin:
             defines['LLVM_USE_LINKER'] = 'ld'
@@ -532,21 +519,8 @@ class LLVMBaseBuilder(CMakeBuilder):  # pylint: disable=abstract-method
         # Building llvm with tests needs python >= 3.6, which may not be available on build server.
         # So always use prebuilts python.
         target = self._config.target_os
-        if target != hosts.Host.Android and target != hosts.Host.Baremetal:
-            defines['Python3_LIBRARY'] = str(paths.get_python_lib(target))
-            defines['Python3_LIBRARIES'] = str(paths.get_python_lib(target))
-            defines['Python3_INCLUDE_DIR'] = str(paths.get_python_include_dir(target))
-            defines['Python3_INCLUDE_DIRS'] = str(paths.get_python_include_dir(target))
-        defines['Python3_EXECUTABLE'] = str(paths.get_python_executable(hosts.build_host()))
-
         return defines
 
-    @property
-    def cflags(self) -> List[str]:
-        # TODO: Remove this once the platform libc++ is updated past LLVM 15.
-        # http://b/175635923
-        return super().cflags + ["-D_LIBCPP_AVAILABILITY_HAS_NO_VERBOSE_ABORT=1"]
-
 
 class LLVMRuntimeBuilder(LLVMBaseBuilder):  # pylint: disable=abstract-method
     """Base builder for llvm runtime libs."""
@@ -563,7 +537,8 @@ class LLVMRuntimeBuilder(LLVMBaseBuilder):  # pylint: disable=abstract-method
     @property
     def cmake_defines(self) -> Dict[str, str]:
         defines: Dict[str, str] = super().cmake_defines
-        defines['LLVM_CMAKE_DIR'] = str(self.toolchain.path)
+        defines['LLVM_CONFIG_PATH'] = str(self.toolchain.path /
+                                          'bin' / 'llvm-config')
         if self._config.target_os.is_android:
             # ANDROID_PLATFORM_LEVEL is checked when enabling TSAN for Android.
             # It's usually set by the NDK's CMake toolchain file, which we don't
@@ -583,9 +558,10 @@ class LLVMBuilder(LLVMBaseBuilder):
     enable_assertions: bool = False
     enable_mlgo: bool = False
     toolchain_name: str
+    use_sccache: bool = False
     libzstd: Optional[LibInfo] = None
     runtimes_triples: List[str] = list()
-    build_cross_runtimes: bool = False
+    build_32bit_runtimes: bool = False
 
     # lldb options.
     build_lldb: bool = True
@@ -652,17 +628,16 @@ class LLVMBuilder(LLVMBaseBuilder):
         else:
             defines['LLDB_ENABLE_LIBXML2'] = 'OFF'
 
-        if self.libncurses:
-            defines['LLDB_ENABLE_CURSES'] = 'ON'
-            defines['CURSES_INCLUDE_DIRS'] = ';'.join([
-                str(self.libncurses.include_dir),
-                str(self.libncurses.include_dir / 'ncurses'),
-            ])
-            curses_libs = ';'.join(str(lib) for lib in self.libncurses.link_libraries)
-            defines['CURSES_LIBRARIES'] = curses_libs
-            defines['PANEL_LIBRARIES'] = curses_libs
+        defines['LLDB_ENABLE_CURSES'] = 'ON'
+
+        if self.libzstd:
+            defines['LLVM_ENABLE_ZSTD'] = 'FORCE_ON'
+            defines['LLVM_USE_STATIC_ZSTD'] = 'ON'
+            defines['zstd_LIBRARY'] = self.libzstd.link_libraries[0]
+            defines['zstd_STATIC_LIBRARY'] = self.libzstd.link_libraries[1]
+            defines['zstd_INCLUDE_DIR'] = self.libzstd.include_dir
         else:
-            defines['LLDB_ENABLE_CURSES'] = 'OFF'
+            defines['LLVM_ENABLE_ZSTD'] = 'OFF'
 
         defines['LLDB_INCLUDE_TESTS'] = 'OFF'
 
@@ -684,13 +659,6 @@ class LLVMBuilder(LLVMBaseBuilder):
             shutil.copy2(self._config.sysroot / 'lib' / 'libc_musl.so', lib_dir / 'libc_musl.so')
 
     def _setup_install_dir(self) -> None:
-        if self.swig_executable:
-            python_prebuilt_dir = paths.get_python_dir(self._config.target_os)
-            python_dest_dir = self.install_dir / 'python3'
-            shutil.copytree(python_prebuilt_dir, python_dest_dir, symlinks=True, dirs_exist_ok=True,
-                            ignore=shutil.ignore_patterns('*.pyc', '__pycache__', 'Android.bp',
-                                                          '.git', '.gitignore'))
-
         lib_dir = self.install_dir / ('bin' if self._config.target_os.is_windows else 'lib')
         lib_dir.mkdir(exist_ok=True, parents=True)
         self._install_lib_deps(lib_dir)
@@ -709,13 +677,15 @@ class LLVMBuilder(LLVMBaseBuilder):
 
             self._install_lib_deps(lib_dir, bin_dir)
 
-    def ldflags_for_runtime(self, config: configs.Config) -> List[str]:
-        raise NotImplementedError()
-
     @property
     def cmake_defines(self) -> Dict[str, str]:
         defines = super().cmake_defines
 
+        if self.use_sccache:
+            defines['CMAKE_C_COMPILER_LAUNCHER'] = 'sccache'
+            defines['CMAKE_CXX_COMPILER_LAUNCHER'] = 'sccache'
+            defines['LLVM_PARALLEL_COMPILE_JOBS'] = int(multiprocessing.cpu_count()) * 10
+
         defines['LLVM_ENABLE_PROJECTS'] = ';'.join(sorted(self.llvm_projects))
         defines['LLVM_ENABLE_RUNTIMES'] = ';'.join(sorted(self.llvm_runtime_projects))
 
@@ -735,7 +705,6 @@ class LLVMBuilder(LLVMBaseBuilder):
 
         defines['LLVM_BUILD_RUNTIME'] = 'ON'
 
-        # Disable LLVM :: Bindings/Go/go.test
         defines['LLVM_INCLUDE_GO_TESTS'] = 'OFF'
 
         if self._config.target_os.is_darwin:
@@ -749,29 +718,12 @@ class LLVMBuilder(LLVMBaseBuilder):
             defines['LIBXML2_INCLUDE_DIR'] = str(self.libxml2.include_dir)
             defines['LIBXML2_LIBRARY'] = str(self.libxml2.link_libraries[0])
 
-        if self.libzstd:
-            defines['LLVM_ENABLE_ZSTD'] = 'FORCE_ON'
-            defines['LLVM_USE_STATIC_ZSTD'] = 'ON'
-            defines['zstd_LIBRARY'] = self.libzstd.link_libraries[0]
-            defines['zstd_STATIC_LIBRARY'] = self.libzstd.link_libraries[1]
-            defines['zstd_INCLUDE_DIR'] = self.libzstd.include_dir
-        else:
-            defines['LLVM_ENABLE_ZSTD'] = 'OFF'
-
         if self.build_lldb:
             self._set_lldb_flags(self._config.target_os, defines)
 
         defines['CLANG_DEFAULT_LINKER'] = 'lld'
         defines['CLANG_DEFAULT_OBJCOPY'] = 'llvm-objcopy'
 
-        # Omit versions on LLVM's Linux and Darwin shared libraries. The versions for the runtimes
-        # (e.g. libc++) are also omitted, using OS-specific versions of the same CMake flag.
-        defines['CMAKE_PLATFORM_NO_VERSIONED_SONAME'] = 'ON'
-
-        # Use static libunwinder for host builds.
-        defines['LIBCXXABI_USE_LLVM_UNWINDER'] = 'ON'
-        defines['LIBCXXABI_ENABLE_STATIC_UNWINDER'] = 'ON'
-
         if self._config.target_os.is_darwin:
             defines['COMPILER_RT_ENABLE_IOS'] = 'OFF'
             defines['COMPILER_RT_ENABLE_TVOS'] = 'OFF'
@@ -780,95 +732,81 @@ class LLVMBuilder(LLVMBaseBuilder):
             runtimes_cmake_args = []
             runtimes_cmake_args.append(f'-DCMAKE_OSX_DEPLOYMENT_TARGET={constants.MAC_MIN_VERSION}')
             runtimes_cmake_args.append('-DCMAKE_OSX_ARCHITECTURES=arm64|x86_64')
-            runtimes_cmake_args.append('-DCMAKE_PLATFORM_NO_VERSIONED_SONAME=ON')
             defines['RUNTIMES_CMAKE_ARGS'] = ';'.join(sorted(runtimes_cmake_args))
 
-            # Add libc++abi to libc++.{a,dylib} for consistency with Linux.
-            defines['LIBCXX_ENABLE_STATIC_ABI_LIBRARY'] = 'ON'
-
         if self._config.target_os.is_linux:
-            runtime_configs: List[configs.Config] = []
-            if self.build_cross_runtimes:
-                # We have to build libc++ for musl for the sake of
-                # MuslHostRuntimeBuilder, which is enabled for both the glibc
-                # and musl builds of LLVM.
-                runtime_configs = [
-                    configs.LinuxMuslHostConfig(hosts.Arch.ARM),
-                    configs.LinuxMuslHostConfig(hosts.Arch.AARCH64),
-                    configs.LinuxMuslHostConfig(hosts.Arch.I386),
-                    configs.LinuxMuslHostConfig(hosts.Arch.X86_64),
-                ]
-                if not self._config.is_musl:
-                    runtime_configs += [
-                        self._config,
-                        configs.LinuxConfig(is_32_bit=True),
-                    ]
-            else:
-                runtime_configs = [self._config]
+            runtime_configs = [self._config]
+            if self.build_32bit_runtimes:
+                if self._config.is_musl:
+                    runtime_configs.append(configs.LinuxMuslHostConfig(hosts.Arch.I386))
+                else:
+                    runtime_configs.append(configs.LinuxConfig(is_32_bit=True))
 
             self.runtimes_triples = list(_config.llvm_triple for _config in runtime_configs)
             triples = ';'.join(self.runtimes_triples)
             defines['LLVM_BUILTIN_TARGETS'] = triples
             defines['LLVM_RUNTIME_TARGETS'] = triples
 
+            # With per-target runtime dirs, clang no longer links the builtins
+            # for the glibc triple when targetting musl.  In the glibc
+            # configuration, build the musl builtins as well.
+            if self.build_32bit_runtimes and not self._config.is_musl:
+                defines['LLVM_BUILTIN_TARGETS'] = triples + ';x86_64-unknown-linux-musl;i686-unknown-linux-musl'
+
             # We need to explicitly propagate some CMake flags to the runtimes
             # CMake invocation that builds compiler-rt, libcxx, and other
             # runtimes for the host.
             runtimes_passthrough_args = [
                     'CMAKE_POSITION_INDEPENDENT_CODE',
                     'LLVM_ENABLE_LIBCXX',
-                    'LIBCXXABI_USE_LLVM_UNWINDER',
-                    'LIBCXXABI_ENABLE_STATIC_UNWINDER',
             ]
 
             for _config in runtime_configs:
                 triple = _config.llvm_triple
                 cflags = _config.cflags + self.cflags
                 cxxflags = _config.cxxflags + self.cxxflags
-                ldflags = _config.ldflags + self.ldflags_for_runtime(_config)
-
-                if _config.sysroot:
-                    cflags.append(f'--sysroot={_config.sysroot}')
+                ldflags = _config.ldflags + self.ldflags
 
                 cflags_str = ' '.join(cflags)
                 cxxflags_str = ' '.join(cxxflags)
                 ldflags_str = ' '.join(ldflags)
 
-                for base in ('BUILTINS', 'RUNTIMES'):
-                    if _config.sysroot:
-                        defines[f'{base}_{triple}_CMAKE_SYSROOT'] = _config.sysroot
-                    defines[f'{base}_{triple}_CMAKE_C_FLAGS'] = cflags_str
-                    defines[f'{base}_{triple}_CMAKE_CXX_FLAGS'] = cxxflags_str
-                    defines[f'{base}_{triple}_CMAKE_EXE_LINKER_FLAGS'] = ldflags_str
-                    defines[f'{base}_{triple}_CMAKE_SHARED_LINKER_FLAGS'] = ldflags_str
-                    defines[f'{base}_{triple}_CMAKE_MODULE_LINKER_FLAGS'] = ldflags_str
-                    defines[f'{base}_{triple}_CMAKE_PLATFORM_NO_VERSIONED_SONAME'] = 'ON'
+                if _config.sysroot:
+                    defines[f'RUNTIMES_{triple}_CMAKE_SYSROOT'] = _config.sysroot
+                defines[f'RUNTIMES_{triple}_CMAKE_C_FLAGS'] = cflags_str
+                defines[f'RUNTIMES_{triple}_CMAKE_CXX_FLAGS'] = cxxflags_str
+                defines[f'RUNTIMES_{triple}_CMAKE_EXE_LINKER_FLAGS'] = ldflags_str
+                defines[f'RUNTIMES_{triple}_CMAKE_SHARED_LINKER_FLAGS'] = ldflags_str
+                defines[f'RUNTIMES_{triple}_CMAKE_MODULE_LINKER_FLAGS'] = ldflags_str
 
-                    if _config.is_musl:
-                        for key, value in _config.cmake_defines.items():
-                            defines[f'{base}_{triple}_{key}'] = value
+                # clang generates call to builtin functions when building
+                # compiler-rt for musl.  Allow use of the builtins library.
+                if _config.is_musl and _config.target_arch == hosts.Arch.I386:
+                    defines[f'RUNTIMES_{triple}_COMPILER_RT_USE_BUILTINS_LIBRARY'] = 'ON'
 
-                    for arg in runtimes_passthrough_args:
-                        defines[f'{base}_{triple}_{arg}'] = defines[arg]
+                for arg in runtimes_passthrough_args:
+                    defines[f'RUNTIMES_{triple}_{arg}'] = defines[arg]
 
-                    # Don't depend on the host libatomic library.
-                    defines[f'{base}_{triple}_LIBCXX_HAS_ATOMIC_LIB'] = 'NO'
+                # Don't depend on the host libatomic library.
+                defines[f'RUNTIMES_{triple}_LIBCXX_HAS_ATOMIC_LIB'] = 'NO'
 
-                    # Make libc++.so a symlink to libc++.so.x instead of a linker script that
-                    # also adds -lc++abi.  Statically link libc++abi to libc++ so it is not
-                    # necessary to pass -lc++abi explicitly.
-                    defines[f'{base}_{triple}_LIBCXX_ENABLE_ABI_LINKER_SCRIPT'] = 'OFF'
-                    defines[f'{base}_{triple}_LIBCXX_ENABLE_STATIC_ABI_LIBRARY'] = 'ON'
+                # Make libc++.so a symlink to libc++.so.x instead of a linker script that
+                # also adds -lc++abi.  Statically link libc++abi to libc++ so it is not
+                # necessary to pass -lc++abi explicitly.  This is needed only for Linux.
+                defines[f'RUNTIMES_{triple}_LIBCXX_ENABLE_ABI_LINKER_SCRIPT'] = 'OFF'
+                defines[f'RUNTIMES_{triple}_LIBCXX_ENABLE_STATIC_ABI_LIBRARY'] = 'ON'
 
-                    # Set LIBCXX variables for compiler and linker flags for tests.
-                    defines[f'{base}_{triple}_LIBCXX_TEST_COMPILER_FLAGS'] = cxxflags_str
-                    defines[f'{base}_{triple}_LIBCXX_TEST_LINKER_FLAGS'] = ldflags_str
+                # Set LIBCXX variables for compiler and linker flags for tests.
+                defines[f'RUNTIMES_{triple}_LIBCXX_TEST_COMPILER_FLAGS'] = cxxflags_str
+                defines[f'RUNTIMES_{triple}_LIBCXX_TEST_LINKER_FLAGS'] = ldflags_str
 
-                    # Don't let libclang_rt.*_cxx.a depend on libc++abi.
-                    defines[f'{base}_{triple}_SANITIZER_ALLOW_CXXABI'] = 'OFF'
+                # Don't let libclang_rt.*_cxx.a depend on libc++abi.
+                defines[f'RUNTIMES_{triple}_SANITIZER_ALLOW_CXXABI'] = 'OFF'
 
         if self.enable_mlgo:
-            defines['TENSORFLOW_AOT_PATH'] = paths.get_tensorflow_path()
+            defines['TENSORFLOW_AOT_PATH'] = os.getenv('TENSORFLOW_INSTALL')
+            defines['LLVM_INLINER_MODEL_PATH'] = paths.mlgo_model('inlining-Oz-99f0063-v1.1')
+            defines['LLVM_RAEVICT_MODEL_PATH'] = paths.mlgo_model('regalloc-evict-e67430c-v1.0')
 
         return defines
 
@@ -888,18 +826,24 @@ class LLVMBuilder(LLVMBaseBuilder):
         """Gets the built Toolchain."""
         return toolchains.Toolchain(self.install_dir, self.output_dir)
 
+    def build(self) -> None:
+        super().build()
+        if self.use_sccache:
+            utils.check_call(['sccache', '--show-stats'])
+
+
     def test(self) -> None:
         with timer.Timer(f'stage2_test'):
             # newer test tools like dexp, clang-query, c-index-test
             # need libedit.so.*, libxml2.so.*, etc. in stage2/lib.
             self._install_lib_deps(self.output_dir / 'lib')
-            # musl cannot run check-cxx yet
-            cxx_triples = [triple for triple in sorted(self.runtimes_triples) if 'linux-musl' not in triple]
-            checks = ['check-clang', 'check-llvm', 'check-clang-tools'] + ['check-cxx-' + triple for triple in cxx_triples]
-            # clangd tests fail intermittently. https://github.com/llvm/llvm-project/issues/64964
-            check_env = {'LIT_FILTER_OUT': 'clangd'}
-            if hosts.build_host().is_darwin:
-                # b/298489611, b/326166097
-                check_env = {'LIT_FILTER_OUT': 'clangd|clang-tidy|xpc|tools\/lto|LineEditor|Interpreter|ClangIncludeCleaner|ClangPseudo'}
-                checks.remove('check-llvm')
-            self._ninja(checks, check_env)
+            self._ninja(
+                ['check-clang', 'check-llvm', 'check-clang-tools'] +
+                ['check-cxx-' + triple for triple in sorted(self.runtimes_triples)])
+        # Known failed tests:
+        #   Clang :: CodeGenCXX/builtins.cpp
+        #   Clang :: CodeGenCXX/unknown-anytype.cpp
+        #   Clang :: Sema/builtin-setjmp.c
+        #   LLVM :: Bindings/Go/go.test (disabled by LLVM_INCLUDE_GO_TESTS=OFF)
+        #   LLVM :: CodeGen/X86/extractelement-fp.ll
+        #   LLVM :: CodeGen/X86/fp-round.ll
diff --git a/bisect_build.py b/bisect_build.py
deleted file mode 100755
index 23cf020..0000000
--- a/bisect_build.py
+++ /dev/null
@@ -1,145 +0,0 @@
-#!/usr/bin/env python3
-#
-# Copyright (C) 2024 The Android Open Source Project
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-import argparse
-import sys
-from typing import List, Optional
-from fetch_kokoro_prebuilts import check_valid_build, check_valid_path, fetch_prebuilts, get_build_number
-from utils import check_tools
-
-
-def parse_args(sys_argv: Optional[List[str]]):
-    """Parse the command line arguments."""
-
-    parser = argparse.ArgumentParser(description="Fetch prebuilts from kokoro.")
-
-    parser.add_argument("good", type=str, nargs=1, help="good SHA")
-    parser.add_argument("bad", type=str, nargs=1, help="bad SHA")
-    parser.add_argument(
-        "target",
-        type=str,
-        nargs=1,
-        help="Target Clang path (e.g. ANDROID_TOP/prebuilts/clang/linux-x86/)",
-    )
-    return parser.parse_args(sys_argv)
-
-
-def get_result(num: str):
-    """Testing the build manually and get the result from users."""
-    print(f"\nTesting build {num}...")
-    res = input("Is build " + num + " a good build?  [y/n]:")
-    while res != "y" and res != "n":
-        res = input("Please enter y or n:")
-    return res
-
-
-def bisect(start: int, end: int, target: str):
-    """Find the build number which might be the root cause by bisection.
-
-    Users need to verify manually. If it is a good build, enter 'y'.
-    Otherwise, enter 'n'. After narrowing the range repeatedly, the
-    possible root cause will be found.
-
-    Args:
-        start: the build id which is a good build.
-        bad: the build id which is a bad build.
-        target: the target path to download the prebuilts.
-
-    Returns:
-        The build number combo that might be the culprit(s).
-    """
-    if start >= end - 1:
-        return (end, end)
-    mid = (start + end) // 2
-    result = fetch_prebuilts(str(mid), target)
-    if result:
-        res = get_result(str(mid))
-        if res == "y":
-            return bisect(mid, end, target)
-        else:
-            return bisect(start, mid, target)
-    else:
-        # Mid point build is broken, find the neighbouring good build
-        left = mid - 1
-        while start < left:
-            result = fetch_prebuilts(str(left), target)
-            if result:
-                break
-            left = left - 1
-
-        right = mid + 1
-        while right < end:
-            result = fetch_prebuilts(str(right), target)
-            if result:
-                break
-            right = right + 1
-
-        if start == left:
-            if end != right:
-                res = get_result(str(right))
-                if res == "y":
-                    return bisect(right, end, target)
-            return (left + 1, right)
-
-        else:
-            res = get_result(str(left))
-            if res == "y":
-                if end == right:
-                    return (left + 1, end)
-                else:
-                    res = get_result(str(right))
-                    if res == "y":
-                        return bisect(right, end, target)
-                    else:
-                        return (left + 1, right)
-            else:
-                return bisect(start, left, target)
-
-
-def main(sys_argv: List[str]):
-    check_tools(True)
-    args_output = parse_args(sys_argv)
-
-    good = get_build_number(args_output.good[0])
-    bad = get_build_number(args_output.bad[0])
-    target = args_output.target[0]
-
-    check_valid_path(target)
-    check_valid_build(good)
-    check_valid_build(bad)
-
-    start = int(good)
-    end = int(bad)
-
-    if start >= end:
-        err_msg = (
-            f"{good} is not smaller than {bad}. Please pass a valid combo."
-        )
-        raise Exception(err_msg)
-
-    result = bisect(start, end, target)
-    if result[0] == result[1]:
-        print(f"The culprit is build {result[0]}.")
-    else:
-        print(
-            f"The culprit is in the range from build {result[0]} to"
-            f" {result[1]}."
-        )
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/bisect_driver.py b/bisect_driver.py
old mode 100644
new mode 100755
diff --git a/builder_registry.py b/builder_registry.py
old mode 100644
new mode 100755
diff --git a/builders.py b/builders.py
old mode 100644
new mode 100755
index d6626a7..761cdc3
--- a/builders.py
+++ b/builders.py
@@ -31,7 +31,6 @@ import hosts
 import mapfile
 import multiprocessing
 import paths
-import tempfile
 import utils
 
 class SanitizerMapFileBuilder(base_builders.Builder):
@@ -60,14 +59,18 @@ class SanitizerMapFileBuilder(base_builders.Builder):
 class Stage1Builder(base_builders.LLVMBuilder):
     name: str = 'stage1'
     install_dir: Path = paths.OUT_DIR / 'stage1-install'
+    build_android_targets: bool = False
     build_extra_tools: bool = False
 
     @property
     def llvm_targets(self) -> Set[str]:
-        if self._config.target_os.is_darwin:
-            return constants.DARWIN_HOST_TARGETS
-        else:
+        if self.build_android_targets:
             return constants.HOST_TARGETS | constants.ANDROID_TARGETS
+        else:
+            if self._config.target_os.is_darwin:
+                return constants.DARWIN_HOST_TARGETS
+            else:
+                return constants.HOST_TARGETS
 
     @property
     def llvm_projects(self) -> Set[str]:
@@ -81,27 +84,22 @@ class Stage1Builder(base_builders.LLVMBuilder):
 
     @property
     def llvm_runtime_projects(self) -> Set[str]:
-        return {'compiler-rt', 'libcxx', 'libcxxabi', 'libunwind'}
+        proj = {'compiler-rt', 'libcxx', 'libcxxabi'}
+        if isinstance(self._config, configs.LinuxMuslConfig):
+            # libcxx builds against libunwind when building for musl
+            proj.add('libunwind')
+        return proj
 
     @property
     def ldflags(self) -> List[str]:
         ldflags = super().ldflags
-        if self._config.target_os.is_darwin:
-            # On Darwin, -static-libstdc++ isn't supported. So use rpath to find c++ runtime.
-            ldflags.append(f'-Wl,-rpath,{self.toolchain.path / "lib"}')
-        else:
-            # Use -static-libstdc++ to statically link the c++ runtime [1].  This
-            # avoids specifying self.toolchain.lib_dirs in rpath to find libc++ at
-            # runtime.
-            # [1] libc++ in our case, despite the flag saying -static-libstdc++.
-            ldflags.append('-static-libstdc++')
-
+        # Use -static-libstdc++ to statically link the c++ runtime [1].  This
+        # avoids specifying self.toolchain.lib_dirs in rpath to find libc++ at
+        # runtime.
+        # [1] libc++ in our case, despite the flag saying -static-libstdc++.
+        ldflags.append('-static-libstdc++')
         return ldflags
 
-    def ldflags_for_runtime(self, _config: configs.Config) -> List[str]:
-        # Stage1 doesn't cross-compile runtimes, so just use the same ldflags for LLVM and runtimes.
-        return self.ldflags
-
     @property
     def cmake_defines(self) -> Dict[str, str]:
         defines = super().cmake_defines
@@ -122,11 +120,6 @@ class Stage1Builder(base_builders.LLVMBuilder):
         # Don't build libfuzzer as part of the first stage build.
         defines['COMPILER_RT_BUILD_LIBFUZZER'] = 'OFF'
 
-        # Use x86_64 models to optimize the release Android Clang.
-        if self.enable_mlgo:
-            defines['LLVM_INLINER_MODEL_PATH'] = paths.mlgo_model('x86_64/inlining-Oz-99f0063-v1.1')
-            defines['LLVM_RAEVICT_MODEL_PATH'] = paths.mlgo_model('x86_64/regalloc-evict-e67430c-v1.0')
-
         return defines
 
     def test(self) -> None:
@@ -159,55 +152,45 @@ class Stage2Builder(base_builders.LLVMBuilder):
 
     @property
     def llvm_runtime_projects(self) -> Set[str]:
-        return {'compiler-rt', 'libcxx', 'libcxxabi', 'libunwind'}
-
-    @property
-    def ld_library_path_env_name(self) -> str:
-        return 'LD_LIBRARY_PATH' if self._config.target_os.is_linux else 'DYLD_LIBRARY_PATH'
+        proj = {'compiler-rt', 'libcxx', 'libcxxabi'}
+        if isinstance(self._config, configs.LinuxMuslConfig):
+            # libcxx builds against libunwind when building for musl
+            proj.add('libunwind')
+        return proj
 
     @property
     def env(self) -> Dict[str, str]:
         env = super().env
-        if self._config.target_os.is_linux:
-            # Point CMake to the libc++ from stage1.  It is possible that once built,
-            # the newly-built libc++ may override this because of the rpath pointing to
-            # $ORIGIN/../lib.  That'd be fine because both libraries are built from
-            # the same sources.
-            # Newer compilers put lib files in lib/x86_64-unknown-linux-gnu.
-            # Include the path to the libc++.so.1 in stage2-install,
-            # to run unittests/.../*Tests programs.
-            env['LD_LIBRARY_PATH'] = (
-                    ':'.join([str(item) for item in self.toolchain.lib_dirs])
-                    + f':{self.install_dir}/lib')
+        # Point CMake to the libc++ from stage1.  It is possible that once built,
+        # the newly-built libc++ may override this because of the rpath pointing to
+        # $ORIGIN/../lib.  That'd be fine because both libraries are built from
+        # the same sources.
+        # Newer compilers put lib files in lib/x86_64-unknown-linux-gnu.
+        # Include the path to the libc++.so.1 in stage2-install,
+        # to run unittests/.../*Tests programs.
+        env['LD_LIBRARY_PATH'] = (
+                ':'.join([str(item) for item in self.toolchain.lib_dirs])
+                + f':{self.install_dir}/lib')
         return env
 
-    def _common_ldflags(self, config: configs.Config) -> List[str]:
-        """Extra ldflags used for both the main LLVM stage2 and the runtimes builds."""
-        ldflags = []
-        # TODO: Turn on ICF for Darwin once it can be built with LLD.
-        if not config.target_os.is_darwin:
-            ldflags.append('-Wl,--icf=safe')
-        return ldflags
-
     @property
     def ldflags(self) -> List[str]:
         ldflags = super().ldflags
         if self._config.target_os.is_linux:
-            ldflags.append(f'-Wl,-rpath,\\$ORIGIN:\\$ORIGIN/../lib/{self._config.llvm_triple}')
+            if isinstance(self._config, configs.LinuxMuslConfig):
+                ldflags.append('-Wl,-rpath,\$ORIGIN/../lib/x86_64-unknown-linux-musl')
+            else:
+                ldflags.append('-Wl,-rpath,\$ORIGIN/../lib/x86_64-unknown-linux-gnu')
         # '$ORIGIN/../lib' is added by llvm's CMake rules.
         if self.bolt_optimize or self.bolt_instrument:
             ldflags.append('-Wl,-q')
+        # TODO: Turn on ICF for Darwin once it can be built with LLD.
+        if not self._config.target_os.is_darwin:
+            ldflags.append('-Wl,--icf=safe')
         if self.lto and self.enable_mlgo:
             ldflags.append('-Wl,-mllvm,-regalloc-enable-advisor=release')
-        if self.lto and not self.profdata_file and self._config.target_os.is_linux:
-            ldflags.append('-Wl,--lto-O0')
-        ldflags += self._common_ldflags(self._config)
         return ldflags
 
-    def ldflags_for_runtime(self, config: configs.Config) -> List[str]:
-        # N.B. The runtimes build doesn't add '$ORIGIN/../lib' implicitly.
-        return ['-Wl,-rpath,\\$ORIGIN'] + self._common_ldflags(config)
-
     @property
     def cflags(self) -> List[str]:
         cflags = super().cflags
@@ -261,38 +244,17 @@ class Stage2Builder(base_builders.LLVMBuilder):
         if self._config.target_os.is_darwin:
             defines['LLVM_BUILD_EXTERNAL_COMPILER_RT'] = 'ON'
 
-        # Embed ARM64 models for optimizing ARM64 AOSP / NDK.
-        if self.enable_mlgo:
-            defines['LLVM_INLINER_MODEL_PATH'] = paths.mlgo_model('arm64/inlining-Oz-chromium')
-            defines['LLVM_RAEVICT_MODEL_PATH'] = paths.mlgo_model('arm64/regalloc-evict-aosp')
-
         return defines
 
-    def _build_config(self) -> None:
-        if self._config.target_os.is_darwin:
-            # Tablegen binaries (like llvm-min-tblgen, llvm-tblgen) are built and ran before
-            # building libc++.dylib. We need someway to help them find libc++.dylib in
-            # stage1-install. Because /usr/lib/libc++.1.dylib may be too old to support them.
-            # On darwin, System Integrity Protection blocks DYLD_LIBRARY_PATH from taking effect
-            # through /bin/bash. And we don't want to change rpath for all binaries built in
-            # stage2. So we copy libc++.dylib from stage1-install to stage2 before building stage2.
-            # This will help us run tablegen binaries. And it will be overwritten by libc++.dylib
-            # built in stage2.
-            lib_dir = self.output_dir / 'lib'
-            lib_dir.mkdir(parents=True, exist_ok=True)
-            libcxx_path = lib_dir / 'libc++.dylib'
-            if not libcxx_path.is_file():
-                shutil.copy2(self.toolchain.path / 'lib' / 'libc++.dylib', libcxx_path)
-        return super()._build_config()
-
     def install_config(self) -> None:
         super().install_config()
         lldb_wrapper_path = self.install_dir / 'bin' / 'lldb.sh'
+        lib_path_env = 'LD_LIBRARY_PATH' if self._config.target_os.is_linux else 'DYLD_LIBRARY_PATH'
         lldb_wrapper_path.write_text(textwrap.dedent(f"""\
             #!/bin/bash
             CURDIR=$(cd $(dirname $0) && pwd)
             export PYTHONHOME="$CURDIR/../python3"
-            export {self.ld_library_path_env_name}="$CURDIR/../python3/lib:${self.ld_library_path_env_name}"
+            export {lib_path_env}="$CURDIR/../python3/lib:${lib_path_env}"
             "$CURDIR/lldb" "$@"
         """))
         lldb_wrapper_path.chmod(0o755)
@@ -325,10 +287,6 @@ class BuiltinsBuilder(base_builders.LLVMRuntimeBuilder):
         riscv64.platform = True
         result.append(riscv64)
         result.append(configs.BaremetalAArch64Config())
-        result.append(configs.BaremetalArmv6MConfig())
-        result.append(configs.BaremetalArmv8MBaseConfig())
-        for fpu in hosts.Armv81MMainFpu:
-            result.append(configs.BaremetalArmv81MMainConfig(fpu))
         # For arm32 and x86, build a special version of the builtins library
         # where the symbols are exported, not hidden. This version is needed
         # to continue exporting builtins from libc.so and libm.so.
@@ -338,7 +296,6 @@ class BuiltinsBuilder(base_builders.LLVMRuntimeBuilder):
             result.append(arch)
         result.append(configs.LinuxMuslConfig(hosts.Arch.AARCH64))
         result.append(configs.LinuxMuslConfig(hosts.Arch.ARM))
-        result.append(configs.LinuxMuslConfig(hosts.Arch.X86_64))
         return result
 
     @property
@@ -370,10 +327,7 @@ class BuiltinsBuilder(base_builders.LLVMRuntimeBuilder):
         # For CMake feature testing, create an archive instead of an executable,
         # because we can't link an executable until builtins have been built.
         defines['CMAKE_TRY_COMPILE_TARGET_TYPE'] = 'STATIC_LIBRARY'
-        # Baremetal Armv6-M does not support atomics and the build
-        # fails with a static assert if they are included.
-        if not isinstance(self._config, configs.BaremetalArmv6MConfig):
-            defines['COMPILER_RT_EXCLUDE_ATOMIC_BUILTIN'] = 'OFF'
+        defines['COMPILER_RT_EXCLUDE_ATOMIC_BUILTIN'] = 'OFF'
         defines['COMPILER_RT_OS_DIR'] = self._config.target_os.crt_dir
         return defines
 
@@ -387,35 +341,21 @@ class BuiltinsBuilder(base_builders.LLVMRuntimeBuilder):
         filename = 'libclang_rt.builtins-' + sarch
         filename += '-android.a' if self._config.target_os.is_android else '.a'
         filename_exported = 'libclang_rt.builtins-' + sarch + '-android-exported.a'
-        if isinstance(self._config, configs.BaremetalArmMultilibConfig):
-            # For ARM targets, compiler-rt uses the triple to decide which sources to include,
-            # however the triple also affects the library suffix (e.g. -armv6m.a vs -arm.a).
-            # In order to ensure the correct sources are used, we have to include the subarch in
-            # the triple, but we keep the suffix as just 'arm' in the final output to support
-            # the commonly used 'arm-none-eabi[hf]' triple.
-            src_filename = 'libclang_rt.builtins-' + self._config.llvm_triple.split('-')[0] + '.a'
-            src_path = self.output_dir / 'lib' / self._config.target_os.crt_dir / src_filename
-            # Copy libs into separate multilib directories to prevent name conflicts.
-            out_res_dir = self.output_resource_dir / self._config.multilib_name / 'lib'
-            res_dir = self.resource_dir / self._config.multilib_name / 'lib'
-        else:
-            src_path = self.output_dir / 'lib' / self._config.target_os.crt_dir / filename
-            out_res_dir = self.output_resource_dir
-            res_dir = self.resource_dir
+        src_path = self.output_dir / 'lib' / self._config.target_os.crt_dir / filename
 
-        out_res_dir.mkdir(parents=True, exist_ok=True)
+        self.output_resource_dir.mkdir(parents=True, exist_ok=True)
         if self.is_exported:
             # This special copy exports its symbols and is only intended for use
             # in Bionic's libc.so.
-            shutil.copy2(src_path, out_res_dir / filename_exported)
+            shutil.copy2(src_path, self.output_resource_dir / filename_exported)
         else:
-            shutil.copy2(src_path, out_res_dir / filename)
+            shutil.copy2(src_path, self.output_resource_dir / filename)
 
             # Also install to self.resource_dir, if it's different,
             # for use when building target libraries.
-            if res_dir != out_res_dir:
-                res_dir.mkdir(parents=True, exist_ok=True)
-                shutil.copy2(src_path, res_dir / filename)
+            if self.resource_dir != self.output_resource_dir:
+                self.resource_dir.mkdir(parents=True, exist_ok=True)
+                shutil.copy2(src_path, self.resource_dir / filename)
 
             # Make a copy for the NDK.
             if self._config.target_os.is_android:
@@ -445,23 +385,6 @@ class CompilerRTBuilder(base_builders.LLVMRuntimeBuilder):
         defines = super().cmake_defines
         defines['COMPILER_RT_BUILD_BUILTINS'] = 'OFF'
         defines['COMPILER_RT_USE_BUILTINS_LIBRARY'] = 'ON'
-        # Link an isolated copy of libc++ into the fuzzer archive.
-        defines['COMPILER_RT_USE_LIBCXX'] = 'ON'
-        # Set ANDROID_NATIVE_API_LEVEL for the sake of the custom libc++ built
-        # for the fuzzer. There is a check for ANDROID_NATIVE_API_LEVEL in
-        # HandleLLVMOptions.cmake that determines the value of
-        # LLVM_FORCE_SMALLFILE_FOR_ANDROID and _FILE_OFFSET_BITS.
-        defines['ANDROID_NATIVE_API_LEVEL'] = str(self._config.api_level)
-        # The fuzzer's isolated copy of libc++ is configured using
-        # -DCMAKE_TRY_COMPILE_TARGET_TYPE=STATIC_LIBRARY, which breaks some of
-        # the feature detection. Set some settings manually. These settings are
-        # passed through to a libc++ CMake invocation by `add_custom_libcxx` in
-        # compiler-rt/cmake/Modules/AddCompilerRT.cmake.
-        # TODO: Once github.com/llvm/llvm-project/pull/70534 is merged, these
-        # settings can be removed.
-        defines['LIBCXX_HAS_PTHREAD_LIB'] = 'OFF'
-        defines['LIBCXX_HAS_RT_LIB'] = 'OFF'
-        defines['LIBCXXABI_HAS_PTHREAD_LIB'] = 'OFF'
         # FIXME: Disable WError build until upstream fixed the compiler-rt
         # personality routine warnings caused by r309226.
         # defines['COMPILER_RT_ENABLE_WERROR'] = 'ON'
@@ -484,7 +407,8 @@ class CompilerRTBuilder(base_builders.LLVMRuntimeBuilder):
         # set. We want this flag on instead to catch unresolved references
         # early.
         defines['SANITIZER_COMMON_LINK_FLAGS'] = '-Wl,-z,defs'
-        defines['COMPILER_RT_HWASAN_WITH_INTERCEPTORS'] = 'OFF'
+        if self._config.platform:
+            defines['COMPILER_RT_HWASAN_WITH_INTERCEPTORS'] = 'OFF'
         return defines
 
     @property
@@ -633,9 +557,8 @@ class LibUnwindBuilder(base_builders.LLVMRuntimeBuilder):
         # Override the default -unwindlib=libunwind. libunwind.a doesn't exist
         # when libunwind is built, and libunwind can't use
         # CMAKE_TRY_COMPILE_TARGET_TYPE=STATIC_LIBRARY because
-        # LIBUNWIND_HAS_PTHREAD_LIB must be set to false. Also avoid linking the
-        # STL because it too does not exist yet.
-        return super().ldflags + ['-unwindlib=none', '-nostdlib++']
+        # LIBUNWIND_HAS_PTHREAD_LIB must be set to false.
+        return super().ldflags + ['-unwindlib=none']
 
     @property
     def cmake_defines(self) -> Dict[str, str]:
@@ -710,6 +633,13 @@ class LibOMPBuilder(base_builders.LLVMRuntimeBuilder):
         defines['LIBOMP_ENABLE_SHARED'] = 'TRUE' if self.is_shared else 'FALSE'
         return defines
 
+    @property
+    def ldflags(self) -> List[str]:
+        # Workaround for undefined version symbols in libomp.
+        # https://reviews.llvm.org/D135402
+        # http://b/258377285
+        return ["-Wl,--undefined-version"]
+
     def install_config(self) -> None:
         # We need to install libomp manually.
         libname = 'libomp.' + ('so' if self.is_shared else 'a')
@@ -731,13 +661,9 @@ class LibNcursesBuilder(base_builders.AutoconfBuilder, base_builders.LibInfo):
 
     @property
     def config_flags(self) -> List[str]:
-        flags = super().config_flags + [
+        return super().config_flags + [
             '--with-shared',
-            '--with-default-terminfo-dir=/usr/share/terminfo',
         ]
-        if self._config.target_os.is_darwin:
-            flags.append('--disable-mixed-case')
-        return flags
 
     @property
     def _lib_names(self) -> List[str]:
@@ -747,19 +673,15 @@ class LibNcursesBuilder(base_builders.AutoconfBuilder, base_builders.LibInfo):
 class LibEditBuilder(base_builders.AutoconfBuilder, base_builders.LibInfo):
     name: str = 'libedit'
     src_dir: Path = paths.LIBEDIT_SRC_DIR
-    libncurses: base_builders.LibInfo
 
     @property
     def ldflags(self) -> List[str]:
         return [
-            f'-L{self.libncurses.link_libraries[0].parent}',
         ] + super().ldflags
 
     @property
     def cflags(self) -> List[str]:
         flags = []
-        flags.append('-I' + str(self.libncurses.include_dir))
-        flags.append('-I' + str(self.libncurses.include_dir / 'ncurses'))
         return flags + super().cflags
 
 
@@ -776,7 +698,6 @@ class SwigBuilder(base_builders.AutoconfBuilder):
     def config_flags(self) -> List[str]:
         flags = super().config_flags
         flags.append('--without-pcre')
-        flags.append('--disable-ccache')
         return flags
 
     @property
@@ -787,13 +708,6 @@ class SwigBuilder(base_builders.AutoconfBuilder):
             ldflags.append(f'-Wl,-rpath,{lib_dir}')
         return ldflags
 
-    @property
-    def env(self) -> List[str]:
-        env = super().env
-        env['BISON'] = paths.BISON_BIN_PATH
-        env['M4'] = paths.M4_BIN_PATH
-        return env
-
 
 class XzBuilder(base_builders.CMakeBuilder, base_builders.LibInfo):
     name: str = 'liblzma'
@@ -924,7 +838,6 @@ class LldbServerBuilder(base_builders.LLVMRuntimeBuilder):
             hosts.Arch.AARCH64: 'AArch64',
             hosts.Arch.I386: 'X86',
             hosts.Arch.X86_64: 'X86',
-            hosts.Arch.RISCV64: 'RISCV',
         }[self._config.target_arch]
 
     @property
@@ -933,11 +846,12 @@ class LldbServerBuilder(base_builders.LLVMRuntimeBuilder):
         # lldb depends on support libraries.
         defines['LLVM_ENABLE_PROJECTS'] = 'clang;lldb'
         defines['LLVM_TARGETS_TO_BUILD'] = self._llvm_target
-        defines['LLVM_NATIVE_TOOL_DIR'] = str(self.toolchain.build_path / 'bin')
+        defines['LLVM_TABLEGEN'] = str(self.toolchain.build_path / 'bin' / 'llvm-tblgen')
+        defines['CLANG_TABLEGEN'] = str(self.toolchain.build_path / 'bin' / 'clang-tblgen')
+        defines['LLDB_TABLEGEN'] = str(self.toolchain.build_path / 'bin' / 'lldb-tblgen')
         triple = self._config.llvm_triple
         defines['LLVM_HOST_TRIPLE'] = triple.replace('i686', 'i386')
         defines['LLDB_ENABLE_LUA'] = 'OFF'
-        defines['LLDB_INCLUDE_TESTS'] = 'OFF'
         return defines
 
     def install_config(self) -> None:
@@ -949,7 +863,7 @@ class LldbServerBuilder(base_builders.LLVMRuntimeBuilder):
 
 class HostSysrootsBuilder(base_builders.Builder):
     name: str = 'host-sysroots'
-    config_list: List[configs.Config] = (configs.MinGWConfig(), configs.MinGWConfig(is_32_bit=True))
+    config_list: List[configs.Config] = (configs.MinGWConfig(),)
 
     def _build_config(self) -> None:
         config = self._config
@@ -959,18 +873,9 @@ class HostSysrootsBuilder(base_builders.Builder):
             shutil.rmtree(sysroot)
         sysroot.parent.mkdir(parents=True, exist_ok=True)
 
-        # Copy the sysroot.
+        # copy sysroot and add libgcc* to it.
         shutil.copytree(config.gcc_root / config.gcc_triple,
                         sysroot, symlinks=True)
-
-        if config.target_arch == hosts.Arch.I386:
-            shutil.rmtree(sysroot / 'lib')
-            (sysroot / 'lib64').unlink()
-            (sysroot / 'lib32').rename(sysroot / 'lib')
-        elif config.target_arch == hosts.Arch.X86_64:
-            shutil.rmtree(sysroot / 'lib32')
-
-        # Add libgcc* to the sysroot.
         shutil.copytree(config.gcc_lib_dir, sysroot_lib, dirs_exist_ok=True)
 
         # b/237425904 cleanup: uncomment to remove libstdc++ after toolchain defaults to
@@ -978,6 +883,14 @@ class HostSysrootsBuilder(base_builders.Builder):
         # (sysroot_lib / 'libstdc++.a').unlink()
         # shutil.rmtree(sysroot / 'include' / 'c++' / '4.8.3')
 
+        # copy libc++ libs and headers from bootstrap prebuilts.  This is needed
+        # for the libcxx builder to pass CMake configuration.  The libcxx
+        # builder will subsequently overwrite these.
+        shutil.copy(paths.WINDOWS_CLANG_PREBUILT_DIR / 'lib' / 'libc++.a', sysroot_lib)
+        shutil.copy(paths.WINDOWS_CLANG_PREBUILT_DIR / 'lib' / 'libc++abi.a', sysroot_lib)
+        shutil.copytree(paths.WINDOWS_CLANG_PREBUILT_DIR / 'include' / 'c++' / 'v1',
+                        sysroot / 'include' / 'c++' / 'v1')
+
 
 class DeviceSysrootsBuilder(base_builders.Builder):
     name: str = 'device-sysroots'
@@ -989,6 +902,7 @@ class DeviceSysrootsBuilder(base_builders.Builder):
     def _build_config(self) -> None:
         config: configs.AndroidConfig = cast(configs.AndroidConfig, self._config)
         arch = config.target_arch
+        platform = config.platform
         sysroot = config.sysroot
         if sysroot.exists():
             shutil.rmtree(sysroot)
@@ -1005,25 +919,25 @@ class DeviceSysrootsBuilder(base_builders.Builder):
         shutil.copytree(src_sysroot / 'usr' / 'include',
                         sysroot / 'usr' / 'include', symlinks=True)
 
-        if arch != hosts.Arch.RISCV64:
-            # Remove the STL headers.
-            shutil.rmtree(sysroot / 'usr' / 'include' / 'c++')
+        if platform:
+            if arch != hosts.Arch.RISCV64:
+                # Remove the STL headers.
+                shutil.rmtree(sysroot / 'usr' / 'include' / 'c++')
+        else:
+            # Add the android_support headers from usr/local/include.
+            shutil.copytree(src_sysroot / 'usr' / 'local' / 'include',
+                            sysroot / 'usr' / 'local' / 'include', symlinks=True)
 
         # Copy over usr/lib/$TRIPLE.
         src_lib = src_sysroot / 'usr' / 'lib' / config.ndk_sysroot_triple
         dest_lib = sysroot / 'usr' / 'lib' / config.ndk_sysroot_triple
         shutil.copytree(src_lib, dest_lib, symlinks=True)
 
-        # For RISCV64, symlink the 10000 api-dir to 35
-        # TODO (http://b/287650094 Remove this hack when we have a risc-v
-        # sysroot in the NDK.
-        if arch == hosts.Arch.RISCV64:
-            (dest_lib / '35').symlink_to('10000')
-
-        # Remove the NDK's libcompiler_rt-extras.  Also remove the NDK libc++,
-        # except for the riscv64 sysroot which doesn't have these files.
+        # Remove the NDK's libcompiler_rt-extras.  For the platform, also remove
+        # the NDK libc++, except for the riscv64 sysroot which doesn't have
+        # these files.
         (dest_lib / 'libcompiler_rt-extras.a').unlink()
-        if arch != hosts.Arch.RISCV64:
+        if platform and arch != hosts.Arch.RISCV64:
             (dest_lib / 'libc++abi.a').unlink()
             (dest_lib / 'libc++_static.a').unlink()
             (dest_lib / 'libc++_shared.so').unlink()
@@ -1033,242 +947,105 @@ class DeviceSysrootsBuilder(base_builders.Builder):
                 continue
             if not re.match(r'\d+$', subdir.name):
                 continue
-            if arch != hosts.Arch.RISCV64:
+            if platform and arch != hosts.Arch.RISCV64:
                 (subdir / 'libc++.a').unlink()
                 (subdir / 'libc++.so').unlink()
         # Verify that there aren't any extra copies somewhere else in the
         # directory hierarchy.
-        verify_gone = [
-            'libc++abi.a',
-            'libc++_static.a',
-            'libc++_shared.so',
-            'libc++.a',
-            'libc++.so',
-            'libcompiler_rt-extras.a',
-            'libunwind.a',
-        ]
+        verify_gone = ['libcompiler_rt-extras.a', 'libunwind.a']
+        if platform:
+            verify_gone += [
+                'libc++abi.a',
+                'libc++_static.a',
+                'libc++_shared.so',
+                'libc++.a',
+                'libc++.so',
+            ]
         for (parent, _, files) in os.walk(sysroot):
             for f in files:
                 if f in verify_gone:
                     raise RuntimeError('sysroot file should have been ' +
                                        f'removed: {os.path.join(parent, f)}')
 
-
-class DeviceLibcxxBuilder(base_builders.LLVMRuntimeBuilder):
-    name = 'device-libcxx'
+        if platform:
+            # Create a stub library for the platform's libc++.
+            platform_stubs = paths.OUT_DIR / 'platform_stubs' / config.ndk_arch
+            platform_stubs.mkdir(parents=True, exist_ok=True)
+            libdir = sysroot / 'usr' / 'lib'
+            libdir.mkdir(parents=True, exist_ok=True)
+            with (platform_stubs / 'libc++.c').open('w') as f:
+                f.write(textwrap.dedent("""\
+                    void __cxa_atexit() {}
+                    void __cxa_demangle() {}
+                    void __cxa_finalize() {}
+                    void __dynamic_cast() {}
+                    void _ZTIN10__cxxabiv117__class_type_infoE() {}
+                    void _ZTIN10__cxxabiv120__si_class_type_infoE() {}
+                    void _ZTIN10__cxxabiv121__vmi_class_type_infoE() {}
+                    void _ZTISt9type_info() {}
+                """))
+
+            utils.check_call([self.toolchain.cc,
+                              f'--target={config.llvm_triple}',
+                              '-fuse-ld=lld', '-nostdlib', '-shared',
+                              '-Wl,-soname,libc++.so',
+                              '-o{}'.format(libdir / 'libc++.so'),
+                              str(platform_stubs / 'libc++.c')])
+
+
+class PlatformLibcxxAbiBuilder(base_builders.LLVMRuntimeBuilder):
+    name = 'platform-libcxxabi'
     src_dir: Path = paths.LLVM_PATH / 'runtimes'
-
-    config_list: List[configs.Config] = (
-        configs.android_configs(platform=True, extra_config={'hwasan': False, 'noexcept': False}) +
-        configs.android_configs(platform=True, extra_config={'hwasan': False, 'noexcept': True}) +
-        configs.android_configs(platform=False, extra_config={'hwasan': False, 'noexcept': False})
-    )
-
-    def _hwasan_config(platform: bool, noexcept: bool) -> configs.Config:
-        result = configs.AndroidAArch64Config()
-        result.platform = platform
-        result.extra_config = {'hwasan': True, 'noexcept': noexcept}
-        # Increase the NDK's API level to the minimum API level for HWASan.
-        if not platform:
-            result.override_api_level = 29
-        return result
-
-    # Use a separate config list so that HWASan libc++ can be built after
-    # compiler-rt, so that libc++.so can be linked against the HWASan lib.
-    hwasan_config_list: List[configs.Config] = [
-        _hwasan_config(platform=True, noexcept=False),
-        _hwasan_config(platform=True, noexcept=True),
-        _hwasan_config(platform=False, noexcept=False),
-    ]
-
-    @property
-    def _is_hwasan(self) -> bool:
-        return self._config.extra_config['hwasan']
-
-    # A special build of libc++_static.a with exceptions turned off, for use in
-    # the Bionic loader where ELF TLS isn't available for accessing EH globals.
-    @property
-    def _is_noexcept(self) -> bool:
-        return self._config.extra_config['noexcept']
-
-    @property
-    def output_dir(self) -> Path:
-        old_path = super().output_dir
-        name = old_path.name
-        if self._is_noexcept:
-            name += '-noexcept'
-        if self._is_hwasan:
-            name += '-hwasan'
-        return old_path.parent / name
-
-    @property
-    def cflags(self) -> list[str]:
-        result = super().cflags
-        if self._config.target_arch is hosts.Arch.ARM:
-            result.append('-mthumb')
-        if self._is_hwasan:
-            result.append('-fsanitize=hwaddress')
-        return result
-
-    @property
-    def cxxflags(self) -> list[str]:
-        base = super().cxxflags
-        # Required to prevent dlclose from causing crashes on thread exit.
-        # https://github.com/android/ndk/issues/1200
-        #
-        # This doesn't actually control whether thread_local is used in most of the code
-        # base, just whether it is used in the implementation of C++ exception storage
-        # for libc++abi.
-        if self._config.target_arch is hosts.Arch.RISCV64:
-            # But rv64 doesn't support TLS yet (emulated or otherwise).
-            # https://github.com/google/android-riscv64/issues/3
-            return base
-        return base + ['-DHAS_THREAD_LOCAL']
-
-    @property
-    def ldflags(self) -> List[str]:
-        # Avoid linking the STL because it does not exist yet.
-        result = super().ldflags + ['-nostdlib++']
-
-        # For the platform (including APEX) libc++ builds, use the unwinder API
-        # exported from libc.so. Link libunwind.a for NDK builds, which must run
-        # on older platforms where libc.so didn't export the unwinder.
-        if self._config.platform:
-            result.append('-unwindlib=none')
-        else:
-            result.append('-unwindlib=libunwind')
-
-        if self._is_hwasan:
-            result.append('-fsanitize=hwaddress')
-
-        return result
+    config_list: List[configs.Config] = configs.android_configs(
+        platform=True, suppress_libcxx_headers=True)
 
     @property
     def cmake_defines(self) -> Dict[str, str]:
-        executor = paths.LLVM_PATH / 'libcxx' / 'utils' / 'adb_run.py'
-
         defines: Dict[str, str] = super().cmake_defines
         defines['LLVM_ENABLE_RUNTIMES'] ='libcxx;libcxxabi'
-
-        # When the libc++ lit tests invoke clang, they set the triple and
-        # sysroot using these generic CMake flags.
-        defines['CMAKE_C_COMPILER_TARGET'] = self._config.llvm_triple
-        defines['CMAKE_CXX_COMPILER_TARGET'] = self._config.llvm_triple
-        defines['CMAKE_SYSROOT'] = self._config.sysroot
-
         defines['LIBCXXABI_ENABLE_SHARED'] = 'OFF'
-        defines['LIBCXXABI_EXECUTOR'] = executor
-        defines['LIBCXXABI_USE_LLVM_UNWINDER'] = 'OFF'
-        if self._config.platform:
-            defines['LIBCXXABI_NON_DEMANGLING_TERMINATE'] = 'ON'
-            defines['LIBCXXABI_STATIC_DEMANGLE_LIBRARY'] = 'ON'
-            # TODO: Set LIBCXXABI_TEST_CONFIG for the platform libc++.so.
-        else:
-            defines['LIBCXXABI_TEST_CONFIG'] = 'llvm-libc++abi-android-ndk.cfg.in'
-
-        if self._is_noexcept:
-            defines['LIBCXX_ENABLE_SHARED'] = 'OFF'
-            defines['LIBCXXABI_ENABLE_EXCEPTIONS'] = 'OFF'
-            defines['LIBCXX_ENABLE_EXCEPTIONS'] = 'OFF'
-            defines['LIBCXX_STATIC_OUTPUT_NAME'] = 'c++_static_noexcept'
-            defines['LIBCXXABI_STATIC_OUTPUT_NAME'] = 'c++abi_noexcept'
-            defines['LIBCXXABI_DEMANGLE_STATIC_OUTPUT_NAME'] = 'c++demangle_noexcept'
-        else:
-            defines['LIBCXX_ENABLE_SHARED'] = 'ON'
-            defines['LIBCXX_STATIC_OUTPUT_NAME'] = 'c++_static'
+        defines['LIBCXXABI_TARGET_TRIPLE'] = self._config.llvm_triple
 
+        defines['LIBCXX_ENABLE_SHARED'] = 'OFF'
+        defines['LIBCXX_TARGET_TRIPLE'] = self._config.llvm_triple
         defines['LIBCXX_ENABLE_ABI_LINKER_SCRIPT'] = 'OFF'
         defines['LIBCXX_ENABLE_STATIC_ABI_LIBRARY'] = 'ON'
-        defines['LIBCXX_STATICALLY_LINK_ABI_IN_SHARED_LIBRARY'] = 'ON'
-        defines['LIBCXX_EXECUTOR'] = executor
-        if self._config.platform:
-            defines['LIBCXX_STATICALLY_LINK_ABI_IN_STATIC_LIBRARY'] = 'ON'
-            # TODO: Set LIBCXX_TEST_CONFIG for the platform libc++.so.
-        else:
-            defines['LIBCXX_SHARED_OUTPUT_NAME'] = 'c++_shared'
-            defines['LIBCXX_STATICALLY_LINK_ABI_IN_STATIC_LIBRARY'] = 'OFF'
-            defines['LIBCXX_ABI_VERSION'] = '1'
-            defines['LIBCXX_ABI_NAMESPACE'] = '__ndk1'
-            defines['LIBCXX_TEST_CONFIG'] = 'llvm-libc++-android-ndk.cfg.in'
-
-        # There is a check for ANDROID_NATIVE_API_LEVEL in
-        # HandleLLVMOptions.cmake that determines the value of
-        # LLVM_FORCE_SMALLFILE_FOR_ANDROID and _FILE_OFFSET_BITS. Maybe it
-        # should use a different name for the API level macro in CMake?
-        defines['ANDROID_NATIVE_API_LEVEL'] = str(self._config.api_level)
-
         return defines
 
+    def _build_config(self) -> None:
+        if super()._is_64bit():
+            # For arm64 and x86_64, build static cxxabi library from
+            # toolchain/libcxxabi and use it when building runtimes.  This
+            # should affect all compiler-rt runtimes that use libcxxabi
+            # (e.g. asan, hwasan, scudo, tsan, ubsan, xray).
+            super()._build_config()
+        else:
+            self.install_config()
+
     def install_config(self) -> None:
         arch = self._config.target_arch
-        sysroot_lib = self._config.sysroot / 'usr' / 'lib'
-
-        if not self._is_hwasan and not self._is_noexcept:
-            # Copy libc++ headers into the NDK+platform sysroot.
-            shutil.copytree(self.output_dir / 'include',
-                            self._config.sysroot / 'usr' / 'include',
-                            dirs_exist_ok=True, symlinks=True)
-
-            # Copy libraries into the NDK sysroot, and generate libc++.{a,so}
-            # linker scripts.
-            if not self._config.platform:
-                for name in ['libc++abi.a', 'libc++_shared.so', 'libc++_static.a']:
-                    shutil.copy2(self.output_dir / 'lib' / name, sysroot_lib / name)
-                with open(sysroot_lib / 'libc++.a', 'w') as out:
-                    out.write('INPUT(-lc++_static -lc++abi)\n')
-                with open(sysroot_lib / 'libc++.so', 'w') as out:
-                    out.write('INPUT(-lc++_shared)\n')
-
-            # Copy libraries into the platform sysroot.
-            if self._config.platform:
-                for name in ['libc++abi.a', 'libc++.so']:
-                    shutil.copy2(self.output_dir / 'lib' / name, sysroot_lib / name)
-
-        # Copy the output files to a directory structure for use with (a) Soong
-        # and (b) the NDK's checkbuild.py. Offer the experimental library in the
-        # NDK but omit it from the platform because we want to discourage
-        # platform developers from using unstable APIs.
-        if self._config.platform:
-            if self._is_noexcept:
-                kind = 'platform_noexcept'
-                libs = ['libc++abi_noexcept.a', 'libc++_static_noexcept.a', 'libc++demangle_noexcept.a']
-            else:
-                kind = 'platform'
-                libs = ['libc++abi.a', 'libc++_static.a', 'libc++.so', 'libc++demangle.a']
+        install_dir = self._config.sysroot / 'usr' / 'lib'
+
+        if super()._is_64bit():
+            src_path = self.output_dir / 'lib' / 'libc++abi.a'
+            shutil.copy2(src_path, install_dir / 'libc++abi.a')
         else:
-            assert not self._is_noexcept
-            kind = 'ndk'
-            libs = ['libc++abi.a', 'libc++_static.a', 'libc++_shared.so', 'libc++experimental.a']
-        if self._is_hwasan:
-            kind += '_hwasan'
-        dst_dir = self.output_toolchain.path / 'android_libc++' / kind / arch.value
-        dst_lib_dir = dst_dir / 'lib'
-        dst_lib_dir.mkdir(parents=True, exist_ok=True)
-        for name in libs:
-            shutil.copy2(self.output_dir / 'lib' / name, dst_lib_dir)
-        dst_inc_dir = dst_dir / 'include' / 'c++' / 'v1'
-        dst_inc_dir.mkdir(parents=True, exist_ok=True)
-        shutil.copy2(self.output_dir / 'include' / 'c++' / 'v1' / '__config_site', dst_inc_dir)
-
-
-class WinLibCxxBuilder(base_builders.LLVMRuntimeBuilder):
-    name = 'win-libcxx'
+            with (install_dir / 'libc++abi.so').open('w') as f:
+                f.write('INPUT(-lc++)')
+
+
+class LibCxxBuilder(base_builders.LLVMRuntimeBuilder):
+    name = 'libcxx'
     src_dir: Path = paths.LLVM_PATH / 'runtimes'
 
     @property
     def install_dir(self):
-        if self._config.target_arch == hosts.Arch.I386:
-            return paths.OUT_DIR / 'windows-libcxx-i686-install'
-        elif self._config.target_arch == hosts.Arch.X86_64:
-            return paths.OUT_DIR / 'windows-libcxx-x86-64-install'
-        else:
-            raise NotImplementedError()
+        return paths.OUT_DIR / 'windows-x86-64-install'
 
     @property
     def cmake_defines(self) -> Dict[str, str]:
         defines: Dict[str, str] = super().cmake_defines
-        defines['LLVM_ENABLE_RUNTIMES'] = 'libcxx;libcxxabi;libunwind'
-        defines['LLVM_ENABLE_PER_TARGET_RUNTIME_DIR'] = 'ON'
+        defines['LLVM_ENABLE_RUNTIMES'] ='libcxx;libcxxabi'
 
         defines['LIBCXX_ENABLE_STATIC_ABI_LIBRARY'] = 'ON'
         defines['LIBCXX_ENABLE_NEW_DELETE_DEFINITIONS'] = 'ON'
@@ -1285,10 +1062,6 @@ class WinLibCxxBuilder(base_builders.LLVMRuntimeBuilder):
         defines['LIBCXXABI_ENABLE_SHARED'] = 'OFF'
         defines['LIBCXX_ENABLE_EXPERIMENTAL_LIBRARY'] = 'OFF'
 
-        # Use static libunwinder for host builds.
-        defines['LIBCXXABI_USE_LLVM_UNWINDER'] = 'ON'
-        defines['LIBCXXABI_ENABLE_STATIC_UNWINDER'] = 'ON'
-
         if self.enable_assertions:
             defines['LIBCXX_ENABLE_ASSERTIONS'] = 'ON'
             defines['LIBCXXABI_ENABLE_ASSERTIONS'] = 'ON'
@@ -1298,68 +1071,31 @@ class WinLibCxxBuilder(base_builders.LLVMRuntimeBuilder):
     def install_config(self) -> None:
         super().install_config()
 
-        # The per-target directory uses '-w64-' instead of '-pc-'.
-        if self._config.target_arch == hosts.Arch.X86_64:
-            triple_dir = 'x86_64-w64-windows-gnu'
-        else:
-            triple_dir = 'i686-w64-windows-gnu'
-
-        win_install_dir = WindowsToolchainBuilder.install_dir
-
-        if self._config.target_arch == hosts.Arch.X86_64:
-            # Copy the x86-64 library and the non-target-specific headers to the sysroot. Clang
-            # doesn't automatically find __config_site in a per-triple include directory, so copy
-            # that header to the non-specific directory.
-            sysroot = self._config.sysroot
-            shutil.copy(self.install_dir / 'lib' / triple_dir / 'libc++.a', sysroot / 'lib')
-            shutil.copy(self.install_dir / 'lib' / triple_dir / 'libc++abi.a', sysroot / 'lib')
-            shutil.copytree(self.install_dir / 'include' / 'c++' / 'v1',
-                            sysroot / 'include' / 'c++' / 'v1', dirs_exist_ok=True)
-            shutil.copy(self.install_dir / 'include' / triple_dir / 'c++' / 'v1' / '__config_site',
-                        sysroot / 'include' / 'c++' / 'v1')
+        # Copy to sysroot in addition to install_dir
+        sysroot = self._config.sysroot
+        (sysroot / 'lib' / 'libc++.a').unlink()
+        (sysroot / 'lib' / 'libc++abi.a').unlink()
+        shutil.rmtree(sysroot / 'include' / 'c++' / 'v1')
 
-            # Copy the non-target-specific headers into the output Windows toolchain.
-            shutil.copytree(self.install_dir / 'include' / 'c++' / 'v1',
-                            win_install_dir / 'include' / 'c++' / 'v1',
-                            dirs_exist_ok=True)
-
-            # Copy the libraries into the output Windows toolchain.
-            # TODO: Maybe we don't need these, because there are per-triple libraries, but I'm not
-            # sure who might be using them.
-            lib_dir = win_install_dir / 'lib'
-            lib_dir.mkdir(parents=True, exist_ok=True)
-            shutil.copy(self.install_dir / 'lib' / triple_dir / 'libc++.a', lib_dir)
-            shutil.copy(self.install_dir / 'lib' / triple_dir / 'libc++abi.a', lib_dir)
-
-            # Place the x86-64 __config_site header into the non-target-specific include directory.
-            # TODO: Maybe we don't need this header either.
-            shutil.copy(self.install_dir / 'include' / triple_dir / 'c++' / 'v1' / '__config_site',
-                        win_install_dir / 'include' / 'c++' / 'v1')
-
-        # Copy the per-triple libraries and __config_site header to per-triple
-        # lib/include directories in both the generated Linux and Windows toolchains.
-        for host_dir in [win_install_dir, Stage2Builder.install_dir]:
-            lib_dir = host_dir / 'lib' / triple_dir
-            lib_dir.mkdir(parents=True, exist_ok=True)
-            shutil.copy(self.install_dir / 'lib' / triple_dir / 'libc++.a', lib_dir)
-            shutil.copy(self.install_dir / 'lib' / triple_dir / 'libc++abi.a', lib_dir)
-            include_dir = host_dir / 'include' / triple_dir / 'c++' / 'v1'
-            include_dir.mkdir(parents=True, exist_ok=True)
-            shutil.copy(self.install_dir / 'include' / triple_dir / 'c++' / 'v1' / '__config_site', include_dir)
+        shutil.copy(self.install_dir / 'lib' / 'libc++.a', sysroot / 'lib')
+        shutil.copy(self.install_dir / 'lib' / 'libc++abi.a', sysroot / 'lib')
+        shutil.copytree(self.install_dir / 'include' / 'c++' / 'v1',
+                        sysroot / 'include' / 'c++' / 'v1', dirs_exist_ok=True)
 
 
 class WindowsToolchainBuilder(base_builders.LLVMBuilder):
     name: str = 'windows-x86-64'
-    install_dir: Path = paths.OUT_DIR / 'windows-x86-64-install'
     toolchain_name: str = 'stage1'
     build_lldb: bool = True
-    lto: bool = False
-    profdata_file: Optional[Path] = None
 
     @property
     def _is_msvc(self) -> bool:
         return isinstance(self._config, configs.MSVCConfig)
 
+    @property
+    def install_dir(self) -> Path:
+        return paths.OUT_DIR / 'windows-x86-64-install'
+
     @property
     def llvm_targets(self) -> Set[str]:
         return constants.ANDROID_TARGETS
@@ -1385,15 +1121,14 @@ class WindowsToolchainBuilder(base_builders.LLVMBuilder):
         defines['LLVM_TOOL_OPENMP_BUILD'] = 'OFF'
         # Don't build tests for Windows.
         defines['LLVM_INCLUDE_TESTS'] = 'OFF'
-        defines['LLVM_NATIVE_TOOL_DIR'] = str(self.toolchain.build_path / 'bin')
-        if self.build_lldb:
-            defines['LLDB_PYTHON_RELATIVE_PATH'] = f'lib/python{paths._PYTHON_VER}/site-packages'
-            defines['LLDB_PYTHON_EXE_RELATIVE_PATH'] = f'python3'
-            defines['LLDB_PYTHON_EXT_SUFFIX'] = '.exe'
-        if self.lto:
-            defines['LLVM_ENABLE_LTO'] = 'Thin'
-        if self.profdata_file:
-            defines['LLVM_PROFDATA_FILE'] = str(self.profdata_file)
+
+        defines['LLVM_CONFIG_PATH'] = str(self.toolchain.build_path / 'bin' / 'llvm-config')
+        defines['LLVM_TABLEGEN'] = str(self.toolchain.build_path / 'bin' / 'llvm-tblgen')
+        defines['CLANG_TABLEGEN'] = str(self.toolchain.build_path / 'bin' / 'clang-tblgen')
+        defines['CLANG_PSEUDO_GEN'] = str(self.toolchain.build_path / 'bin' / 'clang-pseudo-gen')
+        defines['CLANG_TIDY_CONFUSABLE_CHARS_GEN'] = str(self.toolchain.build_path / 'bin' / 'clang-tidy-confusable-chars-gen')
+
+        defines['LLVM_ENABLE_PLUGINS'] = 'ON'
 
         defines['CMAKE_CXX_STANDARD'] = '17'
 
@@ -1425,9 +1160,6 @@ class WindowsToolchainBuilder(base_builders.LLVMBuilder):
         cflags.append('-DLZMA_API_STATIC')
         cflags.append('-DMS_WIN64')
         cflags.append(f'-I{paths.WIN_ZLIB_INCLUDE_PATH}')
-        if self.profdata_file:
-            cflags.append('-Wno-profile-instr-out-of-date')
-            cflags.append('-Wno-profile-instr-unprofiled')
         return cflags
 
     @property
@@ -1504,73 +1236,3 @@ class TsanBuilder(base_builders.LLVMRuntimeBuilder):
         dst_dir.mkdir(exist_ok=True)
         for tsan_lib in lib_dir.glob('*tsan*'):
             shutil.copy(tsan_lib, dst_dir)
-
-
-class LibSimpleperfReadElfBuilder(base_builders.LLVMRuntimeBuilder):
-    """ Build static llvm libraries for reading ELF files on both devices and hosts. It is used
-        by simpleperf.
-    """
-    name: str = 'libsimpleperf_readelf'
-    src_dir: Path = paths.LLVM_PATH / 'llvm'
-    config_list = [*configs.android_configs(platform=True),
-                   configs.LinuxMuslConfig(hosts.Arch.X86_64),
-                   configs.LinuxMuslConfig(hosts.Arch.AARCH64),
-                  ]
-
-    @property
-    def llvm_libs(self) -> List[str]:
-        output = utils.check_output([str(self.toolchain.path / 'bin' / 'llvm-config'),
-                                     '--libs', 'object', '--libnames', '--link-static'])
-        return output.strip().split()
-
-    ninja_targets: List[str] = llvm_libs
-    target_libname: str = 'libsimpleperf_readelf.a'
-
-    @property
-    def cflags(self) -> List[str]:
-        cflags = super().cflags
-        # The build system will add '-stdlib=libc++' automatically. Since we
-        # have -nostdinc++ here, -stdlib is useless. Adds a flag to avoid the
-        # warnings.
-        cflags.append('-Wno-unused-command-line-argument')
-        return cflags
-
-    @property
-    def cmake_defines(self) -> Dict[str, str]:
-        defines = super().cmake_defines
-        defines['LLVM_NATIVE_TOOL_DIR'] = str(self.toolchain.build_path / 'bin')
-        return defines
-
-    @property
-    def install_dir(self) -> Path:
-        if self._config.target_os ==  hosts.Host.Windows:
-            return self.output_toolchain.path / 'lib' / 'x86_64-w64-windows-gnu'
-        if self._config.target_os == hosts.Host.Linux and self._config.is_musl:
-            return self.output_resource_dir / self._config.llvm_triple / 'lib'
-        return super().install_dir
-
-    def install_config(self) -> None:
-        self.build_readelf_lib(self.output_dir / 'lib', self.install_dir)
-
-    def build_readelf_lib(self, llvm_lib_dir: Path, out_dir: Path, is_darwin_lib: bool = False):
-        llvm_lib_dir = llvm_lib_dir.absolute()
-        out_dir = out_dir.absolute()
-        out_dir.mkdir(parents=True, exist_ok=True)
-        out_file = out_dir / self.target_libname
-        out_file.unlink(missing_ok=True)
-        if is_darwin_lib:
-            utils.check_call([str(self.toolchain.path / 'bin' / 'llvm-libtool-darwin'),
-                              '--static', '-o', str(out_file)] + self.llvm_libs, cwd=llvm_lib_dir)
-        else:
-            with tempfile.TemporaryDirectory(dir=paths.OUT_DIR) as tmp_dirname:
-                tmp_dir = Path(tmp_dirname).absolute()
-                for name in self.llvm_libs:
-                    lib_path = llvm_lib_dir / name
-                    assert lib_path.is_file(), f'{lib_path} not found'
-                    # The libraries can have object files with the same name. To avoid conflict,
-                    # extract each library into a distinct directory.
-                    extract_dir = tmp_dir / name[:-2]
-                    extract_dir.mkdir()
-                    utils.check_call([str(self.toolchain.ar), '-x', str(lib_path)], cwd=extract_dir)
-                utils.check_call(f'{self.toolchain.ar} -cqs {out_file} */*', cwd=tmp_dir,
-                                 shell=True)
diff --git a/cherrypick_cl.py b/cherrypick_cl.py
index 41a0b6d..94d42d9 100755
--- a/cherrypick_cl.py
+++ b/cherrypick_cl.py
@@ -18,18 +18,13 @@
 from __future__ import annotations
 import argparse
 import collections
-import copy
 import dataclasses
 from dataclasses import dataclass
 import json
-import logging
 import math
-import os
 from pathlib import Path
 import re
-import sys
 from typing import Any, Dict, List, Optional, Tuple
-import urllib.request
 
 from android_version import get_svn_revision_number
 from merge_from_upstream import fetch_upstream, sha_to_revision
@@ -42,7 +37,6 @@ def parse_args():
     parser = argparse.ArgumentParser(description="Cherry pick upstream LLVM patches.",
                                      formatter_class=argparse.ArgumentDefaultsHelpFormatter)
     parser.add_argument('--sha', nargs='+', help='sha of patches to cherry pick')
-    parser.add_argument('--pr', help='Cherry pick from a GitHub PR, e.g., 84422')
     parser.add_argument(
         '--start-version', default='llvm',
         help="""svn revision to start applying patches. 'llvm' can also be used.""")
@@ -51,8 +45,6 @@ def parse_args():
     parser.add_argument('--create-cl', action='store_true', help='create a CL')
     parser.add_argument('--bug', help='bug to reference in CLs created (if any)')
     parser.add_argument('--reason', help='issue/reason to mention in CL subject line')
-    parser.add_argument('--verbose', help='Enable logging')
-    parser.add_argument('--patch-file', help='Use custom patch file')
     args = parser.parse_args()
     return args
 
@@ -97,14 +89,6 @@ class PatchItem:
         assert m, self.rel_patch_path
         return m.group(1)
 
-    @property
-    def pr_link(self) -> str:
-        m = next(re.match(r'Pull Request: (.+)', line)
-            for line in open(f'patches/{self.rel_patch_path}'))
-
-        assert m, f'No PR link found in: {self.rel_patch_path}'
-        return m.group(1)
-
     @property
     def end_version(self) -> Optional[int]:
         return self.version_range.get('until', None)
@@ -151,7 +135,8 @@ class PatchList(list):
             json.dump(array, fh, indent=4, separators=(',', ': '), sort_keys=True)
             fh.write('\n')
 
-def generate_patch_files(sha_list: List[str], start_version: int, patch_list: PatchList) -> PatchList:
+
+def generate_patch_files(sha_list: List[str], start_version: int) -> PatchList:
     """ generate upstream cherry-pick patch files """
     upstream_dir = paths.TOOLCHAIN_LLVM_PATH
     fetch_upstream()
@@ -159,10 +144,8 @@ def generate_patch_files(sha_list: List[str], start_version: int, patch_list: Pa
     for sha in sha_list:
         if len(sha) < 40:
             sha = get_full_sha(upstream_dir, sha)
-        version = find_version(sha, patch_list, start_version)
-        version_name = '' if version == 1 else f'-v{version}'
-        rel_patch_path = f'cherry/{sha}' + version_name + '.patch'
-        file_path = paths.SCRIPTS_DIR / 'patches' / rel_patch_path
+        file_path = paths.SCRIPTS_DIR / 'patches' / 'cherry' / f'{sha}.patch'
+        assert not file_path.exists(), f'{file_path} already exists'
         with open(file_path, 'w') as fh:
             check_call(f'git format-patch -1 {sha} --stdout',
                        stdout=fh, shell=True, cwd=upstream_dir)
@@ -171,6 +154,7 @@ def generate_patch_files(sha_list: List[str], start_version: int, patch_list: Pa
             f'git log -n1 --format=%s {sha}', shell=True, cwd=upstream_dir)
         info: Optional[List[str]] = []
         title = '[UPSTREAM] ' + commit_subject.strip()
+        rel_patch_path = f'cherry/{sha}.patch'
         end_version = sha_to_revision(sha)
         metadata = { 'info': info, 'title': title }
         platforms = ['android']
@@ -181,178 +165,50 @@ def generate_patch_files(sha_list: List[str], start_version: int, patch_list: Pa
         result.append(PatchItem(metadata, platforms, rel_patch_path, version_range))
     return result
 
-def add_new_patch_for_sha(patch_file: Path, sha: str, start_version: int, patch_list: PatchList) -> PatchList:
-    """ add patch files for sha"""
-    upstream_dir = paths.TOOLCHAIN_LLVM_PATH
-    result = []
-    assert len(sha) >= 40, f'the length of {sha} is {len(sha)} and it is shorter than 40'
-    version = find_version(sha, patch_list, start_version)
-    version_name = '' if version == 1 else f'-v{version}'
-    rel_patch_path = f'cherry/{sha}' + version_name + '.patch'
-    file_path = paths.SCRIPTS_DIR / 'patches' / rel_patch_path
-    with open(patch_file, 'r') as source, open(file_path, 'w') as dest:
-        for line in source:
-           dest.write(line)
-
-    commit_subject = check_output(
-        f'git log -n1 --format=%s {sha}', shell=True, cwd=upstream_dir)
-    info: Optional[List[str]] = []
-    title = '[UPSTREAM] ' + commit_subject.strip()
-    end_version = sha_to_revision(sha)
-    metadata = { 'info': info, 'title': title }
-    platforms = ['android']
-    version_range: Dict[str, Optional[int]] = {
-        'from': start_version,
-        'until': end_version,
-    }
-    result.append(PatchItem(metadata, platforms, rel_patch_path, version_range))
-    return result
 
 def get_full_sha(upstream_dir: Path, short_sha: str) -> str:
     return check_output(['git', 'rev-parse', short_sha], cwd=upstream_dir).strip()
 
 
-def create_cl(new_patches: PatchList, reason: str, bug: Optional[str], cherry: bool):
-    file_list = [
-        str(paths.SCRIPTS_DIR / 'patches' / p.rel_patch_path) for p in new_patches
-    ]
-
-    file_list += ['patches/PATCHES.json']
+def create_cl(new_patches: PatchList, reason: str, bug: Optional[str]):
+    file_list = [p.rel_patch_path for p in new_patches] + ['PATCHES.json']
+    file_list = [str(paths.SCRIPTS_DIR / 'patches' / f) for f in file_list]
     check_call(['git', 'add'] + file_list)
 
     subject = f'[patches] Cherry pick CLS for: {reason}'
     commit_lines = [subject, '']
-    script = os.path.basename(sys.argv[0])
-    argv_deepcopy = copy.deepcopy(sys.argv[1:])
-    for i in range(len(argv_deepcopy)):
-        element = argv_deepcopy[i]
-        if element.startswith('--reason'):
-            del argv_deepcopy[i]
-            if element == '--reason':
-              del argv_deepcopy[i]
-            break
-
-    for patch in new_patches:
-        if cherry: # Add SHA and title for each cherry-pick.
-            sha = patch.sha[:11]
-            subject = patch.metadata['title']
-            if subject.startswith('[UPSTREAM] '):
-                subject = subject[len('[UPSTREAM] '):]
-            commit_line = sha + ' ' + subject
-        else: # Add link to differential revision.
-            commit_line = patch.pr_link
-        commit_lines.append(commit_line)
-    commit_lines.append('')
-
-    args = ' '.join(argv_deepcopy)
-    auto_msg = f'This change is generated automatically by the script:\n  {script} {args}'
-    commit_lines += [auto_msg, '']
     if bug:
         if bug.isnumeric():
             commit_lines += [f'Bug: http://b/{bug}', '']
         else:
             commit_lines += [f'Bug: {bug}', '']
-
+    for patch in new_patches:
+        sha = patch.sha[:11]
+        subject = patch.metadata['title']
+        if subject.startswith('[UPSTREAM] '):
+            subject = subject[len('[UPSTREAM] '):]
+        commit_lines.append(sha + ' ' + subject)
     commit_lines += ['', 'Test: N/A']
     check_call(['git', 'commit', '-m', '\n'.join(commit_lines)])
 
-def create_patch(pr, start_version) -> PatchList:
-    pr_url=f'https://api.github.com/repos/llvm/llvm-project/pulls/{pr}'
-    patch_url_req = urllib.request.Request(f'https://github.com/llvm/llvm-project/pull/{pr}.diff',
-                                        method="HEAD")
-    patch_url = urllib.request.urlopen(patch_url_req).url
-
-    # TODO: Add commit body and author details as well.
-    with urllib.request.urlopen(pr_url) as response:
-      data = json.load(response)
-    title=data['title']
-    assert title, f'Title not found for {pr}'
-    print(f'Creating a patch for {title}')
-    file_name=f'{pr}.patch'
-    abs_file_name= paths.SCRIPTS_DIR / 'patches' / file_name
-    # Download the file from `patch_url` and save in `abs_file_name`:
-    urllib.request.urlretrieve(patch_url, abs_file_name)
-    # Add link to Differential Revision at the beginning of the file
-    patch_prefix=f'Pull Request: {patch_url}\nSubject: {title}'
-    with open(abs_file_name, 'r+') as f:
-        content = f.read()
-        f.seek(0, 0)
-        f.write(patch_prefix + '\n\n---\n' + content)
-
-    # Extend the PATCHES.json
-    result = PatchList()
-    info: Optional[List[str]] = []
-    rel_patch_path = f'{file_name}'
-    end_version = None
-    metadata = { 'info': info, 'title': title }
-    platforms = ['android']
-    version_range: Dict[str, Optional[int]] = {
-       'from': start_version,
-       'until': end_version,
-    }
-    result.append(PatchItem(metadata, platforms, rel_patch_path, version_range))
-    return result
-
-def find_version(sha, patch_list, start_version) -> int:
-    """ Return the next version for the given SHA and update end_revision if needed"""
-    target = f'cherry/{sha}'
-    last_idx = -1
-    version = 1
-    name = ''
-
-    # Find the latest version
-    for i, item in enumerate(patch_list):
-        if item.rel_patch_path.startswith(target):
-           last_idx = i
-           name = item.rel_patch_path.removesuffix('.patch')
-
-    # If this patch is not new, update the end_revision for Vn
-    if last_idx != -1:
-       patch_list[last_idx].version_range['until'] = start_version
-       if name == target:
-        return 2
-       prefix = target + f'-v'
-       version = int(name.removeprefix(prefix)) + 1
-
-    return version
 
 def main():
     args = parse_args()
-    level = logging.DEBUG if args.verbose else logging.INFO
-    logging.basicConfig(level=level)
     patch_list = PatchList.load_from_file()
-
-    assert not (bool(args.sha) and bool(args.pr)), (
-        'Only one of cherry-pick or patch supported.'
-    )
-    if args.pr:
+    if args.sha:
         start_version = parse_start_version(args.start_version)
-        new_patches = create_patch(args.pr, start_version)
+        new_patches = generate_patch_files(args.sha, start_version)
         patch_list.extend(new_patches)
-    elif args.sha:
-        start_version = parse_start_version(args.start_version)
-        if args.patch_file:
-            assert len(args.sha) == 1,  f'error: --patch-file only requires 1 sha, but the size of sha list is {len(args.sha)}'
-            new_patches = add_new_patch_for_sha(args.patch_file, args.sha[0], start_version, patch_list)
-        else:
-            new_patches = generate_patch_files(args.sha, start_version, patch_list)
-        patch_list.extend(new_patches)
-
     patch_list.sort()
     patch_list.save_to_file()
-
     if args.verify_merge:
-        print('Verifying merge...')
-        print('Verifying merge with patch ...')
+        print('verify merge...')
         source_manager.setup_sources()
-        print('Verifying merge with git am ...')
-        source_manager.setup_sources(git_am=True)
     if args.create_cl:
         if not args.reason:
             print('error: --create-cl requires --reason')
             exit(1)
-        cherry = True if args.sha else False
-        create_cl(new_patches, args.reason, args.bug, cherry)
+        create_cl(new_patches, args.reason, args.bug)
 
 
 if __name__ == '__main__':
diff --git a/cherrypick_cl_hook.py b/cherrypick_cl_hook.py
deleted file mode 100755
index 90ff4d3..0000000
--- a/cherrypick_cl_hook.py
+++ /dev/null
@@ -1,38 +0,0 @@
-#!/usr/bin/env python3
-import argparse
-import sys
-
-def main():
-    parser = argparse.ArgumentParser(
-        description='Check if the cherrypick_cl.py script has been run')
-    parser.add_argument('commit_msg', type=str, help='commit message')
-    parser.add_argument('commit_files', type=str, nargs='*',
-                       help='files changed in the commit')
-    args = parser.parse_args()
-
-    commit_msg = args.commit_msg
-    commit_files = args.commit_files
-
-    patch_list = []
-    for file in commit_files:
-        if file.endswith('.patch'):
-            patch_list.append(file)
-
-    if len(patch_list) == 0:
-        return 0
-
-    for line in commit_msg.splitlines():
-        line = line.strip()
-        if line.startswith("This change is generated automatically by the script"):
-            return 0
-
-    print('This change seems to manually add/change the following patch files:')
-    for file in patch_list:
-        print(file)
-    print('Please run llvm_android/cherrypick_cl.py script to cherry pick changes instead.')
-
-    return 1
-
-if __name__ == '__main__':
-    exit_code = main()
-    sys.exit(exit_code)
diff --git a/configs.py b/configs.py
old mode 100644
new mode 100755
index e44450c..37c9045
--- a/configs.py
+++ b/configs.py
@@ -30,7 +30,7 @@ class Config:
 
     name: str
     target_os: hosts.Host
-    target_arch: hosts.Arch = hosts.Arch.X86_64
+    target_arch: hosts.Arch = hosts.Arch.AARCH64
     sysroot: Optional[Path] = None
 
     """Additional config data that a builder can specify."""
@@ -142,11 +142,6 @@ class BaremetalConfig(_BaseConfig):
         cflags.append(f'--target={self.llvm_triple}')
         return cflags
 
-    @property
-    def output_suffix(self) -> str:
-        """The suffix of output directory name."""
-        return f'-{self.target_arch.value}-baremetal'
-
 
 class BaremetalAArch64Config(BaremetalConfig):
     """Configuration for baremetal targets."""
@@ -158,86 +153,6 @@ class BaremetalAArch64Config(BaremetalConfig):
         return 'aarch64-elf'
 
 
-class BaremetalArmMultilibConfig(BaremetalConfig):
-    """Configuration for baremetal ARM multilib targets."""
-
-    target_arch: hosts.Arch = hosts.Arch.ARM
-
-    @property
-    def multilib_name(self) -> str:
-        """The name of the multilib variant."""
-        raise NotImplementedError()
-
-    @property
-    def output_suffix(self) -> str:
-        """The suffix of output directory name."""
-        return f'-{self.multilib_name}-baremetal'
-
-
-class BaremetalArmv6MConfig(BaremetalArmMultilibConfig):
-    """Configuration for baremetal Armv6-M target."""
-
-    @property
-    def llvm_triple(self) -> str:
-        return 'armv6m-none-eabi'
-
-    @property
-    def cflags(self) -> List[str]:
-        cflags = super().cflags
-        cflags.append('-march=armv6-m')
-        cflags.append('-mfloat-abi=soft')
-        return cflags
-
-    @property
-    def multilib_name(self) -> str:
-        return 'armv6-m'
-
-
-class BaremetalArmv8MBaseConfig(BaremetalArmMultilibConfig):
-    """Configuration for baremetal Armv8-M baseline target."""
-
-    @property
-    def llvm_triple(self) -> str:
-        return 'armv8m.base-none-eabi'
-
-    @property
-    def cflags(self) -> List[str]:
-        cflags = super().cflags
-        cflags.append('-march=armv8-m.base')
-        cflags.append('-mfloat-abi=soft')
-        return cflags
-
-    @property
-    def multilib_name(self) -> str:
-        return 'armv8-m.base'
-
-
-class BaremetalArmv81MMainConfig(BaremetalArmMultilibConfig):
-    """Configuration for baremetal Armv8.1-M mainline target."""
-
-    def __init__(self, fpu: hosts.Armv81MMainFpu):
-        self.fpu = fpu
-
-    @property
-    def llvm_triple(self) -> str:
-        if self.fpu == hosts.Armv81MMainFpu.NONE:
-            return 'armv8.1m.main-none-eabi'
-        else:
-            return 'armv8.1m.main-none-eabihf'
-
-    @property
-    def cflags(self) -> List[str]:
-        cflags = super().cflags
-        cflags.append('-march=armv8.1-m.main')
-        cflags.append(f'-mfpu={self.fpu.llvm_fpu}')
-        cflags.append(f'-mfloat-abi={self.fpu.llvm_float_abi}')
-        return cflags
-
-    @property
-    def multilib_name(self) -> str:
-        return f'armv8.1-m.main+{self.fpu.value}'
-
-
 class DarwinConfig(_BaseConfig):
     """Configuration for Darwin targets."""
 
@@ -248,7 +163,7 @@ class DarwinConfig(_BaseConfig):
     @property
     def cflags(self) -> List[str]:
         cflags = super().cflags
-        # Fails if an API used is newer than what specified in -mmacos-version-min.
+        # Fails if an API used is newer than what specified in -mmacosx-version-min.
         cflags.append('-Werror=unguarded-availability')
         return cflags
 
@@ -295,7 +210,7 @@ class LinuxConfig(_GccConfig):
     """Configuration for Linux targets."""
 
     target_os: hosts.Host = hosts.Host.Linux
-    sysroot: Optional[Path] = (paths.GCC_ROOT / 'host' / 'x86_64-linux-glibc2.17-4.8' / 'sysroot')
+    sysroot: Optional[Path] = None
     gcc_root: Path = (paths.GCC_ROOT / 'host' / 'x86_64-linux-glibc2.17-4.8')
     gcc_triple: str = 'x86_64-linux'
     gcc_ver: str = '4.8.3'
@@ -304,14 +219,20 @@ class LinuxConfig(_GccConfig):
 
     @property
     def llvm_triple(self) -> str:
-        return 'i386-unknown-linux-gnu' if self.is_32_bit else 'x86_64-unknown-linux-gnu'
+        return 'i386-unknown-linux-gnu' if self.is_32_bit else 'arm64-unknown-linux-gnu'
 
     @property
-    def cflags(self) -> List[str]:
+    def cflagsS(self) -> List[str]:
         cflags = super().cflags
         if self.is_32_bit and not self.is_musl:
+            # compiler-rt/lib/gwp_asan uses PRIu64 and similar format-specifier macros.
+            # Add __STDC_FORMAT_MACROS so their definition gets included from
+            # inttypes.h.  This explicit flag is only needed here.  64-bit host runtimes
+            # are built in stage1/stage2 and get it from the LLVM CMake configuration.
+            # These are defined unconditionaly in bionic and newer glibc
+            # (https://sourceware.org/git/gitweb.cgi?p=glibc.git;h=1ef74943ce2f114c78b215af57c2ccc72ccdb0b7)
+            cflags.append('-D__STDC_FORMAT_MACROS')
             cflags.append('-march=i686')
-        return cflags
 
 
     @property
@@ -348,7 +269,6 @@ class LinuxMuslConfig(LinuxConfig):
         cflags = super().cflags + [
                 f'--target={self.llvm_triple}',
                 '-D_LIBCPP_HAS_MUSL_LIBC',
-                '-D_LARGEFILE64_SOURCE=1',
                 # gcc does this automatically and glibc includes it in features.h
                 # Neither clang nor musl include it, so add it here.  Otherwise
                 # libedit fails with error: wchar_t must store ISO 10646 characters
@@ -404,9 +324,6 @@ class LinuxMuslConfig(LinuxConfig):
         defines['LIBCXX_USE_COMPILER_RT'] = 'TRUE'
         defines['LIBCXXABI_USE_COMPILER_RT'] = 'TRUE'
         defines['LIBUNWIND_USE_COMPILER_RT'] = 'TRUE'
-        # clang generates call to builtin functions when building
-        # compiler-rt for musl.  Allow use of the builtins library.
-        defines['COMPILER_RT_USE_BUILTINS_LIBRARY'] = 'TRUE'
 
         # The musl sysroots contain empty libdl.a, libpthread.a and librt.a to
         # satisfy the parts of the LLVM build that hardcode -lpthread, etc.,
@@ -436,8 +353,6 @@ class LinuxMuslHostConfig(LinuxMuslConfig):
         return env
 
 
-# TODO: We should kill off 32-bit Windows support, but we still need it because
-# we have a single 32-bit DLL for USB stuff for adb.exe/fastboot.exe.
 class MinGWConfig(_GccConfig):
     """Configuration for MinGW targets."""
 
@@ -445,22 +360,11 @@ class MinGWConfig(_GccConfig):
     gcc_root: Path = (paths.GCC_ROOT / 'host' / 'x86_64-w64-mingw32-4.8')
     gcc_triple: str = 'x86_64-w64-mingw32'
     gcc_ver: str = '4.8.3'
-
-    @property
-    def target_arch(self) -> hosts.Arch:
-        return hosts.Arch.I386 if self.is_32_bit else hosts.Arch.X86_64
+    sysroot: Optional[Path] = paths.SYSROOTS / gcc_triple
 
     @property
     def llvm_triple(self) -> str:
-        return 'i686-pc-windows-gnu' if self.is_32_bit else 'x86_64-pc-windows-gnu'
-
-    @property
-    def sysroot(self) -> Optional[Path]:
-        return paths.SYSROOTS / ('i686-w64-mingw32' if self.is_32_bit else 'x86_64-w64-mingw32')
-
-    @property
-    def output_suffix(self) -> str:
-        return '-windows32' if self.is_32_bit else '-windows'
+        return 'x86_64-pc-windows-gnu'
 
     @property
     def cflags(self) -> List[str]:
@@ -471,18 +375,15 @@ class MinGWConfig(_GccConfig):
         cflags.append('-D_WIN32_WINNT=0x0600')
         cflags.append('-DWINVER=0x0600')
         cflags.append('-D__MSVCRT_VERSION__=0x1400')
-        if self.target_arch == hosts.Arch.I386:
-            cflags.append('-fsjlj-exceptions')
         return cflags
 
     @property
     def ldflags(self) -> List[str]:
         ldflags = super().ldflags
-        if not self.is_32_bit:
-            ldflags.append('-Wl,--dynamicbase')
-            ldflags.append('-Wl,--nxcompat')
-            ldflags.append('-Wl,--high-entropy-va')
-            ldflags.append('-Wl,--Xlink=-Brepro')
+        ldflags.append('-Wl,--dynamicbase')
+        ldflags.append('-Wl,--nxcompat')
+        ldflags.append('-Wl,--high-entropy-va')
+        ldflags.append('-Wl,--Xlink=-Brepro')
         return ldflags
 
     @property
@@ -564,18 +465,13 @@ class AndroidConfig(_BaseConfig):
 
     static: bool = False
     platform: bool = False
+    suppress_libcxx_headers: bool = False
     override_api_level: Optional[int] = None
 
     @property
     def base_llvm_triple(self) -> str:
         """Get base LLVM triple (without API level)."""
-        if self.target_arch == hosts.Arch.ARM:
-            # AndroidARMConfig specifies a "-march=armv7-a" cflag, but including
-            # armv7a in the triple is necessary to build libc++ tests correctly,
-            # which do not allow adding arbitrary cflags.
-            return 'armv7a-linux-androideabi'
-        else:
-            return f'{self.target_arch.llvm_arch}-linux-android'
+        return f'{self.target_arch.llvm_arch}-linux-android'
 
     @property
     def llvm_triple(self) -> str:
@@ -616,9 +512,6 @@ class AndroidConfig(_BaseConfig):
         ldflags.append('-pie')
         if self.static:
             ldflags.append('-static')
-        if (self.target_arch == hosts.Arch.X86_64 or
-                self.target_arch == hosts.Arch.AARCH64):
-            ldflags.append('-Wl,-z,max-page-size=16384')
         return ldflags
 
     @property
@@ -632,21 +525,45 @@ class AndroidConfig(_BaseConfig):
 
         cflags.append('-ffunction-sections')
         cflags.append('-fdata-sections')
-        if (self.target_arch == hosts.Arch.X86_64 or
-                self.target_arch == hosts.Arch.AARCH64):
-            cflags.append('-D__BIONIC_NO_PAGE_SIZE_MACRO')
         return cflags
 
+    @property
+    def _libcxx_header_dirs(self) -> List[Path]:
+        # For the NDK, the sysroot has the C++ headers.
+        assert self.platform
+        if self.suppress_libcxx_headers:
+            return []
+        # <prebuilts>/include/c++/v1 includes the cxxabi headers
+        return [
+            paths.CLANG_PREBUILT_LIBCXX_HEADERS,
+            # The platform sysroot also has Bionic headers from an NDK release,
+            # but override them with the current headers.
+            paths.BIONIC_HEADERS,
+            paths.BIONIC_KERNEL_HEADERS,
+        ]
+
+    @property
+    def cxxflags(self) -> List[str]:
+        cxxflags = super().cxxflags
+        if self.platform:
+            # For the NDK, the sysroot has the C++ headers, but for the
+            # platform, we need to add the headers manually.
+            cxxflags.append('-nostdinc++')
+            cxxflags.extend(f'-isystem {d}' for d in self._libcxx_header_dirs)
+        return cxxflags
+
     @property
     def api_level(self) -> int:
         if self.override_api_level:
             return self.override_api_level
         if self.target_arch == hosts.Arch.RISCV64:
-            return 35
+            return 10000
         if self.static or self.platform:
-            # Set API level for platform to to 30 since these runtimes can be
+            # Set API level for platform to to 29 since these runtimes can be
             # used for apexes targeting that API level.
-            return 30
+            return 29
+        if self.target_arch in [hosts.Arch.ARM, hosts.Arch.I386]:
+            return 19
         return 21
 
     def __str__(self) -> str:
@@ -737,6 +654,7 @@ def host_32bit_config(musl: bool=False) -> Config:
 
 def android_configs(platform: bool=True,
                     static: bool=False,
+                    suppress_libcxx_headers: bool=False,
                     extra_config=None) -> List[Config]:
     """Returns a list of configs for android builds."""
     configs = [
@@ -744,11 +662,14 @@ def android_configs(platform: bool=True,
         AndroidAArch64Config(),
         AndroidI386Config(),
         AndroidX64Config(),
-        AndroidRiscv64Config(),
     ]
+    # There is no NDK for riscv64, only include it in platform configs.
+    if platform:
+        configs.append(AndroidRiscv64Config())
     for config in configs:
         config.static = static
         config.platform = platform
+        config.suppress_libcxx_headers = suppress_libcxx_headers
         config.extra_config = extra_config
     # List is not covariant. Explicit convert is required to make it List[Config].
     return list(configs)
diff --git a/constants.py b/constants.py
old mode 100644
new mode 100755
index 08d313b..a424533
--- a/constants.py
+++ b/constants.py
@@ -21,10 +21,10 @@ from typing import Set
 MAC_MIN_VERSION: str = '10.14'
 
 # This is the baseline stable version of Clang to start our stage-1 build.
-CLANG_PREBUILT_VERSION: str = 'clang-r522817'
+CLANG_PREBUILT_VERSION: str = 'clang-r487747'
 
 # This is the ndk version used to build runtimes.
-NDK_VERSION: str = 'r27'
+NDK_VERSION: str = 'r25'
 
 # Targets for host.
 HOST_TARGETS: Set[str] = set(['X86'])
diff --git a/do_build.py b/do_build.py
index bb9a63a..82e53a9 100755
--- a/do_build.py
+++ b/do_build.py
@@ -24,7 +24,7 @@ import os
 import shutil
 import sys
 import textwrap
-from typing import List, Optional, Set
+from typing import List, NamedTuple, Optional, Set, Tuple
 import re
 
 import android_version
@@ -35,10 +35,10 @@ import configs
 import hosts
 import paths
 import source_manager
-import toolchain_errors
 import timer
 import toolchains
 import utils
+from version import Version
 import win_sdk
 
 def logger():
@@ -51,36 +51,38 @@ def set_default_toolchain(toolchain: toolchains.Toolchain) -> None:
     Builder.toolchain = toolchain
 
 
-def extract_pgo_profile() -> Path:
+class Profile(NamedTuple):
+    """ Optimization profiles including PGO and BOLT. """
+    PgoProfile: Optional[Path]
+    ClangBoltProfile: Optional[Path]
+
+
+def extract_profiles() -> Profile:
     pgo_profdata_tar = paths.pgo_profdata_tar()
     if not pgo_profdata_tar:
-        raise RuntimeError(f'{pgo_profdata_tar} does not exist')
-    utils.extract_tarball(paths.OUT_DIR, pgo_profdata_tar)
+        return Profile(None, None)
+    utils.check_call(['tar', '-jxC', str(paths.OUT_DIR), '-f', str(pgo_profdata_tar)])
     profdata_file = paths.OUT_DIR / paths.pgo_profdata_filename()
     if not profdata_file.exists():
-        raise RuntimeError(f'{profdata_file} does not exist')
-    return profdata_file
-
+        logger().info('PGO profdata missing')
+        return Profile(None, None)
 
-def extract_bolt_profile() -> Path:
     bolt_fdata_tar = paths.bolt_fdata_tar()
     if not bolt_fdata_tar:
-        raise RuntimeError(f'{bolt_fdata_tar} does not exist')
-    utils.extract_tarball(paths.OUT_DIR, bolt_fdata_tar)
+        return Profile(profdata_file, None)
+    utils.check_call(['tar', '-jxC', str(paths.OUT_DIR), '-f', str(bolt_fdata_tar)])
     clang_bolt_fdata_file = paths.OUT_DIR / 'clang.fdata'
     if not clang_bolt_fdata_file.exists():
-        raise RuntimeError(f'{clang_bolt_fdata_file} does not exist')
-    return clang_bolt_fdata_file
+        logger().info('Clang BOLT profile missing')
+        return Profile(profdata_file, None)
+
+    return Profile(profdata_file, clang_bolt_fdata_file)
 
 
 def build_llvm_for_windows(enable_assertions: bool,
-                           enable_lto: bool,
-                           profdata_file: Optional[Path],
                            build_name: str,
                            build_lldb: bool,
-                           swig_builder: Optional[builders.SwigBuilder],
-                           full_build: bool,
-                           build_simpleperf_readelf: bool):
+                           swig_builder: Optional[builders.SwigBuilder]):
     config_list: List[configs.Config]
     if win_sdk.is_enabled():
         config_list = [configs.MSVCConfig()]
@@ -93,56 +95,37 @@ def build_llvm_for_windows(enable_assertions: bool,
 
     if not win_sdk.is_enabled():
         # Build and install libcxxabi and libcxx and use them to build Clang.
-        libcxx_builder = builders.WinLibCxxBuilder(config_list)
+        libcxx_builder = builders.LibCxxBuilder(config_list)
         libcxx_builder.enable_assertions = enable_assertions
         libcxx_builder.build()
 
-        # Also build a 32-bit Windows libc++ for use with the platform
-        # adb/fastboot.
-        libcxx32_builder = builders.WinLibCxxBuilder([configs.MinGWConfig(is_32_bit=True)])
-        libcxx32_builder.enable_assertions = enable_assertions
-        libcxx32_builder.build()
+    libzstd_builder = builders.ZstdBuilder(config_list)
+    libzstd_builder.build()
+    win_builder.libzstd = libzstd_builder
 
     lldb_bins: Set[str] = set()
+    libxml2_builder = builders.LibXml2Builder(config_list)
+    libxml2_builder.build()
+    win_builder.libxml2 = libxml2_builder
+    for lib in libxml2_builder.install_libraries:
+        lldb_bins.add(lib.name)
 
-    if full_build:
-        libzstd_builder = builders.ZstdBuilder(config_list)
-        libzstd_builder.build()
-        win_builder.libzstd = libzstd_builder
+    win_builder.build_lldb = build_lldb
+    if build_lldb:
+        assert swig_builder is not None
+        win_builder.libedit = None
+        win_builder.swig_executable = swig_builder.install_dir / 'bin' / 'swig'
 
-        libxml2_builder = builders.LibXml2Builder(config_list)
-        libxml2_builder.build()
-        win_builder.libxml2 = libxml2_builder
-        for lib in libxml2_builder.install_libraries:
-            lldb_bins.add(lib.name)
+        xz_builder = builders.XzBuilder(config_list)
+        xz_builder.build()
+        win_builder.liblzma = xz_builder
 
-        win_builder.build_lldb = build_lldb
-        if build_lldb:
-            assert swig_builder is not None
-            win_builder.libedit = None
-            win_builder.swig_executable = swig_builder.install_dir / 'bin' / 'swig'
+        lldb_bins.add('liblldb.dll')
 
-            xz_builder = builders.XzBuilder(config_list)
-            xz_builder.build()
-            win_builder.liblzma = xz_builder
-
-            lldb_bins.add('liblldb.dll')
-
-        win_builder.build_name = build_name
-        win_builder.svn_revision = android_version.get_svn_revision()
-        win_builder.enable_assertions = enable_assertions
-        win_builder.lto = enable_lto
-        win_builder.build()
-
-    if build_simpleperf_readelf:
-        libsimpleperf_readelf_builder = builders.LibSimpleperfReadElfBuilder(config_list)
-        if full_build:
-            # The libs have been built in win_builder.
-            libsimpleperf_readelf_builder.build_readelf_lib(
-                win_builder.output_dir / 'lib', libsimpleperf_readelf_builder.install_dir)
-        else:
-            libsimpleperf_readelf_builder.enable_assertions = enable_assertions
-            libsimpleperf_readelf_builder.build()
+    win_builder.build_name = build_name
+    win_builder.svn_revision = android_version.get_svn_revision()
+    win_builder.enable_assertions = enable_assertions
+    win_builder.build()
 
     return (win_builder, lldb_bins)
 
@@ -202,9 +185,8 @@ def build_runtimes(build_lldb_server: bool,
     builders.DeviceSysrootsBuilder().build()
     builders.BuiltinsBuilder().build()
     builders.LibUnwindBuilder().build()
-    builders.DeviceLibcxxBuilder().build()
+    builders.PlatformLibcxxAbiBuilder().build()
     builders.CompilerRTBuilder().build()
-    builders.DeviceLibcxxBuilder(builders.DeviceLibcxxBuilder.hwasan_config_list).build()
     builders.TsanBuilder().build()
     # Build musl runtimes and 32-bit glibc for Linux
     if hosts.build_host().is_linux:
@@ -216,7 +198,6 @@ def build_runtimes(build_lldb_server: bool,
     if build_lldb_server:
         builders.LldbServerBuilder().build()
     builders.SanitizerMapFileBuilder().build()
-    builders.LibSimpleperfReadElfBuilder().build()
 
 
 def install_wrappers(llvm_install_path: Path, llvm_next=False) -> None:
@@ -270,6 +251,90 @@ def install_wrappers(llvm_install_path: Path, llvm_next=False) -> None:
     clangcl_path.symlink_to('clang.real')
 
 
+# Normalize host libraries (libLLVM, libclang, libc++, libc++abi) so that there
+# is just one library, whose SONAME entry matches the actual name.
+def normalize_llvm_host_libs(install_dir: Path,
+                             host: hosts.Host,
+                             version: Version,
+                             host_config: configs.Config) -> None:
+    if host.is_linux:
+        libs = {'libLLVM': 'libLLVM-{version}.so',
+                'libclang': 'libclang.so.{version}',
+                'libclang-cpp': 'libclang-cpp.so.{version}',
+                'libc++': 'libc++.so.{version}',
+                'libc++abi': 'libc++abi.so.{version}'
+               }
+    else:
+        libs = {'libc++': 'libc++.{version}.dylib',
+                'libc++abi': 'libc++abi.{version}.dylib'
+               }
+
+    def getVersions(libname: str) -> Tuple[str, str]:
+        if libname == 'libclang-cpp':
+            return version.major, version.major
+        if not libname.startswith('libc++'):
+            return version.long_version(), version.major
+        else:
+            return '1.0', '1'
+
+    no_llvm_libs = host_config.target_os.is_linux and host_config.is_32_bit
+    libdir = os.path.join(install_dir, 'lib')
+    for libname, libformat in libs.items():
+        if libformat.startswith('libc++'):
+            libprefix = os.path.join(libdir, host_config.llvm_triple)
+            if not os.path.exists(libprefix):
+                libprefix = libdir
+        elif no_llvm_libs:
+            continue
+        else:
+            libprefix = libdir
+
+        short_version, major = getVersions(libname)
+
+        if libname == 'libclang':
+            soname = list(Path(libprefix).glob('libclang.so.[0-9][0-9]'))
+            if len(soname) == 1:
+                soname_version = str(soname[0]).split('.')[-1]
+            else:
+                raise RuntimeError(str(len(soname)) + " versions of libclang.so found, 1 expected")
+        else:
+            soname_version = major
+
+        soname_lib = os.path.join(libprefix, libformat.format(version=soname_version))
+        if libname.startswith('libclang') and libname != 'libclang-cpp':
+            soname_lib = soname_lib[:-3]
+        real_lib = os.path.join(libprefix, libformat.format(version=short_version))
+
+        preserved_libnames = ('libLLVM', 'libclang-cpp')
+        if libname not in preserved_libnames and os.path.exists(real_lib):
+            # Rename the library to match its SONAME
+            if not os.path.isfile(real_lib):
+                raise RuntimeError(real_lib + ' must be a regular file')
+            if not os.path.islink(soname_lib):
+                raise RuntimeError(soname_lib + ' must be a symlink')
+
+            shutil.move(real_lib, soname_lib)
+
+        # Retain only soname_lib and delete other files for this library.  We
+        # still need libc++.so or libc++.dylib symlinks for a subsequent stage1
+        # build using these prebuilts (where CMake tries to find C++ atomics
+        # support) to succeed.  We also need a few checks to ensure libclang-cpp
+        # is not deleted when cleaning up libclang.so* and libc++abi is not
+        # deleted when cleaning up libc++.so*.
+        libcxx_name = 'libc++.so' if host.is_linux else 'libc++.dylib'
+        all_libs = [lib for lib in os.listdir(libprefix) if
+                    lib != libcxx_name and
+                    not lib.startswith('libclang-cpp') and # retain libclang-cpp
+                    not lib.endswith('.a') and # skip static host libraries
+                    (lib.startswith(libname + '.') or # so libc++abi is ignored
+                     lib.startswith(libname + '-'))]
+
+        for lib in all_libs:
+            lib = os.path.join(libprefix, lib)
+            if lib != soname_lib:
+                os.remove(lib)
+
+
 def install_license_files(install_dir: Path) -> None:
     projects = (
         'llvm',
@@ -333,7 +398,6 @@ def bolt_optimize(toolchain_builder: LLVMBuilder, clang_fdata: Path):
         '-icf=1', '--use-gnu-stack', clang_bin_orig
     ]
     utils.check_call(args)
-    os.remove(clang_bin_orig)
 
 
 def bolt_instrument(toolchain_builder: LLVMBuilder):
@@ -358,23 +422,10 @@ def bolt_instrument(toolchain_builder: LLVMBuilder):
     os.makedirs(clang_afdo_path, exist_ok=True)
 
 
-def verify_symlink_exists(link_path: Path, target: Path):
-    if not link_path.exists():
-        raise RuntimeError(f'{link_path} does not exist')
-    if not link_path.is_symlink():
-        raise RuntimeError(f'{link_path} exists but is not a symlink')
-    if link_path.readlink() != target:
-        raise RuntimeError(f'{link_path} points to {link_path.readlink()}, expected {target}')
-
-
-def verify_file_exists(lib_dir: Path, name: str):
-    if not (lib_dir / name).is_file():
-        raise RuntimeError(f'Did not find {name} in {lib_dir}')
-
-
 def package_toolchain(toolchain_builder: LLVMBuilder,
                       necessary_bin_files: Optional[Set[str]]=None,
-                      strip=True, with_runtimes=True, create_tar=True, llvm_next=False):
+                      strip=True, create_tar=True, llvm_next=False):
+    dist_dir = Path(utils.ORIG_ENV.get('DIST_DIR', paths.OUT_DIR))
     build_dir = toolchain_builder.install_dir
     host_config = toolchain_builder.config_list[0]
     host = host_config.target_os
@@ -409,7 +460,6 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
         'clang-check' + ext,
         'clang-cl' + ext,
         'clang-format' + ext,
-        'clang-scan-deps' + ext,
         'clang-tidy' + ext,
         'clangd' + ext,
         'dsymutil' + ext,
@@ -427,7 +477,6 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
         'llvm-cov' + ext,
         'llvm-cxxfilt' + ext,
         'llvm-dis' + ext,
-        'llvm-dlltool' + ext,
         'llvm-dwarfdump' + ext,
         'llvm-dwp' + ext,
         'llvm-ifs' + ext,
@@ -454,7 +503,6 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
         'sanstats' + ext,
         'scan-build' + ext,
         'scan-view' + ext,
-        'wasm-ld' + ext,
     }
 
     if toolchain_builder.build_lldb:
@@ -506,46 +554,27 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
                 else:
                     utils.check_call([strip_cmd, binary])
 
+    # Symlink lib/clang/major_version/ to lib/clang/long_verion/
+    # TODO: Remove this once all users moved to the new directory.
+    long_ver_dir = install_dir / 'lib' / 'clang' / version.long_version()
+    long_ver_dir.symlink_to(version.major_version())
+
     # FIXME: check that all libs under lib/clang/<version>/ are created.
     for necessary_bin_file in necessary_bin_files:
         if not (bin_dir / necessary_bin_file).is_file():
             raise RuntimeError(f'Did not find {necessary_bin_file} in {bin_dir}')
 
     necessary_lib_files = set()
-    if with_runtimes:
-        if not (host.is_windows and win_sdk.is_enabled()):
-            necessary_lib_files |= {
-                'libc++.a',
-                'libc++abi.a',
-            }
-        if host.is_linux:
-            necessary_lib_files |= {
-                'libbolt_rt_instr.a',
-                'libc++.so',
-                'libc++.so.1',
-                'libc++abi.so',
-                'libc++abi.so.1',
-                'libsimpleperf_readelf.a',
-            }
-        if host.is_darwin:
-            necessary_lib_files |= {
-                'libc++.dylib',
-                'libc++abi.dylib',
-                'libsimpleperf_readelf.a',
-            }
-
-        if host.is_windows and not win_sdk.is_enabled():
-            necessary_lib_files.add('libwinpthread-1' + shlib_ext)
-            # For Windows, add other relevant libraries.
-            install_winpthreads(bin_dir, lib_dir)
-
-        # Archive libsimpleperf_readelf.a for linux and darwin hosts from stage2 build.
-        if host.is_linux:
-            builders.LibSimpleperfReadElfBuilder().build_readelf_lib(lib_dir,
-                                                                     lib_dir / host_config.llvm_triple)
-        elif host.is_darwin:
-            builders.LibSimpleperfReadElfBuilder().build_readelf_lib(lib_dir, lib_dir,
-                                                                     is_darwin_lib=True)
+    if not (host.is_windows and win_sdk.is_enabled()):
+        necessary_lib_files |= {
+            'libc++.a',
+            'libc++abi.a',
+        }
+
+    if host.is_windows and not win_sdk.is_enabled():
+        necessary_lib_files.add('libwinpthread-1' + shlib_ext)
+        # For Windows, add other relevant libraries.
+        install_winpthreads(bin_dir, lib_dir)
 
     # Remove unnecessary static libraries.
     remove_static_libraries(lib_dir, necessary_lib_files)
@@ -553,37 +582,25 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
     if host.is_linux:
         install_wrappers(install_dir, llvm_next)
 
-    # Add libc++[abi].so.1 and libc++[abi].1.dylib symlinks for backwards compatibility. These
-    # symlinks point to the unversioned libraries (as opposed to the typical situation where
-    # unversioned symlinks point to the versioned libraries).
-    if host.is_linux:
-        if host_config.is_musl:
-            triple32 = 'i686-unknown-linux-musl'
-            triple64 = 'x86_64-unknown-linux-musl'
-        else:
-            triple32 = 'i386-unknown-linux-gnu'
-            triple64 = 'x86_64-unknown-linux-gnu'
-        for tripleNN in (triple32, triple64):
-            (lib_dir / tripleNN / 'libc++.so.1').symlink_to('libc++.so')
-            (lib_dir / tripleNN / 'libc++abi.so.1').symlink_to('libc++abi.so')
-        (lib_dir / 'libc++.so.1').symlink_to(Path(triple64) / 'libc++.so.1')
-        (lib_dir / 'libc++abi.so.1').symlink_to(Path(triple64) / 'libc++abi.so.1')
+    if not host.is_windows:
+        normalize_llvm_host_libs(install_dir, host, version, host_config)
+        if host.is_linux:
+            # We also need to normalize the 32-bit libc++, libc++abi
+            normalize_llvm_host_libs(install_dir, host, version,
+                                     configs.host_32bit_config(host_config.is_musl))
 
     # Check necessary lib files exist.
     for necessary_lib_file in necessary_lib_files:
-        if necessary_lib_file.startswith('libc++') and (host.is_linux or host.is_windows):
-            verify_file_exists(lib_dir, necessary_lib_file)
-            if necessary_lib_file.endswith('.a'):
-                verify_file_exists(lib_dir / 'i686-w64-windows-gnu', necessary_lib_file)
-                verify_file_exists(lib_dir / 'x86_64-w64-windows-gnu', necessary_lib_file)
-            if host.is_linux:
-                verify_symlink_exists(lib_dir / necessary_lib_file, Path(triple64) / necessary_lib_file)
-                verify_file_exists(lib_dir / triple32, necessary_lib_file)
-                verify_file_exists(lib_dir / triple64, necessary_lib_file)
-        elif necessary_lib_file == 'libsimpleperf_readelf.a' and host.is_linux:
-            verify_file_exists(lib_dir / host_config.llvm_triple, necessary_lib_file)
+        if not host.is_windows and necessary_lib_file.startswith('libc++'):
+            libprefix = lib_dir / 'x86_64-unknown-linux-gnu'
+            if not os.path.exists(libprefix):
+                libprefix = lib_dir / 'x86_64-unknown-linux-musl'
+            if not os.path.exists(libprefix):
+                libprefix = lib_dir
         else:
-            verify_file_exists(lib_dir, necessary_lib_file)
+            libprefix = lib_dir
+        if not (libprefix / necessary_lib_file).is_file():
+            raise RuntimeError(f'Did not find {necessary_lib_file} in {lib_dir}')
 
     # Next, we copy over stdatomic.h and bits/stdatomic.h from bionic.
     libc_include_path = paths.ANDROID_DIR / 'bionic' / 'libc' / 'include'
@@ -606,10 +623,10 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
         svn_revision = android_version.get_svn_revision()
         version_file.write(f'based on {svn_revision}\n')
         version_file.write('for additional information on LLVM revision and '
-                           'cherry-picks, see clang_source_info.md\n')
+                           'cherry-picks, see clang_source_info.md')
 
     clang_source_info_file = paths.OUT_DIR / 'clang_source_info.md'
-    manifest = list(paths.DIST_DIR.glob('manifest_*.xml'))
+    manifest = list(Path(dist_dir).glob('manifest_*.xml'))
 
     # get revision from manifest, update clang_source_info.md
     if manifest:
@@ -618,7 +635,7 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
         get_scripts_sha = re.findall(r'name="toolchain/llvm_android" revision="(.*)" /',
                                      manifest_context)[0]
     else:
-        get_scripts_sha = 'refs/heads/main'
+        get_scripts_sha = 'refs/heads/master'
     with open(clang_source_info_file, 'r') as info:
         info_read = info.read()
     with open(clang_source_info_file, 'w') as info:
@@ -628,13 +645,6 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
     if clang_source_info_file.exists():
         shutil.copy2(clang_source_info_file, install_dir)
 
-    # Add order file scripts to the toolcahin in share_orderfile_dir
-    share_orderfile_dir = install_dir / "share/orderfiles"
-    share_orderfile_dir.mkdir(parents=True, exist_ok=True)
-    for script_file in paths.ORDERFILE_SCRIPTS_DIR.iterdir():
-        if script_file.is_file():
-            shutil.copy2(script_file, share_orderfile_dir)
-    os.remove(share_orderfile_dir / "orderfile_unittest.py")
 
     # Remove optrecord.py to avoid auto-filed bugs about call to yaml.load_all
     os.remove(install_dir / 'share/opt-viewer/optrecord.py')
@@ -652,7 +662,6 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
                         srcs = glob([
                             "bin/*",
                             "lib/*",
-                            "lib/x86_64-unknown-linux-gnu/*",
                         ]),
                     )
 
@@ -660,24 +669,8 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
                         name = "includes",
                         srcs = glob([
                             "lib/clang/*/include/**",
-                            "include/c++/**",
-                            "include/x86_64-unknown-linux-gnu/c++/**",
                         ]),
                     )
-
-                    # Special python3 for u-boot.
-                    py_runtime(
-                        name = "python3",
-                        files = glob(
-                            ["python3/**"],
-                            exclude = [
-                                "**/site-packages/**",
-                            ],
-                        ),
-                        interpreter = "python3/bin/python3",
-                        python_version = "PY3",
-                        visibility = ["//u-boot:__subpackages__"],
-                    )
                     """))
 
         # Create RBE input files.
@@ -688,7 +681,7 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
                             'clang++.real\n'
                             'clang-tidy\n'
                             'clang-tidy.real\n'
-                            '../lib/libc++.so\n'
+                            '../lib/libc++.so.1\n'
                             'lld\n'
                             'ld64.lld\n'
                             'ld.lld\n'
@@ -704,10 +697,11 @@ def package_toolchain(toolchain_builder: LLVMBuilder,
         tag = host.os_tag
         if isinstance(toolchain_builder.config_list[0], configs.LinuxMuslConfig):
             tag = host.os_tag_musl
-        tarball_name = package_name + '-' + tag + '.tar.xz'
-        package_path = paths.DIST_DIR / tarball_name
+        tarball_name = package_name + '-' + tag + '.tar.bz2'
+        package_path = dist_dir / tarball_name
         logger().info(f'Packaging {package_path}')
-        utils.create_tarball(install_host_dir, [package_name], package_path)
+        args = ['tar', '-cjC', install_host_dir, '-f', package_path, package_name]
+        utils.check_call(args)
 
 
 def parse_args():
@@ -823,12 +817,6 @@ def parse_args():
         default=False,
         help='Skip applying local patches. This allows building a vanilla upstream version.')
 
-    parser.add_argument(
-        '--continue-on-errors',
-        action='store_true',
-        default=False,
-        help='Continue build on error. This allows catching all errors at once.')
-
     parser.add_argument(
         '--create-tar',
         action='store_true',
@@ -863,28 +851,11 @@ def parse_args():
         nargs='+',
         help='A list of builders to skip. All builders not listed will be built.')
 
-    bootstrap_group = parser.add_mutually_exclusive_group()
-    bootstrap_group.add_argument(
-        '--bootstrap-build-only',
-        default=False,
-        action='store_true',
-        help='Build the bootstrap compiler and exit.')
-    bootstrap_group.add_argument(
-        '--bootstrap-use',
-        default='',
-        help='Use the given bootstrap compiler.'
-    )
-    bootstrap_group.add_argument(
-        '--bootstrap-use-prebuilt',
-        action='store_true',
-        default=False,
-        help='Skip building the bootstrap compiler and use the prebuilt instead.')
-
     parser.add_argument(
-        '--package-stage2-install',
+        '--single-stage',
         action='store_true',
         default=False,
-        help='Package stage2-install')
+        help='Skip building stage 1 compiler and use the prebuilt instead.')
 
     parser.add_argument(
         '--mlgo',
@@ -915,13 +886,6 @@ def parse_args():
     parser.add_argument('--llvm-rev', help='Fetch specific LLVM revision from upstream instead of \
                         using toolchain/llvm-project (SHA or \'main\')')
 
-    parser.add_argument(
-        "--git_am",
-        action="store_true",
-        default=False,
-        help="If set, use 'git am' to patch instead of GNU 'patch'. ",
-    )
-
     parser.add_argument(
         '--windows-sdk',
         help='Path to a Windows SDK. If set, it will be used instead of MinGW.'
@@ -940,35 +904,19 @@ def parse_args():
         dest='musl',
         help="Don't Build against musl libc")
 
-    incremental_group = parser.add_mutually_exclusive_group()
-    incremental_group.add_argument(
-        '--incremental',
+    parser.add_argument(
+        '--sccache',
         action='store_true',
-        default=True,
-        help='Keep paths.OUT_DIR if it exists')
-    incremental_group.add_argument(
-        '--no-incremental',
-        action='store_false',
-        default=True,
-        dest='incremental',
-        help='Delete paths.OUT_DIR if it exists (default)')
+        default=False,
+        help='Use sccache to speed up development builds. (Do not use for release builds)')
 
     return parser.parse_args()
 
 
 def main():
-    logging.basicConfig(level=logging.DEBUG)
+    dist_dir = Path(utils.ORIG_ENV.get('DIST_DIR', paths.OUT_DIR))
     args = parse_args()
-
-    if paths.OUT_DIR.exists():
-        if not args.incremental:
-            logger().info(f'Removing {paths.OUT_DIR}')
-            utils.clean_out_dir()
-        else:
-            out_dir_items = ' '.join(map(str, paths.OUT_DIR.iterdir()))
-            logger().info(f'Keeping older build in {paths.OUT_DIR}: {out_dir_items}')
-
-    timer.Timer.register_atexit(paths.DIST_DIR / 'build_times.txt')
+    timer.Timer.register_atexit(dist_dir / 'build_times.txt')
 
     if args.skip_build:
         # Skips all builds
@@ -977,6 +925,8 @@ def main():
         BuilderRegistry.add_skips(args.skip)
     elif args.build:
         BuilderRegistry.add_builds(args.build)
+    if args.single_stage:
+        BuilderRegistry.add_skips(['stage1'])
 
     do_bolt = args.bolt and not args.debug and not args.build_instrumented
     do_bolt_instrument = args.bolt_instrument and not args.debug and not args.build_instrumented
@@ -987,84 +937,46 @@ def main():
     build_lldb = 'lldb' not in args.no_build
     mlgo = args.mlgo
     musl = args.musl
+    sccache = args.sccache
 
     host_configs = [configs.host_config(musl)]
 
     android_version.set_llvm_next(args.build_llvm_next)
 
-    if (do_bolt or do_bolt_instrument) and hosts.build_host().is_darwin:
-        raise ValueError("BOLT is not supported for Mach-O binaries. https://github.com/llvm/llvm-project/blob/main/bolt/README.md#input-binary-requirements")
-
-    if mlgo and hosts.build_host().is_darwin:
-        raise ValueError("MLGO is not supported for macOS.")
-
     need_host = hosts.build_host().is_darwin or ('linux' not in args.no_build)
-    need_windows_libcxx = hosts.build_host().is_linux and do_runtimes
     need_windows = hosts.build_host().is_linux and ('windows' not in args.no_build)
 
+    logging.basicConfig(level=logging.DEBUG)
+
     logger().info('do_build=%r do_stage1=%r do_stage2=%r do_runtimes=%r do_package=%r need_windows=%r lto=%r bolt=%r musl=%r' %
                   (not args.skip_build, BuilderRegistry.should_build('stage1'), BuilderRegistry.should_build('stage2'),
                   do_runtimes, do_package, need_windows, args.lto, args.bolt, args.musl))
 
-    if paths.get_tensorflow_path() is None:
-        if mlgo:
-            raise ValueError("MLGO requires tensorflow. Tensorflow not found.")
-        else:
-            logger().info('Tensorflow not found.')
-    else:
-        logger().info('Tensorflow found: ' + paths.get_tensorflow_path())
-
-    build_errors : List[toolchain_errors.ToolchainError] = []
     # Clone sources to be built and apply patches.
     if not args.skip_source_setup:
-        setup_source_result = source_manager.setup_sources(git_am=args.git_am,
-                                         llvm_rev=args.llvm_rev,
-                                         skip_apply_patches=args.skip_apply_patches,
-                                         continue_on_patch_errors=args.continue_on_errors)
-        if setup_source_result:
-            build_errors.append(setup_source_result)
+        source_manager.setup_sources(llvm_rev=args.llvm_rev, skip_apply_patches=args.skip_apply_patches)
 
     # Build the stage1 Clang for the build host
     instrumented = hosts.build_host().is_linux and args.build_instrumented
-    libzstd_builder = builders.ZstdBuilder(host_configs)
-    libzstd_builder.build()
 
-    if not args.bootstrap_use_prebuilt and not args.bootstrap_use:
-        stage1 = builders.Stage1Builder(host_configs)
-        stage1.build_name = 'stage1'
-        stage1.svn_revision = android_version.get_svn_revision()
-        # Build lldb for lldb-tblgen. It will be used to build lldb-server and windows lldb.
-        stage1.build_lldb = build_lldb
-        stage1.enable_mlgo = mlgo
-        stage1.build_extra_tools = args.run_tests_stage1
-        stage1.libzstd = libzstd_builder
-        stage1.build()
-        if hosts.build_host().is_linux:
-            add_header_links('stage1', host_config=configs.host_config(musl))
-        # stage1 test is off by default, turned on by --run-tests-stage1,
-        # and suppressed by --skip-tests.
-        if not args.skip_tests and args.run_tests_stage1:
-            stage1.test()
+    stage1 = builders.Stage1Builder(host_configs)
+    stage1.build_name = 'stage1'
+    stage1.svn_revision = android_version.get_svn_revision()
+    # Build lldb for lldb-tblgen. It will be used to build lldb-server and windows lldb.
+    stage1.build_lldb = build_lldb
+    stage1.enable_mlgo = mlgo
+    stage1.build_extra_tools = args.run_tests_stage1
+    stage1.build_android_targets = args.debug or instrumented
+    stage1.use_sccache = sccache
+    stage1.build()
+    if hosts.build_host().is_linux and not args.single_stage:
+        add_header_links('stage1', host_config=configs.host_config(musl))
+    # stage1 test is off by default, turned on by --run-tests-stage1,
+    # and suppressed by --skip-tests.
+    if not args.skip_tests and args.run_tests_stage1:
+        stage1.test()
+    if not args.single_stage:
         set_default_toolchain(stage1.installed_toolchain)
-    if args.bootstrap_use:
-        # Remove previous install directories, since the bootstrap compiler
-        # will overwrite install directories.
-        if (paths.OUT_DIR / 'stage1-install').exists():
-            shutil.rmtree(paths.OUT_DIR / 'stage1-install')
-        if (paths.OUT_DIR / 'stage2-install').exists():
-            shutil.rmtree(paths.OUT_DIR / 'stage2-install')
-
-        with timer.Timer(f'extract_bootstrap'):
-            utils.extract_tarball(paths.OUT_DIR, args.bootstrap_use)
-
-        # If we were to use the full build as bootstrap, we need to rename it to stage-install.
-        if (paths.OUT_DIR / 'stage2-install').exists():
-            (paths.OUT_DIR / 'stage2-install').rename(paths.OUT_DIR / 'stage1-install')
-        set_default_toolchain(toolchains.Toolchain(paths.OUT_DIR / 'stage1-install', paths.OUT_DIR / 'stage1'))
-    if args.bootstrap_build_only:
-        with timer.Timer(f'package_bootstrap'):
-            utils.create_tarball(paths.OUT_DIR, ['stage1', 'stage1-install'], paths.DIST_DIR / 'stage1-install.tar.xz')
-        return
 
     if build_lldb:
         # Swig is needed for both host and windows lldb.
@@ -1073,17 +985,12 @@ def main():
     else:
         swig_builder = None
 
-    if args.pgo:
-        profdata = extract_pgo_profile()
-    else:
-        profdata = None
-
-    if args.bolt:
-        clang_bolt_fdata = extract_bolt_profile()
-    else:
-        clang_bolt_fdata = None
-
     if need_host:
+        if args.pgo:
+            profdata, clang_bolt_fdata = extract_profiles()
+        else:
+            profdata, clang_bolt_fdata = None, None
+
         stage2 = builders.Stage2Builder(host_configs)
         stage2.build_name = args.build_name
         stage2.svn_revision = android_version.get_svn_revision()
@@ -1094,8 +1001,11 @@ def main():
         stage2.enable_mlgo = mlgo
         stage2.bolt_optimize = args.bolt
         stage2.bolt_instrument = args.bolt_instrument
-        stage2.profdata_file = profdata
-        stage2.build_cross_runtimes = hosts.build_host().is_linux
+        stage2.profdata_file = profdata if profdata else None
+        stage2.build_32bit_runtimes = hosts.build_host().is_linux
+
+        libzstd_builder = builders.ZstdBuilder(host_configs)
+        libzstd_builder.build()
         stage2.libzstd = libzstd_builder
 
         libxml2_builder = builders.LibXml2Builder(host_configs)
@@ -1109,30 +1019,29 @@ def main():
             xz_builder = builders.XzBuilder(host_configs)
             xz_builder.build()
             stage2.liblzma = xz_builder
-
-            libncurses = builders.LibNcursesBuilder(host_configs)
-            libncurses.build()
-            stage2.libncurses = libncurses
+            #
+            # libncurses = builders.LibNcursesBuilder(host_configs)
+            # libncurses.build()
+            # stage2.libncurses = libncurses
 
             libedit_builder = builders.LibEditBuilder(host_configs)
-            libedit_builder.libncurses = libncurses
             libedit_builder.build()
             stage2.libedit = libedit_builder
 
         stage2_tags = []
-        # Annotate the version string with build options.
-        to_tag = lambda c, tag : ('+' if c else '-') + tag
-        stage2_tags.append(to_tag(profdata, 'pgo'))
-        stage2_tags.append(to_tag(clang_bolt_fdata, 'bolt'))
-        stage2_tags.append(to_tag(stage2.lto, 'lto'))
-        stage2_tags.append(to_tag(stage2.enable_mlgo, 'mlgo'))
+        # Annotate the version string if there is no profdata.
+        if profdata is None:
+            stage2_tags.append('NO PGO PROFILE')
+        if clang_bolt_fdata is None:
+            stage2_tags.append('NO BOLT PROFILE')
+        # Annotate the version string if this is an llvm-next build.
         if args.build_llvm_next:
             stage2_tags.append('ANDROID_LLVM_NEXT')
         stage2.build_tags = stage2_tags
 
         stage2.build()
 
-        if do_bolt:
+        if do_bolt and clang_bolt_fdata is not None:
             bolt_optimize(stage2, clang_bolt_fdata)
 
         if not (stage2.build_instrumented or stage2.debug_build):
@@ -1145,20 +1054,16 @@ def main():
                            host_config=configs.host_config(musl),
                            host_32bit_config=configs.host_32bit_config(musl))
 
-    if need_windows or need_windows_libcxx:
+    if need_windows:
         # Host sysroots are currently setup only for Windows
         builders.HostSysrootsBuilder().build()
         if args.windows_sdk:
             win_sdk.set_path(Path(args.windows_sdk))
         win_builder, win_lldb_bins = build_llvm_for_windows(
             enable_assertions=args.enable_assertions,
-            enable_lto=args.lto,
-            profdata_file=profdata,
             build_name=args.build_name,
             build_lldb=build_lldb,
-            swig_builder=swig_builder,
-            full_build=need_windows,
-            build_simpleperf_readelf=need_host)
+            swig_builder=swig_builder)
 
     # stage2 test is on when stage2 is enabled unless --skip-tests or
     # on instrumented builds.
@@ -1166,7 +1071,9 @@ def main():
             BuilderRegistry.should_build('stage2') and \
             (not args.build_instrumented)
     if need_tests:
-       stage2.test()
+        # http://b/197645198 Temporarily skip tests on [Darwin] builds
+        if not (hosts.build_host().is_darwin):
+            stage2.test()
 
     # Instrument with llvm-bolt. Must be the last build step to prevent other
     # build steps generating BOLT profiles.
@@ -1174,15 +1081,10 @@ def main():
         if do_bolt_instrument:
             bolt_instrument(stage2)
 
-    if args.package_stage2_install:
-        utils.create_tarball(paths.OUT_DIR, ['stage2-install'],
-                             paths.DIST_DIR / 'stage2-install.tar.xz')
-
     if do_package and need_host:
         package_toolchain(
             stage2,
             strip=do_strip_host_package,
-            with_runtimes=do_runtimes,
             create_tar=args.create_tar,
             llvm_next=args.build_llvm_next)
 
@@ -1191,12 +1093,8 @@ def main():
             win_builder,
             necessary_bin_files=win_lldb_bins,
             strip=do_strip,
-            with_runtimes=do_runtimes,
             create_tar=args.create_tar)
 
-    if build_errors:
-        logger().info(toolchain_errors.combine_toolchain_errors(build_errors))
-        return len(build_errors)
     return 0
 
 
diff --git a/do_kythe_xref.py b/do_kythe_xref.py
old mode 100644
new mode 100755
index 09ef37b..f6fbada
--- a/do_kythe_xref.py
+++ b/do_kythe_xref.py
@@ -19,7 +19,6 @@ import logging
 import os
 import re
 import shutil
-import subprocess
 import sys
 
 import android_version
@@ -29,11 +28,6 @@ import hosts
 import paths
 import utils
 
-def logger():
-    """Returns the module level logger."""
-    return logging.getLogger(__name__)
-
-
 def build_llvm() -> builders.Stage2Builder:
     host_configs = [configs.host_config()]
     stage2 = builders.Stage2Builder(host_configs)
@@ -52,12 +46,8 @@ def build_llvm() -> builders.Stage2Builder:
     stage2.enable_assertions = True
     stage2.lto = False
     stage2.build_lldb = False
-    stage2.ninja_targets = ['all', 'UnitTests', '-k', '100']
-    try:
-        stage2.build()
-    except subprocess.CalledProcessError:
-        # Ok to fail
-        logger().info('stage2.build() failed, but it\'s ok')
+    stage2.ninja_targets = ['all', 'UnitTests']
+    stage2.build()
     return stage2
 
 # runextractor is expected to fail on these sources.
@@ -78,7 +68,7 @@ def build_kythe_corpus(builder: builders.Stage2Builder) -> None:
     env = {
         'KYTHE_OUTPUT_DIRECTORY': kythe_out_dir,
         'KYTHE_ROOT_DIRECTORY': paths.ANDROID_DIR,
-        'KYTHE_CORPUS': 'android.googlesource.com/toolchain/llvm-project//main',
+        'KYTHE_CORPUS': 'android.googlesource.com/toolchain/llvm-project//master-legacy',
         'KYTHE_VNAMES': paths.KYTHE_VNAMES_JSON
     }
 
@@ -122,8 +112,7 @@ def package(build_name: str) -> None:
     utils.check_call(['build/soong/soong_ui.bash',
                       '--build-mode', '--all-modules',
                       f'--dir={paths.ANDROID_DIR}',
-                      '-k', 'TARGET_RELEASE=trunk_staging',
-                      'merge_zips'])
+                      '-k', 'merge_zips'])
     merge_zips_path = (paths.OUT_DIR / 'host' / hosts.build_host().os_tag /
                        'bin' / 'merge_zips')
 
diff --git a/do_test_compiler.py b/do_test_compiler.py
index 53027d6..749f269 100755
--- a/do_test_compiler.py
+++ b/do_test_compiler.py
@@ -71,8 +71,11 @@ class PgoProfileHandler(ProfileHandler):
         ])
 
         dist_dir = Path(os.environ.get('DIST_DIR', paths.OUT_DIR))
-        utils.create_tarball(profdata_dir, [profdata_filename],
-                             dist_dir / paths.pgo_profdata_tarname())
+        utils.check_call([
+            'tar', '-cjC',
+            str(profdata_dir), profdata_filename, '-f',
+            str(dist_dir / paths.pgo_profdata_tarname())
+        ])
 
 
 class BoltProfileHandler(ProfileHandler):
@@ -92,35 +95,25 @@ class BoltProfileHandler(ProfileHandler):
         ])
 
         dist_dir = Path(os.environ.get('DIST_DIR', paths.OUT_DIR))
-        utils.create_tarball(bolt_collection_path, [clang_fdata_filename],
-                             dist_dir / paths.bolt_fdata_tarname())
+        utils.check_call([
+            'tar', '-cjC',
+            str(bolt_collection_path), clang_fdata_filename, '-f',
+            str(dist_dir / paths.bolt_fdata_tarname())
+        ])
 
 
 def parse_args():
     parser = argparse.ArgumentParser()
     parser.add_argument('android_path', help='Android source directory.')
-
-    clang_group = parser.add_mutually_exclusive_group()
-    clang_group.add_argument(
+    parser.add_argument(
         '--clang-path',
         nargs='?',
-        help='Directory with a previously built Clang toolchain.')
-    clang_group.add_argument(
+        help='Directory with a previously built Clang.')
+    parser.add_argument(
         '--clang-package-path',
         nargs='?',
-        help='Directory of a pre-packaged (.tar.xz) Clang toolchain. '
+        help='Directory of a pre-packaged (.tar.bz2) Clang. '
         'Toolchain extracted from the package will be used.')
-    clang_group.add_argument(
-        '--clang-bootstrap-path',
-        nargs='?',
-        help='Directory of a pre-packaged (.tar.xz) bootstrap (stage-1) Clang. '
-        'Clang extracted from the package will be used for building a full toolchain.')
-    clang_group.add_argument(
-        '--clang-kokoro-build-id',
-        nargs='?',
-        help='Kokoro TOT Clang build ID'
-        'Clang pulled from that build will be used.')
-
     parser.add_argument(
         '-k',
         '--keep-going',
@@ -140,11 +133,6 @@ def parse_args():
         action='store_true',
         default=False,
         help='Build default targets only.')
-    parser.add_argument(
-        '--no-mlgo',
-        action='store_true',
-        default=False,
-        help='Build without mlgo.')
     parser.add_argument(
         '--skip-tests',
         action='store_true',
@@ -209,21 +197,23 @@ def parse_args():
         help='Build BOLT instrumented compiler and gather profiles')
 
     args = parser.parse_args()
+    if args.clang_path and args.clang_package_path:
+        parser.error('Only one of --clang-path and --clang-package-path must'
+                     'be specified')
     if args.build_only and not args.target:
         parser.error('Build target is not specified in build only mode.')
 
     return args
 
 
-def copy_clang(android_base: Path, clang_path: Path) -> None:
+def link_clang(android_base: Path, clang_path: Path) -> None:
     android_clang_path = (android_base / 'prebuilts' / 'clang' / 'host' /
                           hosts.build_host().os_tag / 'clang-dev')
     if android_clang_path.is_symlink() or android_clang_path.is_file():
         android_clang_path.unlink()
     elif android_clang_path.is_dir():
         shutil.rmtree(android_clang_path)
-    # TODO(b/260809113): We can use symlink when this bug is fixed.
-    shutil.copytree(clang_path, android_clang_path, symlinks=True)
+    android_clang_path.symlink_to(clang_path.resolve())
 
 
 def get_connected_device_list() -> List[List[str]]:
@@ -253,7 +243,6 @@ def extract_clang_version(clang_install: Path) -> version.Version:
 def build_target(android_base: Path, clang_version: version.Version,
                  target: str, modules: List[str],
                  max_jobs: int, enable_fallback: bool, with_tidy: bool,
-                 no_mlgo: bool,
                  profiler: Optional[ProfileHandler]=None) -> None:
     jobs = '-j{}'.format(max(1, min(max_jobs, multiprocessing.cpu_count())))
     try:
@@ -293,10 +282,11 @@ def build_target(android_base: Path, clang_version: version.Version,
         env[DISABLED_WARNINGS_KEY] = ' '.join(DISABLED_WARNINGS)
 
     env['LLVM_PREBUILTS_VERSION'] = 'clang-dev'
-    env['LLVM_RELEASE_VERSION'] = clang_version.major_version()
+    env['LLVM_RELEASE_VERSION'] = clang_version.long_version()
     env['LLVM_NEXT'] = 'true'
-    if no_mlgo:
-        env['THINLTO_USE_MLGO'] = 'false'
+
+    # TODO(b/260809113): Remove this when the bug is fixed
+    env['BUILD_BROKEN_DISABLE_BAZEL'] = '1'
 
     if with_tidy:
         env['WITH_TIDY'] = '1'
@@ -325,7 +315,7 @@ def build_target(android_base: Path, clang_version: version.Version,
 
 def test_device(android_base: Path, clang_version: version.Version, device: List[str],
                 modules: List[str], max_jobs: int, clean_output: str, flashall_path: Optional[Path],
-                enable_fallback: bool, with_tidy: bool, no_mlgo: bool) -> bool:
+                enable_fallback: bool, with_tidy: bool) -> bool:
     [label, target] = device[-1].split(':')
     # If current device is not connected correctly we will just skip it.
     if label != 'device':
@@ -335,7 +325,7 @@ def test_device(android_base: Path, clang_version: version.Version, device: List
         target = 'aosp_' + target + '-eng'
     try:
         build_target(android_base, clang_version, target, modules, max_jobs,
-                     enable_fallback, with_tidy, no_mlgo)
+                     enable_fallback, with_tidy)
         if flashall_path is None:
             bin_path = (android_base / 'out' / 'host' /
                         hosts.build_host().os_tag / 'bin')
@@ -358,10 +348,10 @@ def test_device(android_base: Path, clang_version: version.Version, device: List
 
 def extract_packaged_clang(package_path: Path) -> Path:
     # Find package to extract
-    tarballs: List[Path] = sorted(package_path.rglob('*-linux-*.tar.xz'))
+    tarballs: List[Path] = sorted(package_path.rglob('*-linux-*.tar.bz2'))
     if len(tarballs) != 1:
         raise RuntimeError(
-            f'No clang packages (.tar.xz) found in {package_path}')
+            f'No clang packages (.tar.bz2) found in {package_path}')
 
     tarball = tarballs[0]
 
@@ -371,7 +361,8 @@ def extract_packaged_clang(package_path: Path) -> Path:
         shutil.rmtree(extract_dir)
     extract_dir.mkdir(parents=True, exist_ok=True)
 
-    utils.extract_tarball(extract_dir, tarball)
+    args: List[str] = ['tar', '-xjC', str(extract_dir), '-f', str(tarball)]
+    subprocess.check_call(args)
 
     # Find and return a singleton subdir
     extracted: List[Path] = list(extract_dir.iterdir())
@@ -386,27 +377,6 @@ def extract_packaged_clang(package_path: Path) -> Path:
     return clang_path
 
 
-def fetch_kokoro_prebuilt(build_id: str) -> Path:
-    # Extract package to $OUT_DIR/extracted
-    extract_dir = paths.OUT_DIR / 'extracted'
-    if extract_dir.exists():
-        shutil.rmtree(extract_dir)
-    extract_dir.mkdir(parents=True, exist_ok=True)
-
-    utils.check_call([
-        paths.SCRIPTS_DIR / "fetch_kokoro_prebuilts.py", "--build_id", build_id,
-        extract_dir
-    ])
-
-    return extract_dir / f'clang-{build_id}'
-
-
-def is_clang_built_with_mlgo(clang_dir: Path):
-    clang = clang_dir / 'bin' / 'clang'
-    output = utils.check_output([str(clang), '--version'])
-    return '+mlgo' in output
-
-
 def main():
     logging.basicConfig(level=logging.DEBUG)
 
@@ -418,14 +388,8 @@ def main():
         clang_path = Path(args.clang_path)
     elif args.clang_package_path is not None:
         clang_path = extract_packaged_clang(Path(args.clang_package_path))
-    elif args.clang_kokoro_build_id is not None:
-        clang_path = fetch_kokoro_prebuilt(args.clang_kokoro_build_id)
     else:
         cmd = [paths.SCRIPTS_DIR / 'build.py', '--no-build=windows,lldb']
-        if not args.no_mlgo:
-            cmd.append('--mlgo')
-        if args.clang_bootstrap_path:
-            cmd.append(f'--bootstrap-use={args.clang_bootstrap_path}')
         if args.profile:
             cmd.append('--build-instrumented')
             cmd.append('--skip-tests')
@@ -440,9 +404,7 @@ def main():
         utils.check_call(cmd)
         clang_path = paths.get_package_install_path(hosts.build_host(), 'clang-dev')
     clang_version = extract_clang_version(clang_path)
-    copy_clang(Path(args.android_path), clang_path)
-
-    no_mlgo = not is_clang_built_with_mlgo(clang_path)
+    link_clang(Path(args.android_path), clang_path)
 
     if args.build_only:
         if args.profile:
@@ -454,7 +416,7 @@ def main():
 
         build_target(Path(args.android_path), clang_version, args.target,
                      modules, args.jobs,
-                     args.enable_fallback, args.with_tidy, no_mlgo, profiler)
+                     args.enable_fallback, args.with_tidy, profiler)
 
         if profiler is not None:
             profiler.mergeProfiles()
@@ -467,7 +429,7 @@ def main():
             result = test_device(Path(args.android_path), clang_version, device,
                                  modules, args.jobs, args.clean_built_target,
                                  Path(args.flashall_path) if args.flashall_path else None,
-                                 args.enable_fallback, args.with_tidy, no_mlgo)
+                                 args.enable_fallback, args.with_tidy)
             if not result and not args.keep_going:
                 break
 
diff --git a/docker/Dockerfile b/docker/Dockerfile
old mode 100644
new mode 100755
index 934f3ee..e6e800b
--- a/docker/Dockerfile
+++ b/docker/Dockerfile
@@ -1,7 +1,9 @@
 FROM ubuntu:22.04
 RUN apt-get update \
-&& DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get install --no-install-recommends -y \
+&& DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y \
+  bison \
   build-essential \
+  cargo \
   git \
   libssl-dev \
   pkg-config \
@@ -12,8 +14,8 @@ RUN apt-get update \
   ssh \
   unzip \
   zip \
-  # Required by android-build buildbot
-  tzdata \
+  # clang-3289846 uses host libncurses5
+  libncurses5 \
   # Necessary to test 32-bit runtimes.
   gcc-multilib \
   libc6-dev-i386-cross \
@@ -25,5 +27,9 @@ RUN pip install --require-hashes -r /tmp/requirements.txt \
 && rm -rf /root/.cache/pip /tmp/requirements.txt
 ENV TENSORFLOW_INSTALL=/usr/local/lib/python3.10/dist-packages/tensorflow
 
+# Install sccache
+RUN cargo install --root /usr/local --no-default-features --features=gcs sccache \
+&& rm -rf /root/.cargo
+
 # Set up git user identity, as required by repo
-RUN printf '[user]\n  name = Builder\n  email = android-llvm+build@google.com' > /etc/gitconfig
+RUN printf '[user]\n  name = Builder\n  email = android-llvm+kokoro@google.com' > /etc/gitconfig
diff --git a/docker/README b/docker/README
old mode 100644
new mode 100755
index e5053db..cef016d
--- a/docker/README
+++ b/docker/README
@@ -5,7 +5,7 @@ Run prod_env.sh
 
 If you see gcloud permission errors, run
 
-  gcloud auth login && gcloud auth configure-docker us-docker.pkg.dev
+  gcloud auth login && gcloud auth configure-docker us-west1-docker.pkg.dev
 
 to set up gcloud Docker authentication.
 
@@ -15,13 +15,4 @@ Make your changes and run test_env.sh.
 # Deploying a new Docker image
 Command to build and deploy an updated Docker image using Google Cloud Build:
 
-  gcloud builds submit --timeout 3600s --tag us-docker.pkg.dev/google.com/android-llvm-kokoro/android-llvm/llvm-ubuntu
-
-Tag the build with `prod` on Cloud Artifact Registry.
-
-# Generate requirements.txt
-
-docker/prod_env.sh
-pip install pip-tools
-echo tensorflow-cpu > requirements.in
-~/.local/bin/pip-compile --upgrade --generate-hashes --output-file=requirements.txt requirements.in
+  gcloud builds submit --timeout 3600s --tag us-west1-docker.pkg.dev/google.com/android-llvm-kokoro/android-llvm-ubuntu/llvm-ubuntu
diff --git a/docker/prod_env.sh b/docker/prod_env.sh
index 356af80..ff413d1 100755
--- a/docker/prod_env.sh
+++ b/docker/prod_env.sh
@@ -11,9 +11,9 @@ WORK_DIR=/tmpfs/src/git/
 echo build:x:${LOCAL_UID}:${LOCAL_GID}:Build:${WORK_DIR}:/bin/bash > /tmp/passwd.docker
 echo build:*:${LOCAL_GID}: > /tmp/group.docker
 
-docker_img=us-docker.pkg.dev/google.com/android-llvm-kokoro/android-llvm/llvm-ubuntu:prod
+docker_img=us-west1-docker.pkg.dev/google.com/android-llvm-kokoro/android-llvm-ubuntu/llvm-ubuntu
 
-docker pull ${docker_img}
+docker pull ${docker_img}:latest
 docker run -it \
   --rm \
   --user ${LOCAL_UID}:${LOCAL_GID} \
diff --git a/docker/requirements.txt b/docker/requirements.txt
old mode 100644
new mode 100755
index 2b18f3f..f4b1438
--- a/docker/requirements.txt
+++ b/docker/requirements.txt
@@ -2,585 +2,544 @@
 # This file is autogenerated by pip-compile with Python 3.10
 # by the following command:
 #
-#    pip-compile --generate-hashes --output-file=requirements.txt requirements.in
+#    pip-compile --generate-hashes --output-file=requirements.txt -
 #
-absl-py==2.1.0 \
-    --hash=sha256:526a04eadab8b4ee719ce68f204172ead1027549089702d99b9059f129ff1308 \
-    --hash=sha256:7820790efbb316739cde8b4e19357243fc3608a152024288513dd968d7d959ff
+absl-py==1.4.0 \
+    --hash=sha256:0d3fe606adfa4f7db64792dd4c7aee4ee0c38ab75dfd353b7a83ed3e957fcb47 \
+    --hash=sha256:d2c244d01048ba476e7c080bd2c6df5e141d211de80223460d5b3b8a2a58433d
     # via
-    #   keras
     #   tensorboard
     #   tensorflow-cpu
 astunparse==1.6.3 \
     --hash=sha256:5ad93a8456f0d084c3456d059fd9a92cce667963232cbf763eac3bc5b7940872 \
     --hash=sha256:c2652417f2c8b5bb325c885ae329bdf3f86424075c4fd1a128674bc6fba4b8e8
     # via tensorflow-cpu
-certifi==2024.2.2 \
-    --hash=sha256:0569859f95fc761b18b45ef421b1290a0f65f147e92a1e5eb3e635f9a5e4e66f \
-    --hash=sha256:dc383c07b76109f368f6106eee2b593b04a011ea4d55f652c6ca24a754d1cdd1
+cachetools==5.3.0 \
+    --hash=sha256:13dfddc7b8df938c21a940dfa6557ce6e94a2f1cdfa58eb90c805721d58f2c14 \
+    --hash=sha256:429e1a1e845c008ea6c85aa35d4b98b65d6a9763eeef3e37e92728a12d1de9d4
+    # via google-auth
+certifi==2022.12.7 \
+    --hash=sha256:35824b4c3a97115964b408844d64aa14db1cc518f6562e8d7261699d1350a9e3 \
+    --hash=sha256:4ad3232f5e926d6718ec31cfc1fcadfde020920e278684144551c91769c7bc18
     # via requests
-charset-normalizer==3.3.2 \
-    --hash=sha256:06435b539f889b1f6f4ac1758871aae42dc3a8c0e24ac9e60c2384973ad73027 \
-    --hash=sha256:06a81e93cd441c56a9b65d8e1d043daeb97a3d0856d177d5c90ba85acb3db087 \
-    --hash=sha256:0a55554a2fa0d408816b3b5cedf0045f4b8e1a6065aec45849de2d6f3f8e9786 \
-    --hash=sha256:0b2b64d2bb6d3fb9112bafa732def486049e63de9618b5843bcdd081d8144cd8 \
-    --hash=sha256:10955842570876604d404661fbccbc9c7e684caf432c09c715ec38fbae45ae09 \
-    --hash=sha256:122c7fa62b130ed55f8f285bfd56d5f4b4a5b503609d181f9ad85e55c89f4185 \
-    --hash=sha256:1ceae2f17a9c33cb48e3263960dc5fc8005351ee19db217e9b1bb15d28c02574 \
-    --hash=sha256:1d3193f4a680c64b4b6a9115943538edb896edc190f0b222e73761716519268e \
-    --hash=sha256:1f79682fbe303db92bc2b1136016a38a42e835d932bab5b3b1bfcfbf0640e519 \
-    --hash=sha256:2127566c664442652f024c837091890cb1942c30937add288223dc895793f898 \
-    --hash=sha256:22afcb9f253dac0696b5a4be4a1c0f8762f8239e21b99680099abd9b2b1b2269 \
-    --hash=sha256:25baf083bf6f6b341f4121c2f3c548875ee6f5339300e08be3f2b2ba1721cdd3 \
-    --hash=sha256:2e81c7b9c8979ce92ed306c249d46894776a909505d8f5a4ba55b14206e3222f \
-    --hash=sha256:3287761bc4ee9e33561a7e058c72ac0938c4f57fe49a09eae428fd88aafe7bb6 \
-    --hash=sha256:34d1c8da1e78d2e001f363791c98a272bb734000fcef47a491c1e3b0505657a8 \
-    --hash=sha256:37e55c8e51c236f95b033f6fb391d7d7970ba5fe7ff453dad675e88cf303377a \
-    --hash=sha256:3d47fa203a7bd9c5b6cee4736ee84ca03b8ef23193c0d1ca99b5089f72645c73 \
-    --hash=sha256:3e4d1f6587322d2788836a99c69062fbb091331ec940e02d12d179c1d53e25fc \
-    --hash=sha256:42cb296636fcc8b0644486d15c12376cb9fa75443e00fb25de0b8602e64c1714 \
-    --hash=sha256:45485e01ff4d3630ec0d9617310448a8702f70e9c01906b0d0118bdf9d124cf2 \
-    --hash=sha256:4a78b2b446bd7c934f5dcedc588903fb2f5eec172f3d29e52a9096a43722adfc \
-    --hash=sha256:4ab2fe47fae9e0f9dee8c04187ce5d09f48eabe611be8259444906793ab7cbce \
-    --hash=sha256:4d0d1650369165a14e14e1e47b372cfcb31d6ab44e6e33cb2d4e57265290044d \
-    --hash=sha256:549a3a73da901d5bc3ce8d24e0600d1fa85524c10287f6004fbab87672bf3e1e \
-    --hash=sha256:55086ee1064215781fff39a1af09518bc9255b50d6333f2e4c74ca09fac6a8f6 \
-    --hash=sha256:572c3763a264ba47b3cf708a44ce965d98555f618ca42c926a9c1616d8f34269 \
-    --hash=sha256:573f6eac48f4769d667c4442081b1794f52919e7edada77495aaed9236d13a96 \
-    --hash=sha256:5b4c145409bef602a690e7cfad0a15a55c13320ff7a3ad7ca59c13bb8ba4d45d \
-    --hash=sha256:6463effa3186ea09411d50efc7d85360b38d5f09b870c48e4600f63af490e56a \
-    --hash=sha256:65f6f63034100ead094b8744b3b97965785388f308a64cf8d7c34f2f2e5be0c4 \
-    --hash=sha256:663946639d296df6a2bb2aa51b60a2454ca1cb29835324c640dafb5ff2131a77 \
-    --hash=sha256:6897af51655e3691ff853668779c7bad41579facacf5fd7253b0133308cf000d \
-    --hash=sha256:68d1f8a9e9e37c1223b656399be5d6b448dea850bed7d0f87a8311f1ff3dabb0 \
-    --hash=sha256:6ac7ffc7ad6d040517be39eb591cac5ff87416c2537df6ba3cba3bae290c0fed \
-    --hash=sha256:6b3251890fff30ee142c44144871185dbe13b11bab478a88887a639655be1068 \
-    --hash=sha256:6c4caeef8fa63d06bd437cd4bdcf3ffefe6738fb1b25951440d80dc7df8c03ac \
-    --hash=sha256:6ef1d82a3af9d3eecdba2321dc1b3c238245d890843e040e41e470ffa64c3e25 \
-    --hash=sha256:753f10e867343b4511128c6ed8c82f7bec3bd026875576dfd88483c5c73b2fd8 \
-    --hash=sha256:7cd13a2e3ddeed6913a65e66e94b51d80a041145a026c27e6bb76c31a853c6ab \
-    --hash=sha256:7ed9e526742851e8d5cc9e6cf41427dfc6068d4f5a3bb03659444b4cabf6bc26 \
-    --hash=sha256:7f04c839ed0b6b98b1a7501a002144b76c18fb1c1850c8b98d458ac269e26ed2 \
-    --hash=sha256:802fe99cca7457642125a8a88a084cef28ff0cf9407060f7b93dca5aa25480db \
-    --hash=sha256:80402cd6ee291dcb72644d6eac93785fe2c8b9cb30893c1af5b8fdd753b9d40f \
-    --hash=sha256:8465322196c8b4d7ab6d1e049e4c5cb460d0394da4a27d23cc242fbf0034b6b5 \
-    --hash=sha256:86216b5cee4b06df986d214f664305142d9c76df9b6512be2738aa72a2048f99 \
-    --hash=sha256:87d1351268731db79e0f8e745d92493ee2841c974128ef629dc518b937d9194c \
-    --hash=sha256:8bdb58ff7ba23002a4c5808d608e4e6c687175724f54a5dade5fa8c67b604e4d \
-    --hash=sha256:8c622a5fe39a48f78944a87d4fb8a53ee07344641b0562c540d840748571b811 \
-    --hash=sha256:8d756e44e94489e49571086ef83b2bb8ce311e730092d2c34ca8f7d925cb20aa \
-    --hash=sha256:8f4a014bc36d3c57402e2977dada34f9c12300af536839dc38c0beab8878f38a \
-    --hash=sha256:9063e24fdb1e498ab71cb7419e24622516c4a04476b17a2dab57e8baa30d6e03 \
-    --hash=sha256:90d558489962fd4918143277a773316e56c72da56ec7aa3dc3dbbe20fdfed15b \
-    --hash=sha256:923c0c831b7cfcb071580d3f46c4baf50f174be571576556269530f4bbd79d04 \
-    --hash=sha256:95f2a5796329323b8f0512e09dbb7a1860c46a39da62ecb2324f116fa8fdc85c \
-    --hash=sha256:96b02a3dc4381e5494fad39be677abcb5e6634bf7b4fa83a6dd3112607547001 \
-    --hash=sha256:9f96df6923e21816da7e0ad3fd47dd8f94b2a5ce594e00677c0013018b813458 \
-    --hash=sha256:a10af20b82360ab00827f916a6058451b723b4e65030c5a18577c8b2de5b3389 \
-    --hash=sha256:a50aebfa173e157099939b17f18600f72f84eed3049e743b68ad15bd69b6bf99 \
-    --hash=sha256:a981a536974bbc7a512cf44ed14938cf01030a99e9b3a06dd59578882f06f985 \
-    --hash=sha256:a9a8e9031d613fd2009c182b69c7b2c1ef8239a0efb1df3f7c8da66d5dd3d537 \
-    --hash=sha256:ae5f4161f18c61806f411a13b0310bea87f987c7d2ecdbdaad0e94eb2e404238 \
-    --hash=sha256:aed38f6e4fb3f5d6bf81bfa990a07806be9d83cf7bacef998ab1a9bd660a581f \
-    --hash=sha256:b01b88d45a6fcb69667cd6d2f7a9aeb4bf53760d7fc536bf679ec94fe9f3ff3d \
-    --hash=sha256:b261ccdec7821281dade748d088bb6e9b69e6d15b30652b74cbbac25e280b796 \
-    --hash=sha256:b2b0a0c0517616b6869869f8c581d4eb2dd83a4d79e0ebcb7d373ef9956aeb0a \
-    --hash=sha256:b4a23f61ce87adf89be746c8a8974fe1c823c891d8f86eb218bb957c924bb143 \
-    --hash=sha256:bd8f7df7d12c2db9fab40bdd87a7c09b1530128315d047a086fa3ae3435cb3a8 \
-    --hash=sha256:beb58fe5cdb101e3a055192ac291b7a21e3b7ef4f67fa1d74e331a7f2124341c \
-    --hash=sha256:c002b4ffc0be611f0d9da932eb0f704fe2602a9a949d1f738e4c34c75b0863d5 \
-    --hash=sha256:c083af607d2515612056a31f0a8d9e0fcb5876b7bfc0abad3ecd275bc4ebc2d5 \
-    --hash=sha256:c180f51afb394e165eafe4ac2936a14bee3eb10debc9d9e4db8958fe36afe711 \
-    --hash=sha256:c235ebd9baae02f1b77bcea61bce332cb4331dc3617d254df3323aa01ab47bd4 \
-    --hash=sha256:cd70574b12bb8a4d2aaa0094515df2463cb429d8536cfb6c7ce983246983e5a6 \
-    --hash=sha256:d0eccceffcb53201b5bfebb52600a5fb483a20b61da9dbc885f8b103cbe7598c \
-    --hash=sha256:d965bba47ddeec8cd560687584e88cf699fd28f192ceb452d1d7ee807c5597b7 \
-    --hash=sha256:db364eca23f876da6f9e16c9da0df51aa4f104a972735574842618b8c6d999d4 \
-    --hash=sha256:ddbb2551d7e0102e7252db79ba445cdab71b26640817ab1e3e3648dad515003b \
-    --hash=sha256:deb6be0ac38ece9ba87dea880e438f25ca3eddfac8b002a2ec3d9183a454e8ae \
-    --hash=sha256:e06ed3eb3218bc64786f7db41917d4e686cc4856944f53d5bdf83a6884432e12 \
-    --hash=sha256:e27ad930a842b4c5eb8ac0016b0a54f5aebbe679340c26101df33424142c143c \
-    --hash=sha256:e537484df0d8f426ce2afb2d0f8e1c3d0b114b83f8850e5f2fbea0e797bd82ae \
-    --hash=sha256:eb00ed941194665c332bf8e078baf037d6c35d7c4f3102ea2d4f16ca94a26dc8 \
-    --hash=sha256:eb6904c354526e758fda7167b33005998fb68c46fbc10e013ca97f21ca5c8887 \
-    --hash=sha256:eb8821e09e916165e160797a6c17edda0679379a4be5c716c260e836e122f54b \
-    --hash=sha256:efcb3f6676480691518c177e3b465bcddf57cea040302f9f4e6e191af91174d4 \
-    --hash=sha256:f27273b60488abe721a075bcca6d7f3964f9f6f067c8c4c605743023d7d3944f \
-    --hash=sha256:f30c3cb33b24454a82faecaf01b19c18562b1e89558fb6c56de4d9118a032fd5 \
-    --hash=sha256:fb69256e180cb6c8a894fee62b3afebae785babc1ee98b81cdf68bbca1987f33 \
-    --hash=sha256:fd1abc0d89e30cc4e02e4064dc67fcc51bd941eb395c502aac3ec19fab46b519 \
-    --hash=sha256:ff8fa367d09b717b2a17a052544193ad76cd49979c805768879cb63d9ca50561
+charset-normalizer==3.0.1 \
+    --hash=sha256:00d3ffdaafe92a5dc603cb9bd5111aaa36dfa187c8285c543be562e61b755f6b \
+    --hash=sha256:024e606be3ed92216e2b6952ed859d86b4cfa52cd5bc5f050e7dc28f9b43ec42 \
+    --hash=sha256:0298eafff88c99982a4cf66ba2efa1128e4ddaca0b05eec4c456bbc7db691d8d \
+    --hash=sha256:02a51034802cbf38db3f89c66fb5d2ec57e6fe7ef2f4a44d070a593c3688667b \
+    --hash=sha256:083c8d17153ecb403e5e1eb76a7ef4babfc2c48d58899c98fcaa04833e7a2f9a \
+    --hash=sha256:0a11e971ed097d24c534c037d298ad32c6ce81a45736d31e0ff0ad37ab437d59 \
+    --hash=sha256:0bf2dae5291758b6f84cf923bfaa285632816007db0330002fa1de38bfcb7154 \
+    --hash=sha256:0c0a590235ccd933d9892c627dec5bc7511ce6ad6c1011fdf5b11363022746c1 \
+    --hash=sha256:0f438ae3532723fb6ead77e7c604be7c8374094ef4ee2c5e03a3a17f1fca256c \
+    --hash=sha256:109487860ef6a328f3eec66f2bf78b0b72400280d8f8ea05f69c51644ba6521a \
+    --hash=sha256:11b53acf2411c3b09e6af37e4b9005cba376c872503c8f28218c7243582df45d \
+    --hash=sha256:12db3b2c533c23ab812c2b25934f60383361f8a376ae272665f8e48b88e8e1c6 \
+    --hash=sha256:14e76c0f23218b8f46c4d87018ca2e441535aed3632ca134b10239dfb6dadd6b \
+    --hash=sha256:16a8663d6e281208d78806dbe14ee9903715361cf81f6d4309944e4d1e59ac5b \
+    --hash=sha256:292d5e8ba896bbfd6334b096e34bffb56161c81408d6d036a7dfa6929cff8783 \
+    --hash=sha256:2c03cc56021a4bd59be889c2b9257dae13bf55041a3372d3295416f86b295fb5 \
+    --hash=sha256:2e396d70bc4ef5325b72b593a72c8979999aa52fb8bcf03f701c1b03e1166918 \
+    --hash=sha256:2edb64ee7bf1ed524a1da60cdcd2e1f6e2b4f66ef7c077680739f1641f62f555 \
+    --hash=sha256:31a9ddf4718d10ae04d9b18801bd776693487cbb57d74cc3458a7673f6f34639 \
+    --hash=sha256:356541bf4381fa35856dafa6a965916e54bed415ad8a24ee6de6e37deccf2786 \
+    --hash=sha256:358a7c4cb8ba9b46c453b1dd8d9e431452d5249072e4f56cfda3149f6ab1405e \
+    --hash=sha256:37f8febc8ec50c14f3ec9637505f28e58d4f66752207ea177c1d67df25da5aed \
+    --hash=sha256:39049da0ffb96c8cbb65cbf5c5f3ca3168990adf3551bd1dee10c48fce8ae820 \
+    --hash=sha256:39cf9ed17fe3b1bc81f33c9ceb6ce67683ee7526e65fde1447c772afc54a1bb8 \
+    --hash=sha256:3ae1de54a77dc0d6d5fcf623290af4266412a7c4be0b1ff7444394f03f5c54e3 \
+    --hash=sha256:3b590df687e3c5ee0deef9fc8c547d81986d9a1b56073d82de008744452d6541 \
+    --hash=sha256:3e45867f1f2ab0711d60c6c71746ac53537f1684baa699f4f668d4c6f6ce8e14 \
+    --hash=sha256:3fc1c4a2ffd64890aebdb3f97e1278b0cc72579a08ca4de8cd2c04799a3a22be \
+    --hash=sha256:4457ea6774b5611f4bed5eaa5df55f70abde42364d498c5134b7ef4c6958e20e \
+    --hash=sha256:44ba614de5361b3e5278e1241fda3dc1838deed864b50a10d7ce92983797fa76 \
+    --hash=sha256:4a8fcf28c05c1f6d7e177a9a46a1c52798bfe2ad80681d275b10dcf317deaf0b \
+    --hash=sha256:4b0d02d7102dd0f997580b51edc4cebcf2ab6397a7edf89f1c73b586c614272c \
+    --hash=sha256:502218f52498a36d6bf5ea77081844017bf7982cdbe521ad85e64cabee1b608b \
+    --hash=sha256:503e65837c71b875ecdd733877d852adbc465bd82c768a067badd953bf1bc5a3 \
+    --hash=sha256:5995f0164fa7df59db4746112fec3f49c461dd6b31b841873443bdb077c13cfc \
+    --hash=sha256:59e5686dd847347e55dffcc191a96622f016bc0ad89105e24c14e0d6305acbc6 \
+    --hash=sha256:601f36512f9e28f029d9481bdaf8e89e5148ac5d89cffd3b05cd533eeb423b59 \
+    --hash=sha256:608862a7bf6957f2333fc54ab4399e405baad0163dc9f8d99cb236816db169d4 \
+    --hash=sha256:62595ab75873d50d57323a91dd03e6966eb79c41fa834b7a1661ed043b2d404d \
+    --hash=sha256:70990b9c51340e4044cfc394a81f614f3f90d41397104d226f21e66de668730d \
+    --hash=sha256:71140351489970dfe5e60fc621ada3e0f41104a5eddaca47a7acb3c1b851d6d3 \
+    --hash=sha256:72966d1b297c741541ca8cf1223ff262a6febe52481af742036a0b296e35fa5a \
+    --hash=sha256:74292fc76c905c0ef095fe11e188a32ebd03bc38f3f3e9bcb85e4e6db177b7ea \
+    --hash=sha256:761e8904c07ad053d285670f36dd94e1b6ab7f16ce62b9805c475b7aa1cffde6 \
+    --hash=sha256:772b87914ff1152b92a197ef4ea40efe27a378606c39446ded52c8f80f79702e \
+    --hash=sha256:79909e27e8e4fcc9db4addea88aa63f6423ebb171db091fb4373e3312cb6d603 \
+    --hash=sha256:7e189e2e1d3ed2f4aebabd2d5b0f931e883676e51c7624826e0a4e5fe8a0bf24 \
+    --hash=sha256:7eb33a30d75562222b64f569c642ff3dc6689e09adda43a082208397f016c39a \
+    --hash=sha256:81d6741ab457d14fdedc215516665050f3822d3e56508921cc7239f8c8e66a58 \
+    --hash=sha256:8499ca8f4502af841f68135133d8258f7b32a53a1d594aa98cc52013fff55678 \
+    --hash=sha256:84c3990934bae40ea69a82034912ffe5a62c60bbf6ec5bc9691419641d7d5c9a \
+    --hash=sha256:87701167f2a5c930b403e9756fab1d31d4d4da52856143b609e30a1ce7160f3c \
+    --hash=sha256:88600c72ef7587fe1708fd242b385b6ed4b8904976d5da0893e31df8b3480cb6 \
+    --hash=sha256:8ac7b6a045b814cf0c47f3623d21ebd88b3e8cf216a14790b455ea7ff0135d18 \
+    --hash=sha256:8b8af03d2e37866d023ad0ddea594edefc31e827fee64f8de5611a1dbc373174 \
+    --hash=sha256:8c7fe7afa480e3e82eed58e0ca89f751cd14d767638e2550c77a92a9e749c317 \
+    --hash=sha256:8eade758719add78ec36dc13201483f8e9b5d940329285edcd5f70c0a9edbd7f \
+    --hash=sha256:911d8a40b2bef5b8bbae2e36a0b103f142ac53557ab421dc16ac4aafee6f53dc \
+    --hash=sha256:93ad6d87ac18e2a90b0fe89df7c65263b9a99a0eb98f0a3d2e079f12a0735837 \
+    --hash=sha256:95dea361dd73757c6f1c0a1480ac499952c16ac83f7f5f4f84f0658a01b8ef41 \
+    --hash=sha256:9ab77acb98eba3fd2a85cd160851816bfce6871d944d885febf012713f06659c \
+    --hash=sha256:9cb3032517f1627cc012dbc80a8ec976ae76d93ea2b5feaa9d2a5b8882597579 \
+    --hash=sha256:9cf4e8ad252f7c38dd1f676b46514f92dc0ebeb0db5552f5f403509705e24753 \
+    --hash=sha256:9d9153257a3f70d5f69edf2325357251ed20f772b12e593f3b3377b5f78e7ef8 \
+    --hash=sha256:a152f5f33d64a6be73f1d30c9cc82dfc73cec6477ec268e7c6e4c7d23c2d2291 \
+    --hash=sha256:a16418ecf1329f71df119e8a65f3aa68004a3f9383821edcb20f0702934d8087 \
+    --hash=sha256:a60332922359f920193b1d4826953c507a877b523b2395ad7bc716ddd386d866 \
+    --hash=sha256:a8d0fc946c784ff7f7c3742310cc8a57c5c6dc31631269876a88b809dbeff3d3 \
+    --hash=sha256:ab5de034a886f616a5668aa5d098af2b5385ed70142090e2a31bcbd0af0fdb3d \
+    --hash=sha256:c22d3fe05ce11d3671297dc8973267daa0f938b93ec716e12e0f6dee81591dc1 \
+    --hash=sha256:c2ac1b08635a8cd4e0cbeaf6f5e922085908d48eb05d44c5ae9eabab148512ca \
+    --hash=sha256:c512accbd6ff0270939b9ac214b84fb5ada5f0409c44298361b2f5e13f9aed9e \
+    --hash=sha256:c75ffc45f25324e68ab238cb4b5c0a38cd1c3d7f1fb1f72b5541de469e2247db \
+    --hash=sha256:c95a03c79bbe30eec3ec2b7f076074f4281526724c8685a42872974ef4d36b72 \
+    --hash=sha256:cadaeaba78750d58d3cc6ac4d1fd867da6fc73c88156b7a3212a3cd4819d679d \
+    --hash=sha256:cd6056167405314a4dc3c173943f11249fa0f1b204f8b51ed4bde1a9cd1834dc \
+    --hash=sha256:db72b07027db150f468fbada4d85b3b2729a3db39178abf5c543b784c1254539 \
+    --hash=sha256:df2c707231459e8a4028eabcd3cfc827befd635b3ef72eada84ab13b52e1574d \
+    --hash=sha256:e62164b50f84e20601c1ff8eb55620d2ad25fb81b59e3cd776a1902527a788af \
+    --hash=sha256:e696f0dd336161fca9adbb846875d40752e6eba585843c768935ba5c9960722b \
+    --hash=sha256:eaa379fcd227ca235d04152ca6704c7cb55564116f8bc52545ff357628e10602 \
+    --hash=sha256:ebea339af930f8ca5d7a699b921106c6e29c617fe9606fa7baa043c1cdae326f \
+    --hash=sha256:f4c39b0e3eac288fedc2b43055cfc2ca7a60362d0e5e87a637beac5d801ef478 \
+    --hash=sha256:f5057856d21e7586765171eac8b9fc3f7d44ef39425f85dbcccb13b3ebea806c \
+    --hash=sha256:f6f45710b4459401609ebebdbcfb34515da4fc2aa886f95107f556ac69a9147e \
+    --hash=sha256:f97e83fa6c25693c7a35de154681fcc257c1c41b38beb0304b9c4d2d9e164479 \
+    --hash=sha256:f9d0c5c045a3ca9bedfc35dca8526798eb91a07aa7a2c0fee134c6c6f321cbd7 \
+    --hash=sha256:ff6f3db31555657f3163b15a6b7c6938d08df7adbfc9dd13d9d19edad678f1e8
     # via requests
-flatbuffers==24.3.25 \
-    --hash=sha256:8dbdec58f935f3765e4f7f3cf635ac3a77f83568138d6a2311f524ec96364812 \
-    --hash=sha256:de2ec5b203f21441716617f38443e0a8ebf3d25bf0d9c0bb0ce68fa00ad546a4
+flatbuffers==23.1.21 \
+    --hash=sha256:2e4101b291b14f21e87ea20b7bf7127b11563f6084e352d2d708bddd545c9265 \
+    --hash=sha256:a948913bbb5d83c43a1193d7943c90e6c0ab732e7f2983111104250aeb61ff85
     # via tensorflow-cpu
-gast==0.5.4 \
-    --hash=sha256:6fc4fa5fa10b72fb8aab4ae58bcb023058386e67b6fa2e3e34cec5c769360316 \
-    --hash=sha256:9c270fe5f4b130969b54174de7db4e764b09b4f7f67ccfc32480e29f78348d97
+gast==0.4.0 \
+    --hash=sha256:40feb7b8b8434785585ab224d1568b857edb18297e5a3047f1ba012bc83b42c1 \
+    --hash=sha256:b7adcdd5adbebf1adf17378da5ba3f543684dbec47b1cda1f3997e573cd542c4
     # via tensorflow-cpu
+google-auth==2.16.2 \
+    --hash=sha256:07e14f34ec288e3f33e00e2e3cc40c8942aa5d4ceac06256a28cd8e786591420 \
+    --hash=sha256:2fef3cf94876d1a0e204afece58bb4d83fb57228aaa366c64045039fda6770a2
+    # via
+    #   google-auth-oauthlib
+    #   tensorboard
+google-auth-oauthlib==0.4.6 \
+    --hash=sha256:3f2a6e802eebbb6fb736a370fbf3b055edcb6b52878bf2f26330b5e041316c73 \
+    --hash=sha256:a90a072f6993f2c327067bf65270046384cda5a8ecb20b94ea9a687f1f233a7a
+    # via tensorboard
 google-pasta==0.2.0 \
     --hash=sha256:4612951da876b1a10fe3960d7226f0c7682cf901e16ac06e473b267a5afa8954 \
     --hash=sha256:b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed \
     --hash=sha256:c9f2c8dfc8f96d0d5808299920721be30c9eec37f2389f28904f454565c8a16e
     # via tensorflow-cpu
-grpcio==1.64.0 \
-    --hash=sha256:01615bbcae6875eee8091e6b9414072f4e4b00d8b7e141f89635bdae7cf784e5 \
-    --hash=sha256:02cc9cc3f816d30f7993d0d408043b4a7d6a02346d251694d8ab1f78cc723e7e \
-    --hash=sha256:0b2dfe6dcace264807d9123d483d4c43274e3f8c39f90ff51de538245d7a4145 \
-    --hash=sha256:0da1d921f8e4bcee307aeef6c7095eb26e617c471f8cb1c454fd389c5c296d1e \
-    --hash=sha256:0f30596cdcbed3c98024fb4f1d91745146385b3f9fd10c9f2270cbfe2ed7ed91 \
-    --hash=sha256:1ce4cd5a61d4532651079e7aae0fedf9a80e613eed895d5b9743e66b52d15812 \
-    --hash=sha256:1f279ad72dd7d64412e10f2443f9f34872a938c67387863c4cd2fb837f53e7d2 \
-    --hash=sha256:1f5de082d936e0208ce8db9095821361dfa97af8767a6607ae71425ac8ace15c \
-    --hash=sha256:1f8ea18b928e539046bb5f9c124d717fbf00cc4b2d960ae0b8468562846f5aa1 \
-    --hash=sha256:2186d76a7e383e1466e0ea2b0febc343ffeae13928c63c6ec6826533c2d69590 \
-    --hash=sha256:23b6887bb21d77649d022fa1859e05853fdc2e60682fd86c3db652a555a282e0 \
-    --hash=sha256:257baf07f53a571c215eebe9679c3058a313fd1d1f7c4eede5a8660108c52d9c \
-    --hash=sha256:2a18090371d138a57714ee9bffd6c9c9cb2e02ce42c681aac093ae1e7189ed21 \
-    --hash=sha256:2e8fabe2cc57a369638ab1ad8e6043721014fdf9a13baa7c0e35995d3a4a7618 \
-    --hash=sha256:3161a8f8bb38077a6470508c1a7301cd54301c53b8a34bb83e3c9764874ecabd \
-    --hash=sha256:31890b24d47b62cc27da49a462efe3d02f3c120edb0e6c46dcc0025506acf004 \
-    --hash=sha256:3550493ac1d23198d46dc9c9b24b411cef613798dc31160c7138568ec26bc9b4 \
-    --hash=sha256:3b09c3d9de95461214a11d82cc0e6a46a6f4e1f91834b50782f932895215e5db \
-    --hash=sha256:3d2004e85cf5213995d09408501f82c8534700d2babeb81dfdba2a3bff0bb396 \
-    --hash=sha256:46b8b43ba6a2a8f3103f103f97996cad507bcfd72359af6516363c48793d5a7b \
-    --hash=sha256:579dd9fb11bc73f0de061cab5f8b2def21480fd99eb3743ed041ad6a1913ee2f \
-    --hash=sha256:597191370951b477b7a1441e1aaa5cacebeb46a3b0bd240ec3bb2f28298c7553 \
-    --hash=sha256:59c68df3a934a586c3473d15956d23a618b8f05b5e7a3a904d40300e9c69cbf0 \
-    --hash=sha256:5a56797dea8c02e7d3a85dfea879f286175cf4d14fbd9ab3ef2477277b927baa \
-    --hash=sha256:650a8150a9b288f40d5b7c1d5400cc11724eae50bd1f501a66e1ea949173649b \
-    --hash=sha256:6d5541eb460d73a07418524fb64dcfe0adfbcd32e2dac0f8f90ce5b9dd6c046c \
-    --hash=sha256:6ec5ed15b4ffe56e2c6bc76af45e6b591c9be0224b3fb090adfb205c9012367d \
-    --hash=sha256:73f84f9e5985a532e47880b3924867de16fa1aa513fff9b26106220c253c70c5 \
-    --hash=sha256:753cb58683ba0c545306f4e17dabf468d29cb6f6b11832e1e432160bb3f8403c \
-    --hash=sha256:7c1f5b2298244472bcda49b599be04579f26425af0fd80d3f2eb5fd8bc84d106 \
-    --hash=sha256:7e013428ab472892830287dd082b7d129f4d8afef49227a28223a77337555eaa \
-    --hash=sha256:7f17572dc9acd5e6dfd3014d10c0b533e9f79cd9517fc10b0225746f4c24b58e \
-    --hash=sha256:85fda90b81da25993aa47fae66cae747b921f8f6777550895fb62375b776a231 \
-    --hash=sha256:874c741c8a66f0834f653a69e7e64b4e67fcd4a8d40296919b93bab2ccc780ba \
-    --hash=sha256:8d598b5d5e2c9115d7fb7e2cb5508d14286af506a75950762aa1372d60e41851 \
-    --hash=sha256:8de0399b983f8676a7ccfdd45e5b2caec74a7e3cc576c6b1eecf3b3680deda5e \
-    --hash=sha256:a053584079b793a54bece4a7d1d1b5c0645bdbee729215cd433703dc2532f72b \
-    --hash=sha256:a54362f03d4dcfae63be455d0a7d4c1403673498b92c6bfe22157d935b57c7a9 \
-    --hash=sha256:aca4f15427d2df592e0c8f3d38847e25135e4092d7f70f02452c0e90d6a02d6d \
-    --hash=sha256:b2cbdfba18408389a1371f8c2af1659119e1831e5ed24c240cae9e27b4abc38d \
-    --hash=sha256:b52e1ec7185512103dd47d41cf34ea78e7a7361ba460187ddd2416b480e0938c \
-    --hash=sha256:c46fb6bfca17bfc49f011eb53416e61472fa96caa0979b4329176bdd38cbbf2a \
-    --hash=sha256:c56c91bd2923ddb6e7ed28ebb66d15633b03e0df22206f22dfcdde08047e0a48 \
-    --hash=sha256:cf4c8daed18ae2be2f1fc7d613a76ee2a2e28fdf2412d5c128be23144d28283d \
-    --hash=sha256:d7b7bf346391dffa182fba42506adf3a84f4a718a05e445b37824136047686a1 \
-    --hash=sha256:d9171f025a196f5bcfec7e8e7ffb7c3535f7d60aecd3503f9e250296c7cfc150
+grpcio==1.51.3 \
+    --hash=sha256:040eb421613b57c696063abde405916dd830203c184c9000fc8c3b3b3c950325 \
+    --hash=sha256:165b05af77e6aecb4210ae7663e25acf234ba78a7c1c157fa5f2efeb0d6ec53c \
+    --hash=sha256:200d69857f9910f7458b39b9bcf83ee4a180591b40146ba9e49314e3a7419313 \
+    --hash=sha256:22bdfac4f7f27acdd4da359b5e7e1973dc74bf1ed406729b07d0759fde2f064b \
+    --hash=sha256:2a8e17286c4240137d933b8ca506465472248b4ce0fe46f3404459e708b65b68 \
+    --hash=sha256:2cd2e4cefb724cab1ba2df4b7535a9980531b9ec51b4dbb5f137a1f3a3754ef0 \
+    --hash=sha256:2f8ff75e61e1227ba7a3f16b2eadbcc11d0a54096d52ab75a6b88cfbe56f55d1 \
+    --hash=sha256:2fdd6333ce96435408565a9dbbd446212cd5d62e4d26f6a3c0feb1e3c35f1cc8 \
+    --hash=sha256:30e09b5e0531685e176f49679b6a3b190762cc225f4565e55a899f5e14b3aa62 \
+    --hash=sha256:3667c06e37d6cd461afdd51cefe6537702f3d1dc5ff4cac07e88d8b4795dc16f \
+    --hash=sha256:36c8abbc5f837111e7bd619612eedc223c290b0903b952ce0c7b00840ea70f14 \
+    --hash=sha256:3709048fe0aa23dda09b3e69849a12055790171dab9e399a72ea8f9dfbf9ac80 \
+    --hash=sha256:3c1b9f8afa62ff265d86a4747a2990ec5a96e4efce5d5888f245a682d66eca47 \
+    --hash=sha256:3ea4341efe603b049e8c9a5f13c696ca37fcdf8a23ca35f650428ad3606381d9 \
+    --hash=sha256:3f9a7d88082b2a17ae7bd3c2354d13bab0453899e0851733f6afa6918373f476 \
+    --hash=sha256:49ede0528e9dac7e8a9fe30b16c73b630ddd9a576bf4b675eb6b0c53ee5ca00f \
+    --hash=sha256:54b0c29bdd9a3b1e1b61443ab152f060fc719f1c083127ab08d03fac5efd51be \
+    --hash=sha256:54e36c2ee304ff15f2bfbdc43d2b56c63331c52d818c364e5b5214e5bc2ad9f6 \
+    --hash=sha256:5694448256e3cdfe5bd358f1574a3f2f51afa20cc834713c4b9788d60b7cc646 \
+    --hash=sha256:5e77ee138100f0bb55cbd147840f87ee6241dbd25f09ea7cd8afe7efff323449 \
+    --hash=sha256:5eed34994c095e2bf7194ffac7381c6068b057ef1e69f8f08db77771350a7566 \
+    --hash=sha256:6604f614016127ae10969176bbf12eb0e03d2fb3d643f050b3b69e160d144fb4 \
+    --hash=sha256:68a7514b754e38e8de9075f7bb4dee919919515ec68628c43a894027e40ddec4 \
+    --hash=sha256:6972b009638b40a448d10e1bc18e2223143b8a7aa20d7def0d78dd4af4126d12 \
+    --hash=sha256:6c677581ce129f5fa228b8f418cee10bd28dd449f3a544ea73c8ba590ee49d0b \
+    --hash=sha256:6c99a73a6260bdf844b2e5ddad02dcd530310f80e1fa72c300fa19c1c7496962 \
+    --hash=sha256:82b0ad8ac825d4bb31bff9f638557c045f4a6d824d84b21e893968286f88246b \
+    --hash=sha256:881ecb34feabf31c6b3b9bbbddd1a5b57e69f805041e5a2c6c562a28574f71c4 \
+    --hash=sha256:8de30f0b417744288cec65ec8cf84b8a57995cf7f1e84ccad2704d93f05d0aae \
+    --hash=sha256:b69c7adc7ed60da1cb1b502853db61f453fc745f940cbcc25eb97c99965d8f41 \
+    --hash=sha256:be1bf35ce82cdbcac14e39d5102d8de4079a1c1a6a06b68e41fcd9ef64f9dd28 \
+    --hash=sha256:be7b2265b7527bb12109a7727581e274170766d5b3c9258d4e466f4872522d7a \
+    --hash=sha256:c02abd55409bfb293371554adf6a4401197ec2133dd97727c01180889014ba4d \
+    --hash=sha256:c831f31336e81243f85b6daff3e5e8a123302ce0ea1f2726ad752fd7a59f3aee \
+    --hash=sha256:cd0daac21d9ef5e033a5100c1d3aa055bbed28bfcf070b12d8058045c4e821b1 \
+    --hash=sha256:cd9a5e68e79c5f031500e67793048a90209711e0854a9ddee8a3ce51728de4e5 \
+    --hash=sha256:d5cd1389669a847555df54177b911d9ff6f17345b2a6f19388707b7a9f724c88 \
+    --hash=sha256:d81528ffe0e973dc840ec73a4132fd18b8203ad129d7410155d951a0a7e4f5d0 \
+    --hash=sha256:e860a3222139b41d430939bbec2ec9c3f6c740938bf7a04471a9a8caaa965a2e \
+    --hash=sha256:e95c7ccd4c5807adef1602005513bf7c7d14e5a41daebcf9d8d30d8bf51b8f81 \
+    --hash=sha256:eafbe7501a3268d05f2e450e1ddaffb950d842a8620c13ec328b501d25d2e2c3 \
+    --hash=sha256:eef0450a4b5ed11feab639bf3eb1b6e23d0efa9b911bf7b06fb60e14f5f8a585 \
+    --hash=sha256:f601aaeae18dab81930fb8d4f916b0da21e89bb4b5f7367ef793f46b4a76b7b0 \
+    --hash=sha256:f7a0d0bf44438869d307f85a54f25a896ad6b4b0ca12370f76892ad732928d87 \
+    --hash=sha256:ffaaf7e93fcb437356b5a4b23bf36e8a3d0221399ff77fd057e4bc77776a24be
     # via
     #   tensorboard
     #   tensorflow-cpu
-h5py==3.11.0 \
-    --hash=sha256:083e0329ae534a264940d6513f47f5ada617da536d8dccbafc3026aefc33c90e \
-    --hash=sha256:1625fd24ad6cfc9c1ccd44a66dac2396e7ee74940776792772819fc69f3a3731 \
-    --hash=sha256:21dbdc5343f53b2e25404673c4f00a3335aef25521bd5fa8c707ec3833934892 \
-    --hash=sha256:52c416f8eb0daae39dabe71415cb531f95dce2d81e1f61a74537a50c63b28ab3 \
-    --hash=sha256:55106b04e2c83dfb73dc8732e9abad69d83a436b5b82b773481d95d17b9685e1 \
-    --hash=sha256:67462d0669f8f5459529de179f7771bd697389fcb3faab54d63bf788599a48ea \
-    --hash=sha256:6c4b760082626120031d7902cd983d8c1f424cdba2809f1067511ef283629d4b \
-    --hash=sha256:731839240c59ba219d4cb3bc5880d438248533366f102402cfa0621b71796b62 \
-    --hash=sha256:754c0c2e373d13d6309f408325343b642eb0f40f1a6ad21779cfa9502209e150 \
-    --hash=sha256:75bd7b3d93fbeee40860fd70cdc88df4464e06b70a5ad9ce1446f5f32eb84007 \
-    --hash=sha256:77b19a40788e3e362b54af4dcf9e6fde59ca016db2c61360aa30b47c7b7cef00 \
-    --hash=sha256:7b7e8f78072a2edec87c9836f25f34203fd492a4475709a18b417a33cfb21fa9 \
-    --hash=sha256:8ec9df3dd2018904c4cc06331951e274f3f3fd091e6d6cc350aaa90fa9b42a76 \
-    --hash=sha256:a76cae64080210389a571c7d13c94a1a6cf8cb75153044fd1f822a962c97aeab \
-    --hash=sha256:aa6ae84a14103e8dc19266ef4c3e5d7c00b68f21d07f2966f0ca7bdb6c2761fb \
-    --hash=sha256:bbd732a08187a9e2a6ecf9e8af713f1d68256ee0f7c8b652a32795670fb481ba \
-    --hash=sha256:c072655ad1d5fe9ef462445d3e77a8166cbfa5e599045f8aa3c19b75315f10e5 \
-    --hash=sha256:d9c944d364688f827dc889cf83f1fca311caf4fa50b19f009d1f2b525edd33a3 \
-    --hash=sha256:ef4e2f338fc763f50a8113890f455e1a70acd42a4d083370ceb80c463d803972 \
-    --hash=sha256:f3736fe21da2b7d8a13fe8fe415f1272d2a1ccdeff4849c1421d2fb30fd533bc \
-    --hash=sha256:f4e025e852754ca833401777c25888acb96889ee2c27e7e629a19aee288833f0
-    # via
-    #   keras
-    #   tensorflow-cpu
-idna==3.7 \
-    --hash=sha256:028ff3aadf0609c1fd278d8ea3089299412a7a8b9bd005dd08b9f8285bcb5cfc \
-    --hash=sha256:82fee1fc78add43492d3a1898bfa6d8a904cc97d8427f683ed8e798d07761aa0
+h5py==3.8.0 \
+    --hash=sha256:03890b1c123d024fb0239a3279737d5432498c1901c354f8b10d8221d1d16235 \
+    --hash=sha256:0fef76e10b9216657fa37e7edff6d8be0709b25bd5066474c229b56cf0098df9 \
+    --hash=sha256:26ffc344ec9984d2cd3ca0265007299a8bac8d85c1ad48f4639d8d3aed2af171 \
+    --hash=sha256:290e00fa2de74a10688d1bac98d5a9cdd43f14f58e562c580b5b3dfbd358ecae \
+    --hash=sha256:33b15aae79e9147aebe1d0e54099cbcde8d65e3e227cd5b59e49b1272aa0e09d \
+    --hash=sha256:36761693efbe53df179627a775476dcbc37727d6e920958277a7efbc18f1fb73 \
+    --hash=sha256:377865821fe80ad984d003723d6f8890bd54ceeb5981b43c0313b9df95411b30 \
+    --hash=sha256:49bc857635f935fa30e92e61ac1e87496df8f260a6945a3235e43a9890426866 \
+    --hash=sha256:4a506fc223def428f4329e7e1f9fe1c8c593eab226e7c0942c8d75308ad49950 \
+    --hash=sha256:533d7dad466ddb7e3b30af274b630eb7c1a6e4ddf01d1c373a0334dc2152110a \
+    --hash=sha256:5fd2252d1fc364ba0e93dd0b7089f4906b66805cb4e6aca7fa8874ac08649647 \
+    --hash=sha256:6fead82f0c4000cf38d53f9c030780d81bfa0220218aee13b90b7701c937d95f \
+    --hash=sha256:7f3350fc0a8407d668b13247861c2acd23f7f5fe7d060a3ad9b0820f5fcbcae0 \
+    --hash=sha256:8f55d9c6c84d7d09c79fb85979e97b81ec6071cc776a97eb6b96f8f6ec767323 \
+    --hash=sha256:98a240cd4c1bfd568aaa52ec42d263131a2582dab82d74d3d42a0d954cac12be \
+    --hash=sha256:9f6f6ffadd6bfa9b2c5b334805eb4b19ca0a5620433659d8f7fb86692c40a359 \
+    --hash=sha256:b685453e538b2b5934c58a644ac3f3b3d0cec1a01b6fb26d57388e9f9b674ad0 \
+    --hash=sha256:b7865de06779b14d98068da387333ad9bf2756b5b579cc887fac169bc08f87c3 \
+    --hash=sha256:bacaa1c16810dd2b3e4417f8e730971b7c4d53d234de61fe4a918db78e80e1e4 \
+    --hash=sha256:bae730580ae928de409d63cbe4fdca4c82c3ad2bed30511d19d34e995d63c77e \
+    --hash=sha256:c3389b63222b1c7a158bb7fe69d11ca00066740ec5574596d47a2fe5317f563a \
+    --hash=sha256:c873ba9fd4fa875ad62ce0e4891725e257a8fe7f5abdbc17e51a5d54819be55c \
+    --hash=sha256:db03e3f2c716205fbdabb34d0848459840585225eb97b4f08998c743821ca323 \
+    --hash=sha256:f47f757d1b76f0ecb8aa0508ec8d1b390df67a8b67ee2515dc1b046f3a1596ea \
+    --hash=sha256:f891b17e3a3e974e93f9e34e7cca9f530806543571ce078998676a555837d91d
+    # via tensorflow-cpu
+idna==3.4 \
+    --hash=sha256:814f528e8dead7d329833b91c5faa87d60bf71824cd12a7530b5526063d02cb4 \
+    --hash=sha256:90b77e79eaa3eba6de819a0c442c0b4ceefc341a7a2ab77d7562bf49f425c5c2
     # via requests
-keras==3.3.3 \
-    --hash=sha256:260df9ef71c6b89eb6816ce1c60f139c38ccdddd16f24e7005d2be127cdef8e4 \
-    --hash=sha256:f2fdffc8434fd77045cf8fb21816dbaa2308d5f76974ca924b2f60b40433b1a0
+keras==2.11.0 \
+    --hash=sha256:38c6fff0ea9a8b06a2717736565c92a73c8cd9b1c239e7125ccb188b7848f65e
     # via tensorflow-cpu
-libclang==18.1.1 \
-    --hash=sha256:3f0e1f49f04d3cd198985fea0511576b0aee16f9ff0e0f0cad7f9c57ec3c20e8 \
-    --hash=sha256:4dd2d3b82fab35e2bf9ca717d7b63ac990a3519c7e312f19fa8e86dcc712f7fb \
-    --hash=sha256:54dda940a4a0491a9d1532bf071ea3ef26e6dbaf03b5000ed94dd7174e8f9592 \
-    --hash=sha256:69f8eb8f65c279e765ffd28aaa7e9e364c776c17618af8bff22a8df58677ff4f \
-    --hash=sha256:6f14c3f194704e5d09769108f03185fce7acaf1d1ae4bbb2f30a72c2400cb7c5 \
-    --hash=sha256:83ce5045d101b669ac38e6da8e58765f12da2d3aafb3b9b98d88b286a60964d8 \
-    --hash=sha256:a1214966d08d73d971287fc3ead8dfaf82eb07fb197680d8b3859dbbbbf78250 \
-    --hash=sha256:c533091d8a3bbf7460a00cb6c1a71da93bffe148f172c7d03b1c31fbf8aa2a0b \
-    --hash=sha256:cf4a99b05376513717ab5d82a0db832c56ccea4fd61a69dbb7bccf2dfb207dbe
+libclang==15.0.6.1 \
+    --hash=sha256:4a5188184b937132c198ee9de9a8a2316d5fdd1a825398d5ad1a8f5e06f9b40e \
+    --hash=sha256:687d8549c110c700fece58dd87727421d0710fdd111aa7eecb01faf8e3f50d4e \
+    --hash=sha256:69b01a23ab543908a661532595daa23cf88bd96d80e41f58ba0eaa6a378fe0d8 \
+    --hash=sha256:85afb47630d2070e74b886040ceea1846097ca53cc88d0f1d7751d0f49220028 \
+    --hash=sha256:8621795e07b87e17fc7aac9f071bc7fe6b52ed6110c0a96a9975d8113c8c2527 \
+    --hash=sha256:a1a8fe038af2962c787c5bac81bfa4b82bb8e279e61e70cc934c10f6e20c73ec \
+    --hash=sha256:aaebb6aa1db73bac3a0ac41e57ef78743079eb68728adbf7e80ee917ae171529 \
+    --hash=sha256:f7ffa02ac5e586cfffde039dcccc439d88d0feac7d77bf9426d9ba7543d16545
     # via tensorflow-cpu
-markdown==3.6 \
-    --hash=sha256:48f276f4d8cfb8ce6527c8f79e2ee29708508bf4d40aa410fbc3b4ee832c850f \
-    --hash=sha256:ed4f41f6daecbeeb96e576ce414c41d2d876daa9a16cb35fa8ed8c2ddfad0224
+markdown==3.4.1 \
+    --hash=sha256:08fb8465cffd03d10b9dd34a5c3fea908e20391a2a90b88d66362cb05beed186 \
+    --hash=sha256:3b809086bb6efad416156e00a0da66fe47618a5d6918dd688f53f40c8e4cfeff
     # via tensorboard
-markdown-it-py==3.0.0 \
-    --hash=sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1 \
-    --hash=sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb
-    # via rich
-markupsafe==2.1.5 \
-    --hash=sha256:00e046b6dd71aa03a41079792f8473dc494d564611a8f89bbbd7cb93295ebdcf \
-    --hash=sha256:075202fa5b72c86ad32dc7d0b56024ebdbcf2048c0ba09f1cde31bfdd57bcfff \
-    --hash=sha256:0e397ac966fdf721b2c528cf028494e86172b4feba51d65f81ffd65c63798f3f \
-    --hash=sha256:17b950fccb810b3293638215058e432159d2b71005c74371d784862b7e4683f3 \
-    --hash=sha256:1f3fbcb7ef1f16e48246f704ab79d79da8a46891e2da03f8783a5b6fa41a9532 \
-    --hash=sha256:2174c595a0d73a3080ca3257b40096db99799265e1c27cc5a610743acd86d62f \
-    --hash=sha256:2b7c57a4dfc4f16f7142221afe5ba4e093e09e728ca65c51f5620c9aaeb9a617 \
-    --hash=sha256:2d2d793e36e230fd32babe143b04cec8a8b3eb8a3122d2aceb4a371e6b09b8df \
-    --hash=sha256:30b600cf0a7ac9234b2638fbc0fb6158ba5bdcdf46aeb631ead21248b9affbc4 \
-    --hash=sha256:397081c1a0bfb5124355710fe79478cdbeb39626492b15d399526ae53422b906 \
-    --hash=sha256:3a57fdd7ce31c7ff06cdfbf31dafa96cc533c21e443d57f5b1ecc6cdc668ec7f \
-    --hash=sha256:3c6b973f22eb18a789b1460b4b91bf04ae3f0c4234a0a6aa6b0a92f6f7b951d4 \
-    --hash=sha256:3e53af139f8579a6d5f7b76549125f0d94d7e630761a2111bc431fd820e163b8 \
-    --hash=sha256:4096e9de5c6fdf43fb4f04c26fb114f61ef0bf2e5604b6ee3019d51b69e8c371 \
-    --hash=sha256:4275d846e41ecefa46e2015117a9f491e57a71ddd59bbead77e904dc02b1bed2 \
-    --hash=sha256:4c31f53cdae6ecfa91a77820e8b151dba54ab528ba65dfd235c80b086d68a465 \
-    --hash=sha256:4f11aa001c540f62c6166c7726f71f7573b52c68c31f014c25cc7901deea0b52 \
-    --hash=sha256:5049256f536511ee3f7e1b3f87d1d1209d327e818e6ae1365e8653d7e3abb6a6 \
-    --hash=sha256:58c98fee265677f63a4385256a6d7683ab1832f3ddd1e66fe948d5880c21a169 \
-    --hash=sha256:598e3276b64aff0e7b3451b72e94fa3c238d452e7ddcd893c3ab324717456bad \
-    --hash=sha256:5b7b716f97b52c5a14bffdf688f971b2d5ef4029127f1ad7a513973cfd818df2 \
-    --hash=sha256:5dedb4db619ba5a2787a94d877bc8ffc0566f92a01c0ef214865e54ecc9ee5e0 \
-    --hash=sha256:619bc166c4f2de5caa5a633b8b7326fbe98e0ccbfacabd87268a2b15ff73a029 \
-    --hash=sha256:629ddd2ca402ae6dbedfceeba9c46d5f7b2a61d9749597d4307f943ef198fc1f \
-    --hash=sha256:656f7526c69fac7f600bd1f400991cc282b417d17539a1b228617081106feb4a \
-    --hash=sha256:6ec585f69cec0aa07d945b20805be741395e28ac1627333b1c5b0105962ffced \
-    --hash=sha256:72b6be590cc35924b02c78ef34b467da4ba07e4e0f0454a2c5907f473fc50ce5 \
-    --hash=sha256:7502934a33b54030eaf1194c21c692a534196063db72176b0c4028e140f8f32c \
-    --hash=sha256:7a68b554d356a91cce1236aa7682dc01df0edba8d043fd1ce607c49dd3c1edcf \
-    --hash=sha256:7b2e5a267c855eea6b4283940daa6e88a285f5f2a67f2220203786dfa59b37e9 \
-    --hash=sha256:823b65d8706e32ad2df51ed89496147a42a2a6e01c13cfb6ffb8b1e92bc910bb \
-    --hash=sha256:8590b4ae07a35970728874632fed7bd57b26b0102df2d2b233b6d9d82f6c62ad \
-    --hash=sha256:8dd717634f5a044f860435c1d8c16a270ddf0ef8588d4887037c5028b859b0c3 \
-    --hash=sha256:8dec4936e9c3100156f8a2dc89c4b88d5c435175ff03413b443469c7c8c5f4d1 \
-    --hash=sha256:97cafb1f3cbcd3fd2b6fbfb99ae11cdb14deea0736fc2b0952ee177f2b813a46 \
-    --hash=sha256:a17a92de5231666cfbe003f0e4b9b3a7ae3afb1ec2845aadc2bacc93ff85febc \
-    --hash=sha256:a549b9c31bec33820e885335b451286e2969a2d9e24879f83fe904a5ce59d70a \
-    --hash=sha256:ac07bad82163452a6884fe8fa0963fb98c2346ba78d779ec06bd7a6262132aee \
-    --hash=sha256:ae2ad8ae6ebee9d2d94b17fb62763125f3f374c25618198f40cbb8b525411900 \
-    --hash=sha256:b91c037585eba9095565a3556f611e3cbfaa42ca1e865f7b8015fe5c7336d5a5 \
-    --hash=sha256:bc1667f8b83f48511b94671e0e441401371dfd0f0a795c7daa4a3cd1dde55bea \
-    --hash=sha256:bec0a414d016ac1a18862a519e54b2fd0fc8bbfd6890376898a6c0891dd82e9f \
-    --hash=sha256:bf50cd79a75d181c9181df03572cdce0fbb75cc353bc350712073108cba98de5 \
-    --hash=sha256:bff1b4290a66b490a2f4719358c0cdcd9bafb6b8f061e45c7a2460866bf50c2e \
-    --hash=sha256:c061bb86a71b42465156a3ee7bd58c8c2ceacdbeb95d05a99893e08b8467359a \
-    --hash=sha256:c8b29db45f8fe46ad280a7294f5c3ec36dbac9491f2d1c17345be8e69cc5928f \
-    --hash=sha256:ce409136744f6521e39fd8e2a24c53fa18ad67aa5bc7c2cf83645cce5b5c4e50 \
-    --hash=sha256:d050b3361367a06d752db6ead6e7edeb0009be66bc3bae0ee9d97fb326badc2a \
-    --hash=sha256:d283d37a890ba4c1ae73ffadf8046435c76e7bc2247bbb63c00bd1a709c6544b \
-    --hash=sha256:d9fad5155d72433c921b782e58892377c44bd6252b5af2f67f16b194987338a4 \
-    --hash=sha256:daa4ee5a243f0f20d528d939d06670a298dd39b1ad5f8a72a4275124a7819eff \
-    --hash=sha256:db0b55e0f3cc0be60c1f19efdde9a637c32740486004f20d1cff53c3c0ece4d2 \
-    --hash=sha256:e61659ba32cf2cf1481e575d0462554625196a1f2fc06a1c777d3f48e8865d46 \
-    --hash=sha256:ea3d8a3d18833cf4304cd2fc9cbb1efe188ca9b5efef2bdac7adc20594a0e46b \
-    --hash=sha256:ec6a563cff360b50eed26f13adc43e61bc0c04d94b8be985e6fb24b81f6dcfdf \
-    --hash=sha256:f5dfb42c4604dddc8e4305050aa6deb084540643ed5804d7455b5df8fe16f5e5 \
-    --hash=sha256:fa173ec60341d6bb97a89f5ea19c85c5643c1e7dedebc22f5181eb73573142c5 \
-    --hash=sha256:fa9db3f79de01457b03d4f01b34cf91bc0048eb2c3846ff26f66687c2f6d16ab \
-    --hash=sha256:fce659a462a1be54d2ffcacea5e3ba2d74daa74f30f5f143fe0c58636e355fdd \
-    --hash=sha256:ffee1f21e5ef0d712f9033568f8344d5da8cc2869dbd08d87c84656e6a2d2f68
+markupsafe==2.1.2 \
+    --hash=sha256:0576fe974b40a400449768941d5d0858cc624e3249dfd1e0c33674e5c7ca7aed \
+    --hash=sha256:085fd3201e7b12809f9e6e9bc1e5c96a368c8523fad5afb02afe3c051ae4afcc \
+    --hash=sha256:090376d812fb6ac5f171e5938e82e7f2d7adc2b629101cec0db8b267815c85e2 \
+    --hash=sha256:0b462104ba25f1ac006fdab8b6a01ebbfbce9ed37fd37fd4acd70c67c973e460 \
+    --hash=sha256:137678c63c977754abe9086a3ec011e8fd985ab90631145dfb9294ad09c102a7 \
+    --hash=sha256:1bea30e9bf331f3fef67e0a3877b2288593c98a21ccb2cf29b74c581a4eb3af0 \
+    --hash=sha256:22152d00bf4a9c7c83960521fc558f55a1adbc0631fbb00a9471e097b19d72e1 \
+    --hash=sha256:22731d79ed2eb25059ae3df1dfc9cb1546691cc41f4e3130fe6bfbc3ecbbecfa \
+    --hash=sha256:2298c859cfc5463f1b64bd55cb3e602528db6fa0f3cfd568d3605c50678f8f03 \
+    --hash=sha256:28057e985dace2f478e042eaa15606c7efccb700797660629da387eb289b9323 \
+    --hash=sha256:2e7821bffe00aa6bd07a23913b7f4e01328c3d5cc0b40b36c0bd81d362faeb65 \
+    --hash=sha256:2ec4f2d48ae59bbb9d1f9d7efb9236ab81429a764dedca114f5fdabbc3788013 \
+    --hash=sha256:340bea174e9761308703ae988e982005aedf427de816d1afe98147668cc03036 \
+    --hash=sha256:40627dcf047dadb22cd25ea7ecfe9cbf3bbbad0482ee5920b582f3809c97654f \
+    --hash=sha256:40dfd3fefbef579ee058f139733ac336312663c6706d1163b82b3003fb1925c4 \
+    --hash=sha256:4cf06cdc1dda95223e9d2d3c58d3b178aa5dacb35ee7e3bbac10e4e1faacb419 \
+    --hash=sha256:50c42830a633fa0cf9e7d27664637532791bfc31c731a87b202d2d8ac40c3ea2 \
+    --hash=sha256:55f44b440d491028addb3b88f72207d71eeebfb7b5dbf0643f7c023ae1fba619 \
+    --hash=sha256:608e7073dfa9e38a85d38474c082d4281f4ce276ac0010224eaba11e929dd53a \
+    --hash=sha256:63ba06c9941e46fa389d389644e2d8225e0e3e5ebcc4ff1ea8506dce646f8c8a \
+    --hash=sha256:65608c35bfb8a76763f37036547f7adfd09270fbdbf96608be2bead319728fcd \
+    --hash=sha256:665a36ae6f8f20a4676b53224e33d456a6f5a72657d9c83c2aa00765072f31f7 \
+    --hash=sha256:6d6607f98fcf17e534162f0709aaad3ab7a96032723d8ac8750ffe17ae5a0666 \
+    --hash=sha256:7313ce6a199651c4ed9d7e4cfb4aa56fe923b1adf9af3b420ee14e6d9a73df65 \
+    --hash=sha256:7668b52e102d0ed87cb082380a7e2e1e78737ddecdde129acadb0eccc5423859 \
+    --hash=sha256:7df70907e00c970c60b9ef2938d894a9381f38e6b9db73c5be35e59d92e06625 \
+    --hash=sha256:7e007132af78ea9df29495dbf7b5824cb71648d7133cf7848a2a5dd00d36f9ff \
+    --hash=sha256:835fb5e38fd89328e9c81067fd642b3593c33e1e17e2fdbf77f5676abb14a156 \
+    --hash=sha256:8bca7e26c1dd751236cfb0c6c72d4ad61d986e9a41bbf76cb445f69488b2a2bd \
+    --hash=sha256:8db032bf0ce9022a8e41a22598eefc802314e81b879ae093f36ce9ddf39ab1ba \
+    --hash=sha256:99625a92da8229df6d44335e6fcc558a5037dd0a760e11d84be2260e6f37002f \
+    --hash=sha256:9cad97ab29dfc3f0249b483412c85c8ef4766d96cdf9dcf5a1e3caa3f3661cf1 \
+    --hash=sha256:a4abaec6ca3ad8660690236d11bfe28dfd707778e2442b45addd2f086d6ef094 \
+    --hash=sha256:a6e40afa7f45939ca356f348c8e23048e02cb109ced1eb8420961b2f40fb373a \
+    --hash=sha256:a6f2fcca746e8d5910e18782f976489939d54a91f9411c32051b4aab2bd7c513 \
+    --hash=sha256:a806db027852538d2ad7555b203300173dd1b77ba116de92da9afbc3a3be3eed \
+    --hash=sha256:abcabc8c2b26036d62d4c746381a6f7cf60aafcc653198ad678306986b09450d \
+    --hash=sha256:b8526c6d437855442cdd3d87eede9c425c4445ea011ca38d937db299382e6fa3 \
+    --hash=sha256:bb06feb762bade6bf3c8b844462274db0c76acc95c52abe8dbed28ae3d44a147 \
+    --hash=sha256:c0a33bc9f02c2b17c3ea382f91b4db0e6cde90b63b296422a939886a7a80de1c \
+    --hash=sha256:c4a549890a45f57f1ebf99c067a4ad0cb423a05544accaf2b065246827ed9603 \
+    --hash=sha256:ca244fa73f50a800cf8c3ebf7fd93149ec37f5cb9596aa8873ae2c1d23498601 \
+    --hash=sha256:cf877ab4ed6e302ec1d04952ca358b381a882fbd9d1b07cccbfd61783561f98a \
+    --hash=sha256:d9d971ec1e79906046aa3ca266de79eac42f1dbf3612a05dc9368125952bd1a1 \
+    --hash=sha256:da25303d91526aac3672ee6d49a2f3db2d9502a4a60b55519feb1a4c7714e07d \
+    --hash=sha256:e55e40ff0cc8cc5c07996915ad367fa47da6b3fc091fdadca7f5403239c5fec3 \
+    --hash=sha256:f03a532d7dee1bed20bc4884194a16160a2de9ffc6354b3878ec9682bb623c54 \
+    --hash=sha256:f1cd098434e83e656abf198f103a8207a8187c0fc110306691a2e94a78d0abb2 \
+    --hash=sha256:f2bfb563d0211ce16b63c7cb9395d2c682a23187f54c3d79bfec33e6705473c6 \
+    --hash=sha256:f8ffb705ffcf5ddd0e80b65ddf7bed7ee4f5a441ea7d3419e861a12eaf41af58
     # via werkzeug
-mdurl==0.1.2 \
-    --hash=sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8 \
-    --hash=sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba
-    # via markdown-it-py
-ml-dtypes==0.3.2 \
-    --hash=sha256:2c34f2ba9660b21fe1034b608308a01be82bbef2a92fb8199f24dc6bad0d5226 \
-    --hash=sha256:3a17ef2322e60858d93584e9c52a5be7dd6236b056b7fa1ec57f1bb6ba043e33 \
-    --hash=sha256:533059bc5f1764fac071ef54598db358c167c51a718f68f5bb55e3dee79d2967 \
-    --hash=sha256:6604877d567a29bfe7cc02969ae0f2425260e5335505cf5e7fefc3e5465f5655 \
-    --hash=sha256:6b35c4e8ca957c877ac35c79ffa77724ecc3702a1e4b18b08306c03feae597bb \
-    --hash=sha256:763697ab8a88d47443997a7cdf3aac7340049aed45f7521f6b0ec8a0594821fe \
-    --hash=sha256:7a4c3fcbf86fa52d0204f07cfd23947ef05b4ad743a1a988e163caa34a201e5e \
-    --hash=sha256:7afde548890a92b41c0fed3a6c525f1200a5727205f73dc21181a2726571bb53 \
-    --hash=sha256:7ba8e1fafc7fff3e643f453bffa7d082df1678a73286ce8187d3e825e776eb94 \
-    --hash=sha256:91f8783fd1f2c23fd3b9ee5ad66b785dafa58ba3cdb050c4458021fa4d1eb226 \
-    --hash=sha256:93b78f53431c93953f7850bb1b925a17f0ab5d97527e38a7e865b5b4bc5cfc18 \
-    --hash=sha256:961134ea44c7b8ca63eda902a44b58cd8bd670e21d62e255c81fba0a8e70d9b7 \
-    --hash=sha256:b89b194e9501a92d289c1ffd411380baf5daafb9818109a4f49b0a1b6dce4462 \
-    --hash=sha256:c7b3fb3d4f6b39bcd4f6c4b98f406291f0d681a895490ee29a0f95bab850d53c \
-    --hash=sha256:d1a746fe5fb9cd974a91070174258f0be129c592b93f9ce7df6cc336416c3fbd \
-    --hash=sha256:e8505946df1665db01332d885c2020b4cb9e84a8b1241eb4ba69d59591f65855 \
-    --hash=sha256:f47619d978ab1ae7dfdc4052ea97c636c6263e1f19bd1be0e42c346b98d15ff4
-    # via
-    #   keras
-    #   tensorflow-cpu
-namex==0.0.8 \
-    --hash=sha256:32a50f6c565c0bb10aa76298c959507abdc0e850efe085dc38f3440fcb3aa90b \
-    --hash=sha256:7ddb6c2bb0e753a311b7590f84f6da659dd0c05e65cb89d519d54c0a250c0487
-    # via keras
-numpy==1.26.4 \
-    --hash=sha256:03a8c78d01d9781b28a6989f6fa1bb2c4f2d51201cf99d3dd875df6fbd96b23b \
-    --hash=sha256:08beddf13648eb95f8d867350f6a018a4be2e5ad54c8d8caed89ebca558b2818 \
-    --hash=sha256:1af303d6b2210eb850fcf03064d364652b7120803a0b872f5211f5234b399f20 \
-    --hash=sha256:1dda2e7b4ec9dd512f84935c5f126c8bd8b9f2fc001e9f54af255e8c5f16b0e0 \
-    --hash=sha256:2a02aba9ed12e4ac4eb3ea9421c420301a0c6460d9830d74a9df87efa4912010 \
-    --hash=sha256:2e4ee3380d6de9c9ec04745830fd9e2eccb3e6cf790d39d7b98ffd19b0dd754a \
-    --hash=sha256:3373d5d70a5fe74a2c1bb6d2cfd9609ecf686d47a2d7b1d37a8f3b6bf6003aea \
-    --hash=sha256:47711010ad8555514b434df65f7d7b076bb8261df1ca9bb78f53d3b2db02e95c \
-    --hash=sha256:4c66707fabe114439db9068ee468c26bbdf909cac0fb58686a42a24de1760c71 \
-    --hash=sha256:50193e430acfc1346175fcbdaa28ffec49947a06918b7b92130744e81e640110 \
-    --hash=sha256:52b8b60467cd7dd1e9ed082188b4e6bb35aa5cdd01777621a1658910745b90be \
-    --hash=sha256:60dedbb91afcbfdc9bc0b1f3f402804070deed7392c23eb7a7f07fa857868e8a \
-    --hash=sha256:62b8e4b1e28009ef2846b4c7852046736bab361f7aeadeb6a5b89ebec3c7055a \
-    --hash=sha256:666dbfb6ec68962c033a450943ded891bed2d54e6755e35e5835d63f4f6931d5 \
-    --hash=sha256:675d61ffbfa78604709862923189bad94014bef562cc35cf61d3a07bba02a7ed \
-    --hash=sha256:679b0076f67ecc0138fd2ede3a8fd196dddc2ad3254069bcb9faf9a79b1cebcd \
-    --hash=sha256:7349ab0fa0c429c82442a27a9673fc802ffdb7c7775fad780226cb234965e53c \
-    --hash=sha256:7ab55401287bfec946ced39700c053796e7cc0e3acbef09993a9ad2adba6ca6e \
-    --hash=sha256:7e50d0a0cc3189f9cb0aeb3a6a6af18c16f59f004b866cd2be1c14b36134a4a0 \
-    --hash=sha256:95a7476c59002f2f6c590b9b7b998306fba6a5aa646b1e22ddfeaf8f78c3a29c \
-    --hash=sha256:96ff0b2ad353d8f990b63294c8986f1ec3cb19d749234014f4e7eb0112ceba5a \
-    --hash=sha256:9fad7dcb1aac3c7f0584a5a8133e3a43eeb2fe127f47e3632d43d677c66c102b \
-    --hash=sha256:9ff0f4f29c51e2803569d7a51c2304de5554655a60c5d776e35b4a41413830d0 \
-    --hash=sha256:a354325ee03388678242a4d7ebcd08b5c727033fcff3b2f536aea978e15ee9e6 \
-    --hash=sha256:a4abb4f9001ad2858e7ac189089c42178fcce737e4169dc61321660f1a96c7d2 \
-    --hash=sha256:ab47dbe5cc8210f55aa58e4805fe224dac469cde56b9f731a4c098b91917159a \
-    --hash=sha256:afedb719a9dcfc7eaf2287b839d8198e06dcd4cb5d276a3df279231138e83d30 \
-    --hash=sha256:b3ce300f3644fb06443ee2222c2201dd3a89ea6040541412b8fa189341847218 \
-    --hash=sha256:b97fe8060236edf3662adfc2c633f56a08ae30560c56310562cb4f95500022d5 \
-    --hash=sha256:bfe25acf8b437eb2a8b2d49d443800a5f18508cd811fea3181723922a8a82b07 \
-    --hash=sha256:cd25bcecc4974d09257ffcd1f098ee778f7834c3ad767fe5db785be9a4aa9cb2 \
-    --hash=sha256:d209d8969599b27ad20994c8e41936ee0964e6da07478d6c35016bc386b66ad4 \
-    --hash=sha256:d5241e0a80d808d70546c697135da2c613f30e28251ff8307eb72ba696945764 \
-    --hash=sha256:edd8b5fe47dab091176d21bb6de568acdd906d1887a4584a15a9a96a1dca06ef \
-    --hash=sha256:f870204a840a60da0b12273ef34f7051e98c3b5961b61b0c2c1be6dfd64fbcd3 \
-    --hash=sha256:ffa75af20b44f8dba823498024771d5ac50620e6915abac414251bd971b4529f
+numpy==1.24.2 \
+    --hash=sha256:003a9f530e880cb2cd177cba1af7220b9aa42def9c4afc2a2fc3ee6be7eb2b22 \
+    --hash=sha256:150947adbdfeceec4e5926d956a06865c1c690f2fd902efede4ca6fe2e657c3f \
+    --hash=sha256:2620e8592136e073bd12ee4536149380695fbe9ebeae845b81237f986479ffc9 \
+    --hash=sha256:2eabd64ddb96a1239791da78fa5f4e1693ae2dadc82a76bc76a14cbb2b966e96 \
+    --hash=sha256:4173bde9fa2a005c2c6e2ea8ac1618e2ed2c1c6ec8a7657237854d42094123a0 \
+    --hash=sha256:4199e7cfc307a778f72d293372736223e39ec9ac096ff0a2e64853b866a8e18a \
+    --hash=sha256:4cecaed30dc14123020f77b03601559fff3e6cd0c048f8b5289f4eeabb0eb281 \
+    --hash=sha256:557d42778a6869c2162deb40ad82612645e21d79e11c1dc62c6e82a2220ffb04 \
+    --hash=sha256:63e45511ee4d9d976637d11e6c9864eae50e12dc9598f531c035265991910468 \
+    --hash=sha256:6524630f71631be2dabe0c541e7675db82651eb998496bbe16bc4f77f0772253 \
+    --hash=sha256:76807b4063f0002c8532cfeac47a3068a69561e9c8715efdad3c642eb27c0756 \
+    --hash=sha256:7de8fdde0003f4294655aa5d5f0a89c26b9f22c0a58790c38fae1ed392d44a5a \
+    --hash=sha256:889b2cc88b837d86eda1b17008ebeb679d82875022200c6e8e4ce6cf549b7acb \
+    --hash=sha256:92011118955724465fb6853def593cf397b4a1367495e0b59a7e69d40c4eb71d \
+    --hash=sha256:97cf27e51fa078078c649a51d7ade3c92d9e709ba2bfb97493007103c741f1d0 \
+    --hash=sha256:9a23f8440561a633204a67fb44617ce2a299beecf3295f0d13c495518908e910 \
+    --hash=sha256:a51725a815a6188c662fb66fb32077709a9ca38053f0274640293a14fdd22978 \
+    --hash=sha256:a77d3e1163a7770164404607b7ba3967fb49b24782a6ef85d9b5f54126cc39e5 \
+    --hash=sha256:adbdce121896fd3a17a77ab0b0b5eedf05a9834a18699db6829a64e1dfccca7f \
+    --hash=sha256:c29e6bd0ec49a44d7690ecb623a8eac5ab8a923bce0bea6293953992edf3a76a \
+    --hash=sha256:c72a6b2f4af1adfe193f7beb91ddf708ff867a3f977ef2ec53c0ffb8283ab9f5 \
+    --hash=sha256:d0a2db9d20117bf523dde15858398e7c0858aadca7c0f088ac0d6edd360e9ad2 \
+    --hash=sha256:e3ab5d32784e843fc0dd3ab6dcafc67ef806e6b6828dc6af2f689be0eb4d781d \
+    --hash=sha256:e428c4fbfa085f947b536706a2fc349245d7baa8334f0c5723c56a10595f9b95 \
+    --hash=sha256:e8d2859428712785e8a8b7d2b3ef0a1d1565892367b32f915c4a4df44d0e64f5 \
+    --hash=sha256:eef70b4fc1e872ebddc38cddacc87c19a3709c0e3e5d20bf3954c147b1dd941d \
+    --hash=sha256:f64bb98ac59b3ea3bf74b02f13836eb2e24e48e0ab0145bbda646295769bd780 \
+    --hash=sha256:f9006288bcf4895917d02583cf3411f98631275bc67cce355a7f39f8c14338fa
     # via
     #   h5py
-    #   keras
-    #   ml-dtypes
     #   opt-einsum
     #   tensorboard
     #   tensorflow-cpu
+oauthlib==3.2.2 \
+    --hash=sha256:8139f29aac13e25d502680e9e19963e83f16838d48a0d71c287fe40e7067fbca \
+    --hash=sha256:9859c40929662bec5d64f34d01c99e093149682a3f38915dc0655d5a633dd918
+    # via requests-oauthlib
 opt-einsum==3.3.0 \
     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \
     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549
     # via tensorflow-cpu
-optree==0.11.0 \
-    --hash=sha256:00a63f10d4a476e8e9aa2988daba9b2e88cb369c5aacc12545957d7d00bcd1a7 \
-    --hash=sha256:0db6968394096223881053dffdcaf2b8e220fd85db904f14aa931e4dc422c046 \
-    --hash=sha256:0df9a3923725aabb112ec7f10c74fa96b6c640da1cd30e7bc62fd4b03ef02875 \
-    --hash=sha256:162ed3ff2eb3f1c358e131e72c025f2b93d69b906e9057a811d014032ec71dc8 \
-    --hash=sha256:228b97e8c991739b10c8548c118747ba32ee765f88236342e492bf9648afc0bc \
-    --hash=sha256:234a4f8f97a1217f13390df7ac416771689749d9a1c8eda31bf8622cd333219e \
-    --hash=sha256:26b1230f9b75b579923a4f837c7c13db8b8d815cf68ce5af31dda5d818a877b2 \
-    --hash=sha256:2b3bb59324d635f2015bb3e237fd772b1fd548eee6cc80e008fbe0f092e9228d \
-    --hash=sha256:2bc08fb9691f43afc3a01119dead6b823ce3d7239e42fc3e47d4028eed50a6a2 \
-    --hash=sha256:31d444684ebd8c9f09a3d806fb3277843138ef9952b7a2954908e440e3b22519 \
-    --hash=sha256:39bed744a61e2f795e172d2853779ac59b8dea236982dc160ea22063afc99ca3 \
-    --hash=sha256:3cdc9fac9888d9eff11128ccfc4d4c10309163e372f312f7942ecee8df3d7824 \
-    --hash=sha256:4144126dd3c2ece2d2dd1d5e0b39fb91adf1c46f660c2c5a2df7f80666989d5d \
-    --hash=sha256:418850ceff364f51a6d81f32a1efd06a4e2d8df79a162e892685bc20c0aedd72 \
-    --hash=sha256:5e250144eacdd5813dec0b18d91df0229197e3be402db42fd8e254ec90ea343d \
-    --hash=sha256:5e5df0e8aaca124cc1ffca311786cc909810f3c046de090729cdafbf910082f8 \
-    --hash=sha256:63e020a34b7168b5d0701a265c7c95b07984ff699d4894b20fa601282be88f20 \
-    --hash=sha256:64c2e00fe508f50a42c50838df0d1f5be0dce5b4bef2373db8ad72b860211015 \
-    --hash=sha256:6a406eee5acd3fd4875fa44c3972d29ae6d4329e7296e9219986fe6ff8e92ea0 \
-    --hash=sha256:6cdd625dab2dff5374ff9c6792e8702fced8f0ea713ce959fc8f95499b5ecb2f \
-    --hash=sha256:6e8c3757088cd7fce666f2a5e031b65d7898e210452380d2657c0fc0a7ec9932 \
-    --hash=sha256:738e8bf4158e9c11cd051d89c2e453aeacf80ff8719ebc3251069015646554d0 \
-    --hash=sha256:8e6a46e95c3ea8546055087d6fe52a1dcd56de5182365f1469106cc72cdf3307 \
-    --hash=sha256:979ffc2b96f16595c219fb7a89597dd2fa00ac47a3b411fdcf8ae6821da52290 \
-    --hash=sha256:9bf322ad14f907ad4660ca286e731e750546d54934a94cc5ba7efe8860c60ab4 \
-    --hash=sha256:9d9d644e5448db9f32e2497487aca3bb2d3f92cbb50429a411ccda3f1f0968f3 \
-    --hash=sha256:a5f37bcfe4e363e3bb8d36c5698fb829546956b2fe88951994387162a1859625 \
-    --hash=sha256:a64df43fce2d8eeafd7db6e27447c56b3fa64842df847819684b3b1cc254c016 \
-    --hash=sha256:a91840f9d45e7c01f151ba1815ae32b4c3c21e4290298772ee4b13314f729856 \
-    --hash=sha256:b201a9405e250cf5770955863af2a236e382bdf5e4e086897ff03c41418c39da \
-    --hash=sha256:b26ac807d8993b7e43081b4b7bbb0378b4e5f3e6525daf923c470bc176cc3327 \
-    --hash=sha256:b8126d81ecb2c9e3554420834014ba343251f564c905ee3bef09d205b924b0c0 \
-    --hash=sha256:b9d236bc1491a5e366921b95fecc05aa6ff55989a81f2242cd11121b82c24503 \
-    --hash=sha256:bc17f9d085cd75a2de4f299a9c5e3c3520138eac7596061e581230b03862b44d \
-    --hash=sha256:d666099a78f7bf31bf3a520d6871ddcae65484bcff095fc4271a391553b09c75 \
-    --hash=sha256:e2d47bd28eff690eb2f7432e490265a291b04d6d346cf7b586491b2e2337bf97 \
-    --hash=sha256:ee208f0bec6436085a9fa3ae98af54bfcb8822086894fc1ade283e80a6f11fd7 \
-    --hash=sha256:f53951bfb640417558568284a8949d67bcdbf21fa0113107e20bd9403aa20b2b \
-    --hash=sha256:fa9ed745d4cbac5e15df70339b30867ba033542b87f7b734f4cacae5ec73ba00
-    # via keras
-packaging==24.0 \
-    --hash=sha256:2ddfb553fdf02fb784c234c7ba6ccc288296ceabec964ad2eae3777778130bc5 \
-    --hash=sha256:eb82c5e3e56209074766e6885bb04b8c38a0c015d0a30036ebe7ece34c9989e9
+packaging==23.0 \
+    --hash=sha256:714ac14496c3e68c99c29b00845f7a2b85f3bb6f1078fd9f72fd20f0570002b2 \
+    --hash=sha256:b6ad297f8907de0fa2fe1ccbd26fdaf387f5f47c7275fedf8cce89f99446cf97
     # via tensorflow-cpu
-protobuf==4.25.3 \
-    --hash=sha256:19b270aeaa0099f16d3ca02628546b8baefe2955bbe23224aaf856134eccf1e4 \
-    --hash=sha256:209ba4cc916bab46f64e56b85b090607a676f66b473e6b762e6f1d9d591eb2e8 \
-    --hash=sha256:25b5d0b42fd000320bd7830b349e3b696435f3b329810427a6bcce6a5492cc5c \
-    --hash=sha256:7c8daa26095f82482307bc717364e7c13f4f1c99659be82890dcfc215194554d \
-    --hash=sha256:c053062984e61144385022e53678fbded7aea14ebb3e0305ae3592fb219ccfa4 \
-    --hash=sha256:d4198877797a83cbfe9bffa3803602bbe1625dc30d8a097365dbc762e5790faa \
-    --hash=sha256:e3c97a1555fd6388f857770ff8b9703083de6bf1f9274a002a332d65fbb56c8c \
-    --hash=sha256:e7cb0ae90dd83727f0c0718634ed56837bfeeee29a5f82a7514c03ee1364c019 \
-    --hash=sha256:f0700d54bcf45424477e46a9f0944155b46fb0639d69728739c0e47bab83f2b9 \
-    --hash=sha256:f1279ab38ecbfae7e456a108c5c0681e4956d5b1090027c1de0f934dfdb4b35c \
-    --hash=sha256:f4f118245c4a087776e0a8408be33cf09f6c547442c00395fbfb116fac2f8ac2
+protobuf==3.19.6 \
+    --hash=sha256:010be24d5a44be7b0613750ab40bc8b8cedc796db468eae6c779b395f50d1fa1 \
+    --hash=sha256:0469bc66160180165e4e29de7f445e57a34ab68f49357392c5b2f54c656ab25e \
+    --hash=sha256:0c0714b025ec057b5a7600cb66ce7c693815f897cfda6d6efb58201c472e3437 \
+    --hash=sha256:11478547958c2dfea921920617eb457bc26867b0d1aa065ab05f35080c5d9eb6 \
+    --hash=sha256:14082457dc02be946f60b15aad35e9f5c69e738f80ebbc0900a19bc83734a5a4 \
+    --hash=sha256:2b2d2913bcda0e0ec9a784d194bc490f5dc3d9d71d322d070b11a0ade32ff6ba \
+    --hash=sha256:30a15015d86b9c3b8d6bf78d5b8c7749f2512c29f168ca259c9d7727604d0e39 \
+    --hash=sha256:30f5370d50295b246eaa0296533403961f7e64b03ea12265d6dfce3a391d8992 \
+    --hash=sha256:347b393d4dd06fb93a77620781e11c058b3b0a5289262f094379ada2920a3730 \
+    --hash=sha256:4bc98de3cdccfb5cd769620d5785b92c662b6bfad03a202b83799b6ed3fa1fa7 \
+    --hash=sha256:5057c64052a1f1dd7d4450e9aac25af6bf36cfbfb3a1cd89d16393a036c49157 \
+    --hash=sha256:559670e006e3173308c9254d63facb2c03865818f22204037ab76f7a0ff70b5f \
+    --hash=sha256:5a0d7539a1b1fb7e76bf5faa0b44b30f812758e989e59c40f77a7dab320e79b9 \
+    --hash=sha256:5f5540d57a43042389e87661c6eaa50f47c19c6176e8cf1c4f287aeefeccb5c4 \
+    --hash=sha256:7a552af4dc34793803f4e735aabe97ffc45962dfd3a237bdde242bff5a3de684 \
+    --hash=sha256:84a04134866861b11556a82dd91ea6daf1f4925746b992f277b84013a7cc1229 \
+    --hash=sha256:878b4cd080a21ddda6ac6d1e163403ec6eea2e206cf225982ae04567d39be7b0 \
+    --hash=sha256:90b0d02163c4e67279ddb6dc25e063db0130fc299aefabb5d481053509fae5c8 \
+    --hash=sha256:91d5f1e139ff92c37e0ff07f391101df77e55ebb97f46bbc1535298d72019462 \
+    --hash=sha256:a8ce5ae0de28b51dff886fb922012dad885e66176663950cb2344c0439ecb473 \
+    --hash=sha256:aa3b82ca1f24ab5326dcf4ea00fcbda703e986b22f3d27541654f749564d778b \
+    --hash=sha256:bb6776bd18f01ffe9920e78e03a8676530a5d6c5911934c6a1ac6eb78973ecb6 \
+    --hash=sha256:bbf5cea5048272e1c60d235c7bd12ce1b14b8a16e76917f371c718bd3005f045 \
+    --hash=sha256:c0ccd3f940fe7f3b35a261b1dd1b4fc850c8fde9f74207015431f174be5976b3 \
+    --hash=sha256:d0b635cefebd7a8a0f92020562dead912f81f401af7e71f16bf9506ff3bdbb38
     # via
     #   tensorboard
     #   tensorflow-cpu
-pygments==2.18.0 \
-    --hash=sha256:786ff802f32e91311bff3889f6e9a86e81505fe99f2735bb6d60ae0c5004f199 \
-    --hash=sha256:b8e6aca0523f3ab76fee51799c488e38782ac06eafcf95e7ba832985c8e7b13a
-    # via rich
-requests==2.32.2 \
-    --hash=sha256:dd951ff5ecf3e3b3aa26b40703ba77495dab41da839ae72ef3c8e5d8e2433289 \
-    --hash=sha256:fc06670dd0ed212426dfeb94fc1b983d917c4f9847c863f313c9dfaaffb7c23c
-    # via tensorflow-cpu
-rich==13.7.1 \
-    --hash=sha256:4edbae314f59eb482f54e9e30bf00d33350aaa94f4bfcd4e9e3110e64d0d7222 \
-    --hash=sha256:9be308cb1fe2f1f57d67ce99e95af38a1e2bc71ad9813b0e247cf7ffbcc3a432
-    # via keras
+pyasn1==0.4.8 \
+    --hash=sha256:39c7e2ec30515947ff4e87fb6f456dfc6e84857d34be479c9d4a4ba4bf46aa5d \
+    --hash=sha256:aef77c9fb94a3ac588e87841208bdec464471d9871bd5050a287cc9a475cd0ba
+    # via
+    #   pyasn1-modules
+    #   rsa
+pyasn1-modules==0.2.8 \
+    --hash=sha256:905f84c712230b2c592c19470d3ca8d552de726050d1d1716282a1f6146be65e \
+    --hash=sha256:a50b808ffeb97cb3601dd25981f6b016cbb3d31fbf57a8b8a87428e6158d0c74
+    # via google-auth
+requests==2.28.2 \
+    --hash=sha256:64299f4909223da747622c030b781c0d7811e359c37124b4bd368fb8c6518baa \
+    --hash=sha256:98b1b2782e3c6c4904938b84c0eb932721069dfdb9134313beff7c83c2df24bf
+    # via
+    #   requests-oauthlib
+    #   tensorboard
+requests-oauthlib==1.3.1 \
+    --hash=sha256:2577c501a2fb8d05a304c09d090d6e47c306fef15809d102b327cf8364bddab5 \
+    --hash=sha256:75beac4a47881eeb94d5ea5d6ad31ef88856affe2332b9aafb52c6452ccf0d7a
+    # via google-auth-oauthlib
+rsa==4.9 \
+    --hash=sha256:90260d9058e514786967344d0ef75fa8727eed8a7d2e43ce9f4bcf1b536174f7 \
+    --hash=sha256:e38464a49c6c85d7f1351b0126661487a7e0a14a50f1675ec50eb34d4f20ef21
+    # via google-auth
 six==1.16.0 \
     --hash=sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926 \
     --hash=sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254
     # via
     #   astunparse
+    #   google-auth
     #   google-pasta
-    #   tensorboard
     #   tensorflow-cpu
-tensorboard==2.16.2 \
-    --hash=sha256:9f2b4e7dad86667615c0e5cd072f1ea8403fc032a299f0072d6f74855775cc45
+tensorboard==2.11.2 \
+    --hash=sha256:cbaa2210c375f3af1509f8571360a19ccc3ded1d9641533414874b5deca47e89
     # via tensorflow-cpu
-tensorboard-data-server==0.7.2 \
-    --hash=sha256:7e0610d205889588983836ec05dc098e80f97b7e7bbff7e994ebb78f578d0ddb \
-    --hash=sha256:9fe5d24221b29625dbc7328b0436ca7fc1c23de4acf4d272f1180856e32f9f60 \
-    --hash=sha256:ef687163c24185ae9754ed5650eb5bc4d84ff257aabdc33f0cc6f74d8ba54530
+tensorboard-data-server==0.6.1 \
+    --hash=sha256:809fe9887682d35c1f7d1f54f0f40f98bb1f771b14265b453ca051e2ce58fca7 \
+    --hash=sha256:d8237580755e58eff68d1f3abefb5b1e39ae5c8b127cc40920f9c4fb33f4b98a \
+    --hash=sha256:fa8cef9be4fcae2f2363c88176638baf2da19c5ec90addb49b1cde05c95c88ee
+    # via tensorboard
+tensorboard-plugin-wit==1.8.1 \
+    --hash=sha256:ff26bdd583d155aa951ee3b152b3d0cffae8005dc697f72b44a8e8c2a77a8cbe
     # via tensorboard
-tensorflow-cpu==2.16.1 \
-    --hash=sha256:050f550a8a1aa77959826fd642024d527699a817cbf3e16c59773981c1fae0a2 \
-    --hash=sha256:10daa2bda40c85f7b0ed8d036e6c0394fe24e1806bec0835b5331f8a451e182d \
-    --hash=sha256:1ca39bd2f4e28c78f86f744e2cd751d317e42b3b2f8454a9bef1e21aa15e7775 \
-    --hash=sha256:282503444a5a61d330fb0a522a56d4c79a241941eb0a074916dfa37a10285e69 \
-    --hash=sha256:32190a26ef5a4cc259926e5ed5e3c8c94cf47b7b04bdb18b3e54ec7769673ebd \
-    --hash=sha256:3c79d15e51aab9d9cbeb1b4dc13f2c83a80e63540162afc1ee8c66b89ceb123a \
-    --hash=sha256:45b31b5258726fbd2c9f2422415beb6fe737d8fe63f1c461e3648dad5c088348 \
-    --hash=sha256:67bb51840057ba3a2f46ca6d5cee738974c80b44c9e94df39a55c558d392fc46 \
-    --hash=sha256:8fde4a1a1515f3099119c2d71e1653aa6e7ae81ec58b7cb045cab5e5bb147b8b \
-    --hash=sha256:98843927dcdc8ab1e4cc20ca0998c69d8623e6c5c779f4de4c82be613de37abd \
-    --hash=sha256:aea6520308b1f15511e69bd40b52ba9478143e6e1e8e49d57cd36410321b7b6f \
-    --hash=sha256:f7136781cfd6818b2fe74ffea4c585b020c9140652507cbce1558169f2058b58
-    # via -r requirements.in
-tensorflow-io-gcs-filesystem==0.37.0 \
-    --hash=sha256:03d5598b8007551f4e1391bf85a83a1865e3fa0789beef15a200efaa06a23fb5 \
-    --hash=sha256:0e2901bc4a91158fa0a10d37594c8a5efb1445dd5a041b1b5b90f782a5d1b15e \
-    --hash=sha256:13bc337f2c2db63a39c81c8fd0ececc0c3d5fcf4ce229dfed0b0085a23dd60e9 \
-    --hash=sha256:48a8e7aec651bea8db410f6426c6446a56d16a5ab32201a70d8d684c113137b7 \
-    --hash=sha256:4ec3c0d0a9d3676a2e74198e3dff66d74c7c34f974257f2176236d0703b31a0e \
-    --hash=sha256:500ec871a8d59cf78992b7fd4750d86ea3d35e231fb0bea7a7eabcf73abfceeb \
-    --hash=sha256:57e7af9c81e79bf8fb552985dc8972ac90437d34bd4c1c9019a92a07eb12bc98 \
-    --hash=sha256:677d6d7c84a94a3b27ea5d16633ea09adadef09c2630480e8e94209558828b02 \
-    --hash=sha256:71ccf64a137efcb2be2627225b4e48110cbf34da39b23c5cc688fe803f2510f1 \
-    --hash=sha256:82cc4d8e26fb143fc814ac8ab95fede83363a315f5b62f8ae68312f1aca1cc6e \
-    --hash=sha256:8385f4fe447812bd8e2e11ef523cf02765319100e5d9e4a9b5a876d4440c900c \
-    --hash=sha256:8d3ad5f30b6dbe09baefdb80e9aa7ff3869c772928b865f8ffc8402be7675a43 \
-    --hash=sha256:8e5d1ac4d2010e8cdf259918ba1500c942b51b7ed2e549f55b404c1fb52f695d \
-    --hash=sha256:95bb229e968fca943806c6ac04e81dc4966fc4a36ab83efaa061a4ecb3ea5e85 \
-    --hash=sha256:af0f79400656bb88bf326d2b8e63aef49c07a0ce8c14c3e2589a62e765d8c21f \
-    --hash=sha256:eab6e4c1daf7ddbfef608cd8e2102861021678dfb3f6a7fb3f613db9d6992919
+tensorflow-cpu==2.11.0 \
+    --hash=sha256:08cc63ea4728ac0246063cef4f79911367c194515a45cc247ac05eb6684cd4aa \
+    --hash=sha256:0c9bbd54abc00858bd4722ddaa6ba6469f9730d626786b7bd19a544defb61f11 \
+    --hash=sha256:1954bccbd78681c3df0d4ac9f020a0ee44b17bd6b5962ebb8848479879f45bc7 \
+    --hash=sha256:57aee7f2f3eed2f6e26bc3695c967fa889c98cefb4b8bfb2f47e171d96c13a0a \
+    --hash=sha256:6bb3f3a8b6a96025fdffde2526ca2c58bb36410a74163a498ca9b2d68d3ccfcf \
+    --hash=sha256:8a125157fdb2b1191ca6321e78127f032ce06ae17349e9affd75595782cca4cf \
+    --hash=sha256:91bac68200ddbdff757c9d3aec8a03ad12b5fef21b937ff287721076e43b58b4 \
+    --hash=sha256:b318429219392b2e73f72099db5b92cfd516171c1e10e4ef37b0f53166f627da \
+    --hash=sha256:c302c1b9728b4ce32eca8041e1375d51896832d84c84ce8eeb2577b73ffb0392 \
+    --hash=sha256:d47df7bf4e684639d3d83cc27d150c6d29b8bd5f0586ca0a9a040af6840a92b0 \
+    --hash=sha256:d5e3c0666abdc0d9c63790238a1b91a41f2e622b488df7276750f61351b12ccc \
+    --hash=sha256:fdcc9f733285bb1c917cde6731edcbf2ecc5ca4bd8c6a4c168a7f478e4056654
+    # via -r -
+tensorflow-estimator==2.11.0 \
+    --hash=sha256:ea3b64acfff3d9a244f06178c9bdedcbdd3f125b67d0888dba8229498d06468b
     # via tensorflow-cpu
-termcolor==2.4.0 \
-    --hash=sha256:9297c0df9c99445c2412e832e882a7884038a25617c60cea2ad69488d4040d63 \
-    --hash=sha256:aab9e56047c8ac41ed798fa36d892a37aca6b3e9159f3e0c24bc64a9b3ac7b7a
+tensorflow-io-gcs-filesystem==0.31.0 \
+    --hash=sha256:20e3ee5df01f2bd81d37fc715816c329b7533ccca967c47946eb458a5b7a7280 \
+    --hash=sha256:359134ecbd3bf938bb0cf65be4526106c30da461b2e2ce05446a229ed35f6832 \
+    --hash=sha256:37c40e3c4ee1f8dda3b545deea6b8839192c82037d8021db9f589908034ad975 \
+    --hash=sha256:4bb37d23f21c434687b11059cb7ffd094d52a7813368915ba1b7057e3c16e414 \
+    --hash=sha256:68b89ef9f63f297de1cd9d545bc45dddc7d8fe12bcda4266279b244e8cf3b7c0 \
+    --hash=sha256:8909c4344b0e96aa356230ab460ffafe5900c33c1aaced65fafae71d177a1966 \
+    --hash=sha256:961353b38c76471fa296bb7d883322c66b91415e7d47087236a6706db3ab2758 \
+    --hash=sha256:97ebb9a8001a38f615aa1f90d2e998b7bd6eddae7aafc92897833610b039401b \
+    --hash=sha256:a71421f8d75a093b6aac65b4c8c8d2f768c3ca6215307cf8c16192e62d992bcf \
+    --hash=sha256:a7e8d4bd0a25de7637e562997c011294d7ea595a76f315427a5dd522d56e9d49 \
+    --hash=sha256:b4ebb30ad7ce5f3769e3d959ea99bd95d80a44099bcf94da6042f9755ac6e850 \
+    --hash=sha256:b658b33567552f155af2ed848130f787bfda29381fa78cd905d5ee8254364f3c \
+    --hash=sha256:bd628609b77aee0e385eadf1628222486f19b8f1d81b5f0a344f2470204df116 \
+    --hash=sha256:cb7459c15608fe42973a78e4d3ad7ac79cfc7adae1ccb1b1846db3165fbc081a \
+    --hash=sha256:e3933059b1c53e062075de2e355ec136b655da5883c3c26736c45dfeb1901945 \
+    --hash=sha256:e417faf8755aafe52d8f8c6b5ae5bae6e4fae8326ee3acd5e9181b83bbfbae87 \
+    --hash=sha256:e6d8cc7b14ade870168b9704ee44f9c55b468b9a00ed40e12d20fffd321193b5 \
+    --hash=sha256:f0adfbcd264262797d429311843733da2d5c1ffb119fbfa6339269b6c0414113 \
+    --hash=sha256:fbcfb4aa2eaa9a3038d2487e570ff93feb1dbe51c3a4663d7d9ab9f9a9f9a9d8
     # via tensorflow-cpu
-typing-extensions==4.11.0 \
-    --hash=sha256:83f085bd5ca59c80295fc2a82ab5dac679cbe02b9f33f7d83af68e241bea51b0 \
-    --hash=sha256:c1f94d72897edaf4ce775bb7558d5b79d8126906a14ea5ed1635921406c0387a
-    # via
-    #   optree
-    #   tensorflow-cpu
-urllib3==2.2.1 \
-    --hash=sha256:450b20ec296a467077128bff42b73080516e71b56ff59a60a02bef2232c4fa9d \
-    --hash=sha256:d0570876c61ab9e520d776c38acbbb5b05a776d3f9ff98a5c8fd5162a444cf19
+termcolor==2.2.0 \
+    --hash=sha256:91ddd848e7251200eac969846cbae2dacd7d71c2871e92733289e7e3666f48e7 \
+    --hash=sha256:dfc8ac3f350788f23b2947b3e6cfa5a53b630b612e6cd8965a015a776020b99a
+    # via tensorflow-cpu
+typing-extensions==4.5.0 \
+    --hash=sha256:5cb5f4a79139d699607b3ef622a1dedafa84e115ab0024e0d9c044a9479ca7cb \
+    --hash=sha256:fb33085c39dd998ac16d1431ebc293a8b3eedd00fd4a32de0ff79002c19511b4
+    # via tensorflow-cpu
+urllib3==1.26.14 \
+    --hash=sha256:076907bf8fd355cde77728471316625a4d2f7e713c125f51953bb5b3eecf4f72 \
+    --hash=sha256:75edcdc2f7d85b137124a6c3c9fc3933cdeaa12ecb9a6a959f22797a0feca7e1
     # via requests
-werkzeug==3.0.3 \
-    --hash=sha256:097e5bfda9f0aba8da6b8545146def481d06aa7d3266e7448e2cccf67dd8bd18 \
-    --hash=sha256:fc9645dc43e03e4d630d23143a04a7f947a9a3b5727cd535fdfe155a17cc48c8
+werkzeug==2.2.3 \
+    --hash=sha256:2e1ccc9417d4da358b9de6f174e3ac094391ea1d4fbef2d667865d819dfd0afe \
+    --hash=sha256:56433961bc1f12533306c624f3be5e744389ac61d722175d543e1751285da612
     # via tensorboard
-wheel==0.43.0 \
-    --hash=sha256:465ef92c69fa5c5da2d1cf8ac40559a8c940886afcef87dcf14b9470862f1d85 \
-    --hash=sha256:55c570405f142630c6b9f72fe09d9b67cf1477fcf543ae5b8dcb1f5b7377da81
-    # via astunparse
-wrapt==1.16.0 \
-    --hash=sha256:0d2691979e93d06a95a26257adb7bfd0c93818e89b1406f5a28f36e0d8c1e1fc \
-    --hash=sha256:14d7dc606219cdd7405133c713f2c218d4252f2a469003f8c46bb92d5d095d81 \
-    --hash=sha256:1a5db485fe2de4403f13fafdc231b0dbae5eca4359232d2efc79025527375b09 \
-    --hash=sha256:1acd723ee2a8826f3d53910255643e33673e1d11db84ce5880675954183ec47e \
-    --hash=sha256:1ca9b6085e4f866bd584fb135a041bfc32cab916e69f714a7d1d397f8c4891ca \
-    --hash=sha256:1dd50a2696ff89f57bd8847647a1c363b687d3d796dc30d4dd4a9d1689a706f0 \
-    --hash=sha256:2076fad65c6736184e77d7d4729b63a6d1ae0b70da4868adeec40989858eb3fb \
-    --hash=sha256:2a88e6010048489cda82b1326889ec075a8c856c2e6a256072b28eaee3ccf487 \
-    --hash=sha256:3ebf019be5c09d400cf7b024aa52b1f3aeebeff51550d007e92c3c1c4afc2a40 \
-    --hash=sha256:418abb18146475c310d7a6dc71143d6f7adec5b004ac9ce08dc7a34e2babdc5c \
-    --hash=sha256:43aa59eadec7890d9958748db829df269f0368521ba6dc68cc172d5d03ed8060 \
-    --hash=sha256:44a2754372e32ab315734c6c73b24351d06e77ffff6ae27d2ecf14cf3d229202 \
-    --hash=sha256:490b0ee15c1a55be9c1bd8609b8cecd60e325f0575fc98f50058eae366e01f41 \
-    --hash=sha256:49aac49dc4782cb04f58986e81ea0b4768e4ff197b57324dcbd7699c5dfb40b9 \
-    --hash=sha256:5eb404d89131ec9b4f748fa5cfb5346802e5ee8836f57d516576e61f304f3b7b \
-    --hash=sha256:5f15814a33e42b04e3de432e573aa557f9f0f56458745c2074952f564c50e664 \
-    --hash=sha256:5f370f952971e7d17c7d1ead40e49f32345a7f7a5373571ef44d800d06b1899d \
-    --hash=sha256:66027d667efe95cc4fa945af59f92c5a02c6f5bb6012bff9e60542c74c75c362 \
-    --hash=sha256:66dfbaa7cfa3eb707bbfcd46dab2bc6207b005cbc9caa2199bcbc81d95071a00 \
-    --hash=sha256:685f568fa5e627e93f3b52fda002c7ed2fa1800b50ce51f6ed1d572d8ab3e7fc \
-    --hash=sha256:6906c4100a8fcbf2fa735f6059214bb13b97f75b1a61777fcf6432121ef12ef1 \
-    --hash=sha256:6a42cd0cfa8ffc1915aef79cb4284f6383d8a3e9dcca70c445dcfdd639d51267 \
-    --hash=sha256:6dcfcffe73710be01d90cae08c3e548d90932d37b39ef83969ae135d36ef3956 \
-    --hash=sha256:6f6eac2360f2d543cc875a0e5efd413b6cbd483cb3ad7ebf888884a6e0d2e966 \
-    --hash=sha256:72554a23c78a8e7aa02abbd699d129eead8b147a23c56e08d08dfc29cfdddca1 \
-    --hash=sha256:73870c364c11f03ed072dda68ff7aea6d2a3a5c3fe250d917a429c7432e15228 \
-    --hash=sha256:73aa7d98215d39b8455f103de64391cb79dfcad601701a3aa0dddacf74911d72 \
-    --hash=sha256:75ea7d0ee2a15733684badb16de6794894ed9c55aa5e9903260922f0482e687d \
-    --hash=sha256:7bd2d7ff69a2cac767fbf7a2b206add2e9a210e57947dd7ce03e25d03d2de292 \
-    --hash=sha256:807cc8543a477ab7422f1120a217054f958a66ef7314f76dd9e77d3f02cdccd0 \
-    --hash=sha256:8e9723528b9f787dc59168369e42ae1c3b0d3fadb2f1a71de14531d321ee05b0 \
-    --hash=sha256:9090c9e676d5236a6948330e83cb89969f433b1943a558968f659ead07cb3b36 \
-    --hash=sha256:9153ed35fc5e4fa3b2fe97bddaa7cbec0ed22412b85bcdaf54aeba92ea37428c \
-    --hash=sha256:9159485323798c8dc530a224bd3ffcf76659319ccc7bbd52e01e73bd0241a0c5 \
-    --hash=sha256:941988b89b4fd6b41c3f0bfb20e92bd23746579736b7343283297c4c8cbae68f \
-    --hash=sha256:94265b00870aa407bd0cbcfd536f17ecde43b94fb8d228560a1e9d3041462d73 \
-    --hash=sha256:98b5e1f498a8ca1858a1cdbffb023bfd954da4e3fa2c0cb5853d40014557248b \
-    --hash=sha256:9b201ae332c3637a42f02d1045e1d0cccfdc41f1f2f801dafbaa7e9b4797bfc2 \
-    --hash=sha256:a0ea261ce52b5952bf669684a251a66df239ec6d441ccb59ec7afa882265d593 \
-    --hash=sha256:a33a747400b94b6d6b8a165e4480264a64a78c8a4c734b62136062e9a248dd39 \
-    --hash=sha256:a452f9ca3e3267cd4d0fcf2edd0d035b1934ac2bd7e0e57ac91ad6b95c0c6389 \
-    --hash=sha256:a86373cf37cd7764f2201b76496aba58a52e76dedfaa698ef9e9688bfd9e41cf \
-    --hash=sha256:ac83a914ebaf589b69f7d0a1277602ff494e21f4c2f743313414378f8f50a4cf \
-    --hash=sha256:aefbc4cb0a54f91af643660a0a150ce2c090d3652cf4052a5397fb2de549cd89 \
-    --hash=sha256:b3646eefa23daeba62643a58aac816945cadc0afaf21800a1421eeba5f6cfb9c \
-    --hash=sha256:b47cfad9e9bbbed2339081f4e346c93ecd7ab504299403320bf85f7f85c7d46c \
-    --hash=sha256:b935ae30c6e7400022b50f8d359c03ed233d45b725cfdd299462f41ee5ffba6f \
-    --hash=sha256:bb2dee3874a500de01c93d5c71415fcaef1d858370d405824783e7a8ef5db440 \
-    --hash=sha256:bc57efac2da352a51cc4658878a68d2b1b67dbe9d33c36cb826ca449d80a8465 \
-    --hash=sha256:bf5703fdeb350e36885f2875d853ce13172ae281c56e509f4e6eca049bdfb136 \
-    --hash=sha256:c31f72b1b6624c9d863fc095da460802f43a7c6868c5dda140f51da24fd47d7b \
-    --hash=sha256:c5cd603b575ebceca7da5a3a251e69561bec509e0b46e4993e1cac402b7247b8 \
-    --hash=sha256:d2efee35b4b0a347e0d99d28e884dfd82797852d62fcd7ebdeee26f3ceb72cf3 \
-    --hash=sha256:d462f28826f4657968ae51d2181a074dfe03c200d6131690b7d65d55b0f360f8 \
-    --hash=sha256:d5e49454f19ef621089e204f862388d29e6e8d8b162efce05208913dde5b9ad6 \
-    --hash=sha256:da4813f751142436b075ed7aa012a8778aa43a99f7b36afe9b742d3ed8bdc95e \
-    --hash=sha256:db2e408d983b0e61e238cf579c09ef7020560441906ca990fe8412153e3b291f \
-    --hash=sha256:db98ad84a55eb09b3c32a96c576476777e87c520a34e2519d3e59c44710c002c \
-    --hash=sha256:dbed418ba5c3dce92619656802cc5355cb679e58d0d89b50f116e4a9d5a9603e \
-    --hash=sha256:dcdba5c86e368442528f7060039eda390cc4091bfd1dca41e8046af7c910dda8 \
-    --hash=sha256:decbfa2f618fa8ed81c95ee18a387ff973143c656ef800c9f24fb7e9c16054e2 \
-    --hash=sha256:e4fdb9275308292e880dcbeb12546df7f3e0f96c6b41197e0cf37d2826359020 \
-    --hash=sha256:eb1b046be06b0fce7249f1d025cd359b4b80fc1c3e24ad9eca33e0dcdb2e4a35 \
-    --hash=sha256:eb6e651000a19c96f452c85132811d25e9264d836951022d6e81df2fff38337d \
-    --hash=sha256:ed867c42c268f876097248e05b6117a65bcd1e63b779e916fe2e33cd6fd0d3c3 \
-    --hash=sha256:edfad1d29c73f9b863ebe7082ae9321374ccb10879eeabc84ba3b69f2579d537 \
-    --hash=sha256:f2058f813d4f2b5e3a9eb2eb3faf8f1d99b81c3e51aeda4b168406443e8ba809 \
-    --hash=sha256:f6b2d0c6703c988d334f297aa5df18c45e97b0af3679bb75059e0e0bd8b1069d \
-    --hash=sha256:f8212564d49c50eb4565e502814f694e240c55551a5f1bc841d4fcaabb0a9b8a \
-    --hash=sha256:ffa565331890b90056c01db69c0fe634a776f8019c143a5ae265f9c6bc4bd6d4
+wheel==0.38.4 \
+    --hash=sha256:965f5259b566725405b05e7cf774052044b1ed30119b5d586b2703aafe8719ac \
+    --hash=sha256:b60533f3f5d530e971d6737ca6d58681ee434818fab630c83a734bb10c083ce8
+    # via
+    #   astunparse
+    #   tensorboard
+wrapt==1.15.0 \
+    --hash=sha256:02fce1852f755f44f95af51f69d22e45080102e9d00258053b79367d07af39c0 \
+    --hash=sha256:077ff0d1f9d9e4ce6476c1a924a3332452c1406e59d90a2cf24aeb29eeac9420 \
+    --hash=sha256:078e2a1a86544e644a68422f881c48b84fef6d18f8c7a957ffd3f2e0a74a0d4a \
+    --hash=sha256:0970ddb69bba00670e58955f8019bec4a42d1785db3faa043c33d81de2bf843c \
+    --hash=sha256:1286eb30261894e4c70d124d44b7fd07825340869945c79d05bda53a40caa079 \
+    --hash=sha256:21f6d9a0d5b3a207cdf7acf8e58d7d13d463e639f0c7e01d82cdb671e6cb7923 \
+    --hash=sha256:230ae493696a371f1dbffaad3dafbb742a4d27a0afd2b1aecebe52b740167e7f \
+    --hash=sha256:26458da5653aa5b3d8dc8b24192f574a58984c749401f98fff994d41d3f08da1 \
+    --hash=sha256:2cf56d0e237280baed46f0b5316661da892565ff58309d4d2ed7dba763d984b8 \
+    --hash=sha256:2e51de54d4fb8fb50d6ee8327f9828306a959ae394d3e01a1ba8b2f937747d86 \
+    --hash=sha256:2fbfbca668dd15b744418265a9607baa970c347eefd0db6a518aaf0cfbd153c0 \
+    --hash=sha256:38adf7198f8f154502883242f9fe7333ab05a5b02de7d83aa2d88ea621f13364 \
+    --hash=sha256:3a8564f283394634a7a7054b7983e47dbf39c07712d7b177b37e03f2467a024e \
+    --hash=sha256:3abbe948c3cbde2689370a262a8d04e32ec2dd4f27103669a45c6929bcdbfe7c \
+    --hash=sha256:3bbe623731d03b186b3d6b0d6f51865bf598587c38d6f7b0be2e27414f7f214e \
+    --hash=sha256:40737a081d7497efea35ab9304b829b857f21558acfc7b3272f908d33b0d9d4c \
+    --hash=sha256:41d07d029dd4157ae27beab04d22b8e261eddfc6ecd64ff7000b10dc8b3a5727 \
+    --hash=sha256:46ed616d5fb42f98630ed70c3529541408166c22cdfd4540b88d5f21006b0eff \
+    --hash=sha256:493d389a2b63c88ad56cdc35d0fa5752daac56ca755805b1b0c530f785767d5e \
+    --hash=sha256:4ff0d20f2e670800d3ed2b220d40984162089a6e2c9646fdb09b85e6f9a8fc29 \
+    --hash=sha256:54accd4b8bc202966bafafd16e69da9d5640ff92389d33d28555c5fd4f25ccb7 \
+    --hash=sha256:56374914b132c702aa9aa9959c550004b8847148f95e1b824772d453ac204a72 \
+    --hash=sha256:578383d740457fa790fdf85e6d346fda1416a40549fe8db08e5e9bd281c6a475 \
+    --hash=sha256:58d7a75d731e8c63614222bcb21dd992b4ab01a399f1f09dd82af17bbfc2368a \
+    --hash=sha256:5c5aa28df055697d7c37d2099a7bc09f559d5053c3349b1ad0c39000e611d317 \
+    --hash=sha256:5fc8e02f5984a55d2c653f5fea93531e9836abbd84342c1d1e17abc4a15084c2 \
+    --hash=sha256:63424c681923b9f3bfbc5e3205aafe790904053d42ddcc08542181a30a7a51bd \
+    --hash=sha256:64b1df0f83706b4ef4cfb4fb0e4c2669100fd7ecacfb59e091fad300d4e04640 \
+    --hash=sha256:74934ebd71950e3db69960a7da29204f89624dde411afbfb3b4858c1409b1e98 \
+    --hash=sha256:75669d77bb2c071333417617a235324a1618dba66f82a750362eccbe5b61d248 \
+    --hash=sha256:75760a47c06b5974aa5e01949bf7e66d2af4d08cb8c1d6516af5e39595397f5e \
+    --hash=sha256:76407ab327158c510f44ded207e2f76b657303e17cb7a572ffe2f5a8a48aa04d \
+    --hash=sha256:76e9c727a874b4856d11a32fb0b389afc61ce8aaf281ada613713ddeadd1cfec \
+    --hash=sha256:77d4c1b881076c3ba173484dfa53d3582c1c8ff1f914c6461ab70c8428b796c1 \
+    --hash=sha256:780c82a41dc493b62fc5884fb1d3a3b81106642c5c5c78d6a0d4cbe96d62ba7e \
+    --hash=sha256:7dc0713bf81287a00516ef43137273b23ee414fe41a3c14be10dd95ed98a2df9 \
+    --hash=sha256:7eebcdbe3677e58dd4c0e03b4f2cfa346ed4049687d839adad68cc38bb559c92 \
+    --hash=sha256:896689fddba4f23ef7c718279e42f8834041a21342d95e56922e1c10c0cc7afb \
+    --hash=sha256:96177eb5645b1c6985f5c11d03fc2dbda9ad24ec0f3a46dcce91445747e15094 \
+    --hash=sha256:96e25c8603a155559231c19c0349245eeb4ac0096fe3c1d0be5c47e075bd4f46 \
+    --hash=sha256:9d37ac69edc5614b90516807de32d08cb8e7b12260a285ee330955604ed9dd29 \
+    --hash=sha256:9ed6aa0726b9b60911f4aed8ec5b8dd7bf3491476015819f56473ffaef8959bd \
+    --hash=sha256:a487f72a25904e2b4bbc0817ce7a8de94363bd7e79890510174da9d901c38705 \
+    --hash=sha256:a4cbb9ff5795cd66f0066bdf5947f170f5d63a9274f99bdbca02fd973adcf2a8 \
+    --hash=sha256:a74d56552ddbde46c246b5b89199cb3fd182f9c346c784e1a93e4dc3f5ec9975 \
+    --hash=sha256:a89ce3fd220ff144bd9d54da333ec0de0399b52c9ac3d2ce34b569cf1a5748fb \
+    --hash=sha256:abd52a09d03adf9c763d706df707c343293d5d106aea53483e0ec8d9e310ad5e \
+    --hash=sha256:abd8f36c99512755b8456047b7be10372fca271bf1467a1caa88db991e7c421b \
+    --hash=sha256:af5bd9ccb188f6a5fdda9f1f09d9f4c86cc8a539bd48a0bfdc97723970348418 \
+    --hash=sha256:b02f21c1e2074943312d03d243ac4388319f2456576b2c6023041c4d57cd7019 \
+    --hash=sha256:b06fa97478a5f478fb05e1980980a7cdf2712015493b44d0c87606c1513ed5b1 \
+    --hash=sha256:b0724f05c396b0a4c36a3226c31648385deb6a65d8992644c12a4963c70326ba \
+    --hash=sha256:b130fe77361d6771ecf5a219d8e0817d61b236b7d8b37cc045172e574ed219e6 \
+    --hash=sha256:b56d5519e470d3f2fe4aa7585f0632b060d532d0696c5bdfb5e8319e1d0f69a2 \
+    --hash=sha256:b67b819628e3b748fd3c2192c15fb951f549d0f47c0449af0764d7647302fda3 \
+    --hash=sha256:ba1711cda2d30634a7e452fc79eabcadaffedf241ff206db2ee93dd2c89a60e7 \
+    --hash=sha256:bbeccb1aa40ab88cd29e6c7d8585582c99548f55f9b2581dfc5ba68c59a85752 \
+    --hash=sha256:bd84395aab8e4d36263cd1b9308cd504f6cf713b7d6d3ce25ea55670baec5416 \
+    --hash=sha256:c99f4309f5145b93eca6e35ac1a988f0dc0a7ccf9ccdcd78d3c0adf57224e62f \
+    --hash=sha256:ca1cccf838cd28d5a0883b342474c630ac48cac5df0ee6eacc9c7290f76b11c1 \
+    --hash=sha256:cd525e0e52a5ff16653a3fc9e3dd827981917d34996600bbc34c05d048ca35cc \
+    --hash=sha256:cdb4f085756c96a3af04e6eca7f08b1345e94b53af8921b25c72f096e704e145 \
+    --hash=sha256:ce42618f67741d4697684e501ef02f29e758a123aa2d669e2d964ff734ee00ee \
+    --hash=sha256:d06730c6aed78cee4126234cf2d071e01b44b915e725a6cb439a879ec9754a3a \
+    --hash=sha256:d5fe3e099cf07d0fb5a1e23d399e5d4d1ca3e6dfcbe5c8570ccff3e9208274f7 \
+    --hash=sha256:d6bcbfc99f55655c3d93feb7ef3800bd5bbe963a755687cbf1f490a71fb7794b \
+    --hash=sha256:d787272ed958a05b2c86311d3a4135d3c2aeea4fc655705f074130aa57d71653 \
+    --hash=sha256:e169e957c33576f47e21864cf3fc9ff47c223a4ebca8960079b8bd36cb014fd0 \
+    --hash=sha256:e20076a211cd6f9b44a6be58f7eeafa7ab5720eb796975d0c03f05b47d89eb90 \
+    --hash=sha256:e826aadda3cae59295b95343db8f3d965fb31059da7de01ee8d1c40a60398b29 \
+    --hash=sha256:eef4d64c650f33347c1f9266fa5ae001440b232ad9b98f1f43dfe7a79435c0a6 \
+    --hash=sha256:f2e69b3ed24544b0d3dbe2c5c0ba5153ce50dcebb576fdc4696d52aa22db6034 \
+    --hash=sha256:f87ec75864c37c4c6cb908d282e1969e79763e0d9becdfe9fe5473b7bb1e5f09 \
+    --hash=sha256:fbec11614dba0424ca72f4e8ba3c420dba07b4a7c206c8c8e4e73f2e98f4c559 \
+    --hash=sha256:fd69666217b62fa5d7c6aa88e507493a34dec4fa20c5bd925e4bc12fce586639
     # via tensorflow-cpu
 
 # WARNING: The following packages were not pinned, but pip requires them to be
-# pinned when the requirements file includes hashes and the requirement is not
-# satisfied by a package already installed. Consider using the --allow-unsafe flag.
+# pinned when the requirements file includes hashes. Consider using the --allow-unsafe flag.
 # setuptools
diff --git a/docs/README.md b/docs/README.md
deleted file mode 100644
index 4a4e9a1..0000000
--- a/docs/README.md
+++ /dev/null
@@ -1,5 +0,0 @@
-# Documentation on llvm android toolchain
-
-- [Toolchain errors](./toolchain-errors.md)
-- [Compiler errors](./compiler-errors.md)
-- [TODO List of documentation](./TODO.md)
\ No newline at end of file
diff --git a/docs/TODO.md b/docs/TODO.md
deleted file mode 100644
index 5f2ce57..0000000
--- a/docs/TODO.md
+++ /dev/null
@@ -1,7 +0,0 @@
-# TODO list of documentation
-
-- Reducing compiler errors
-- Sharing linker reproducer
-- Generate preprocessed file
-- Artifacts to share when reporting compiler error
-- cvise to reduce compiler error.
diff --git a/docs/compiler-errors.md b/docs/compiler-errors.md
deleted file mode 100644
index d98d102..0000000
--- a/docs/compiler-errors.md
+++ /dev/null
@@ -1,59 +0,0 @@
-# Dealing with compiler errors
-
-It becomes quite difficult to reproduce a compiler error on someone
-else's machine when they do not have the same setup. There are several
-variables that can come into play.
-
-- Compiler version (e.g., clang-16 vs. clang-17)
-- Toolchain version (e.g., r25 vs r26)
-- Compiler invocation commands
-  - Optimization flags (e.g., `-O3 -fno-vectorize`)
-  - Macro definitions (e.g., `-DNDEBUG`)
-  - Include paths (e.g., `-I/my/local/usr/include/`)
-  - Warning flags (e.g., `-Wall`)
-  - Language specific flags (e.g., `-std=c++20`)
-- Local machine environment (e.g, Windows vs. Linux)
-
-While all of them may not be affecting the issue at hand, it is better
-to minimize the differences in order to quickly get to the bottom of
-it. Many of the variable above can be reduced by providing the
-compiler invocation command and a preprocessed file.
-
-## Things to share when reporting compiler error
-
-- The preprocessed file (.i for C, .ii for C++)
-- Compiler invocation command
-- Details on development environment
-
-## Generating preprocessed file
-
-A preprocessed file is a standalone translation unit that doesn't need
-any header file depenencies and macro definitions to reproduce
-compilation on another machine.
-
-Use the `--save-temps` compiler option, for example:
-
-```sh
-user@laptop: ~/Desktop/bugs$ clang++ -std=c++20 a.cpp --save-temps -c
-```
-
-This will generate `a.ii` file in the same directory from where clang
-was invoked e.g., `~/Desktop/bugs/a.ii` in this case.
-
-## Getting the compiler invocation command
-
-A compiler invocation command is what the compiler is fed when
-building a translation unit. For example `clang++ -std=c++20 a.cpp
---save-temps -c` is a compiler invocation. When working with complex
-build systems(e.g., bazel, scons) it may not be straightforward to get
-hold of the compiler invocation. Usually when there are compiler
-errors, the build system will print the command on the console. Build
-systems might also have a **verbose** mode to print all the compiler
-invocations.
-
-## Getting the details of developer environment
-
-It is often helpful to know more about the developer environment like
-
-- Operating system (Linux, Windows, Mac etc.)
-- Processor type (X86_64, Arm64 etc.)
diff --git a/docs/toolchain-errors.md b/docs/toolchain-errors.md
deleted file mode 100644
index cfc5dfb..0000000
--- a/docs/toolchain-errors.md
+++ /dev/null
@@ -1,18 +0,0 @@
-# Different types of errors while working with system software
-
-A loose classification of different types of errors are listed
-below. The goal is to help programmers make informed decision to debug
-themselves.
-
-## Programmer error
-
-- Syntax error
-- Semantic error
-- Logical error
-- Build error
-
-## Tools error
-
-- Internal compiler error
-- Miscompile
-- Library incompatibilities
diff --git a/fetch_kokoro_prebuilts.py b/fetch_kokoro_prebuilts.py
deleted file mode 100755
index 6d2202f..0000000
--- a/fetch_kokoro_prebuilts.py
+++ /dev/null
@@ -1,218 +0,0 @@
-#!/usr/bin/env python3
-#
-# Copyright (C) 2024 The Android Open Source Project
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-import argparse
-import os
-import subprocess
-import sys
-import tempfile
-from typing import List, Optional
-from utils import check_tools, extract_tarball
-
-prefix = "gs://android-llvm-kokoro-ci-artifacts/prod/android-llvm/linux-tot/continuous/"
-
-
-def parse_args(sys_argv: Optional[List[str]]):
-    """Parse the command line arguments."""
-
-    parser = argparse.ArgumentParser(description="Fetch prebuilts from kokoro.")
-
-    ref = parser.add_mutually_exclusive_group(required=True)
-    ref.add_argument(
-        "--sha",
-        type=str,
-        nargs="?",
-        help="the build sha of llvm-project of one Kokoro build in Test fusion",
-    )
-
-    ref.add_argument(
-        "--build_id",
-        type=str,
-        nargs="?",
-        help=(
-            "the build id in"
-            " https://pantheon.corp.google.com/storage/browser/android-llvm-kokoro-ci-artifacts"
-        ),
-    )
-
-    parser.add_argument(
-        "target",
-        type=str,
-        nargs=1,
-        help="Target Clang path (e.g. ANDROID_TOP/prebuilts/clang/linux-x86/)",
-    )
-    return parser.parse_args(sys_argv)
-
-
-def get_url(build_id: str):
-    suffix = "/**.tar.xz"
-    url = prefix + build_id + suffix
-    return url
-
-
-def fetch_prebuilts(build_id: str, target: str):
-    gsutil_url = get_url(build_id)
-    with tempfile.TemporaryDirectory() as td:
-        cmd = ["gsutil", "cp", gsutil_url, td]
-        result = subprocess.run(cmd, stderr=subprocess.PIPE)
-        if result.returncode > 0:
-            err_string = str(result.stderr, encoding="utf-8")
-            if "CommandException: No URLs matched" in err_string:
-                print(f"Build {build_id} failed to build.")
-            else:
-                print(err_string)
-        else:
-            print(f"Download build {build_id} successful!")
-
-            # extract the toolchain
-            tar = os.path.abspath(td) + "/" + os.listdir(td)[0]
-            extract_tarball(target, tar)
-            return True
-    return False
-
-
-def check_valid_build(build_id: str):
-    url = prefix + build_id + "/"
-    output = subprocess.check_output(["gsutil", "ls", "-L", prefix])
-    if url not in str(output):
-        err_msg = build_id + " doesn't exist. Please pick a valid build id."
-        raise Exception(err_msg)
-
-
-def check_valid_path(target: str):
-    if not os.path.exists(target):
-        err_msg = target + " doesn't exist. Please pass a valid path."
-        raise Exception(err_msg)
-
-
-def get_build_number(sha: str):
-    """Find the build number that contains the SHA.
-
-    Find if SHA is the latest change in any build.
-    If not, find if the SHA is included in the latest build and get the
-    latest build number so we can walk the kokoro builds.
-    Keep looking at older builds until we find a match or exhaust all builds.
-
-    Args:
-        sha: the SHA of the llvm-project change.
-
-    Returns:
-        The build number that contains the SHA.
-    """
-    request = f"""
-        full_job_name: "android-llvm/linux-tot/continuous"
-        multi_scm_revision: {{
-          git_on_borg_scm_revision: {{
-            name: "toolchain/llvm-project",
-            branch: "main",
-            sha1: "{sha}"
-          }}
-        }}
-    """
-
-    build_number = None
-    # Send the request.
-    cmd = [
-        "stubby",
-        "call",
-        "blade:kokoro-api",
-        "KokoroApi.GetBuildStatus",
-        request,
-    ]
-
-    response = subprocess.run(cmd, capture_output=True, text=True)
-    if response.returncode == 0:
-        # The sha is from the latest change in any build.
-        data = response.stdout
-        for line in data.splitlines():
-            if line.startswith("build_number:"):
-                build_number = line.split()[-1]
-                return build_number
-
-    found = False
-    id_line = f'id: "{sha}"'
-    request = """
-        full_job_name: "android-llvm/linux-tot/continuous"
-        multi_scm_revision: {
-        git_on_borg_scm_revision: {
-            name: "toolchain/llvm-project",
-            branch: "main"
-        }
-        }
-    """
-
-    while not found:
-        try:
-            cmd[-1] = request
-            response = subprocess.check_output(cmd)
-        except subprocess.CalledProcessError as e:
-            print(
-                f"No build number for {sha} matched!",
-                file=sys.stderr,
-            )
-            sys.exit(1)
-
-        # Parse the response.
-        data = response.decode("utf-8")
-        for line in data.splitlines():
-            line = line.strip()
-            if line == id_line:
-                found = True
-            if line.startswith("build_number:"):
-                build_number = line.split()[-1]
-                break
-
-        # The current build doesn't have the SHA.
-        # Try to find the SHA in the next older build.
-        if not found:
-            build_number = str(int(build_number) - 1)
-            request = f"""
-                build_number: {build_number}
-                full_job_name: "android-llvm/linux-tot/continuous"
-                multi_scm_revision: {{
-                git_on_borg_scm_revision: {{
-                    name: "toolchain/llvm-project",
-                    branch: "main"
-                }}
-                }}
-            """
-
-    return build_number
-
-
-def main(sys_argv: List[str]):
-    args_output = parse_args(sys_argv)
-
-    check_tools(args_output.sha)
-
-    target = args_output.target[0]
-    check_valid_path(target)
-
-    if args_output.sha:
-        build_id = get_build_number(args_output.sha)
-    else:
-        build_id = args_output.build_id
-
-    check_valid_build(build_id)
-
-    fetch_prebuilts(build_id, target)
-
-    return 0
-
-
-if __name__ == "__main__":
-    main(sys.argv[1:])
diff --git a/hosts.py b/hosts.py
old mode 100644
new mode 100755
index 13fb1a9..ab113be
--- a/hosts.py
+++ b/hosts.py
@@ -91,28 +91,6 @@ class Arch(enum.Enum):
         }[self]
 
 
-@enum.unique
-class Armv81MMainFpu(enum.Enum):
-    """Enumeration of supported Armv8.1-M mainline FPUs."""
-    NONE = 'nofp'
-    SINGLE = 'fp'
-    DOUBLE = 'fp.dp'
-
-    @property
-    def llvm_fpu(self) -> str:
-        """Converts to llvm FPU name."""
-        return {
-            Armv81MMainFpu.NONE: 'none',
-            Armv81MMainFpu.SINGLE: 'fp-armv8-fullfp16-sp-d16',
-            Armv81MMainFpu.DOUBLE: 'fp-armv8-fullfp16-d16',
-        }[self]
-
-    @property
-    def llvm_float_abi(self) -> str:
-        """Converts to llvm float-abi."""
-        return 'soft' if self == Armv81MMainFpu.NONE else 'hard'
-
-
 def _get_default_host() -> Host:
     """Returns the Host matching the current machine."""
     if sys.platform.startswith('linux'):
diff --git a/kernel-boot-tests/.gitignore b/kernel-boot-tests/.gitignore
old mode 100644
new mode 100755
diff --git a/kokoro/aosp-master-plus-llvm.cfg b/kokoro/aosp-master-plus-llvm.cfg
new file mode 100755
index 0000000..72cbdc8
--- /dev/null
+++ b/kokoro/aosp-master-plus-llvm.cfg
@@ -0,0 +1,7 @@
+build_file: "git/toolchain/llvm_android/kokoro/aosp-master-plus-llvm_build.sh"
+
+action {
+  define_artifacts {
+    regex: "git/dist/*"
+  }
+}
diff --git a/kokoro/aosp-master-plus-llvm_build.sh b/kokoro/aosp-master-plus-llvm_build.sh
index 6c9420d..6c83cf7 100755
--- a/kokoro/aosp-master-plus-llvm_build.sh
+++ b/kokoro/aosp-master-plus-llvm_build.sh
@@ -2,6 +2,8 @@
 set -e
 
 TOP=$(cd $(dirname $0)/../../.. && pwd)
+OUT=$TOP/out
+DIST=$TOP/dist
 
 # Kokoro will rsync back everything created by the build. Since we don't care
 # about any artifacts on this build, nuke EVERYTHING at the end of the build.
@@ -10,29 +12,12 @@ function cleanup {
 }
 trap cleanup EXIT
 
-cd $TOP
-
 # Fetch aosp-plus-llvm-master repo
-repo init -u https://android.googlesource.com/platform/manifest -b main --depth=1 < /dev/null
-repo sync -c
+(cd $TOP; \
+  repo init -u https://android.googlesource.com/platform/manifest -b master --depth=1 < /dev/null; \
+  repo sync -c -j8)
 
-mkdir dist
-USE_RBE=true \
-RBE_instance="projects/android-dev-builds/instances/default_instance" \
-RBE_service="remotebuildexecution.googleapis.com:443" \
-RBE_use_gce_credentials=true \
-RBE_CXX=false \
-RBE_JAVAC=false \
-RBE_D8=true \
-RBE_D8_EXEC_STRATEGY=remote_local_fallback \
-RBE_R8=true \
-RBE_R8_EXEC_STRATEGY=remote_local_fallback \
-RBE_METALAVA=true \
-RBE_METALAVA_EXEC_STRATEGY=remote_local_fallback \
-RBE_METALAVA_REMOTE_UPDATE_CACHE=false \
-DIST_DIR=dist \
-OUT_DIR=out \
-prebuilts/python/linux-x86/bin/python3 \
-  toolchain/llvm_android/test_compiler.py --build-only \
-  --target ${AOSP_BUILD_TARGET}-trunk_staging-userdebug \
-  --clang-package-path ${KOKORO_GFILE_DIR} .
+mkdir "${DIST}"
+DIST_DIR="${DIST}" OUT_DIR="${OUT}" $TOP/prebuilts/python/linux-x86/bin/python3 \
+  $TOP/toolchain/llvm_android/test_compiler.py --build-only --target aosp_arm64-userdebug \
+  --clang-package-path ${KOKORO_GFILE_DIR} $TOP
diff --git a/kokoro/aosp_arm64.cfg b/kokoro/aosp_arm64.cfg
deleted file mode 100644
index 22d3e15..0000000
--- a/kokoro/aosp_arm64.cfg
+++ /dev/null
@@ -1,10 +0,0 @@
-# Format: //devtools/kokoro/config/proto/build.proto
-
-build_file: "git/toolchain/llvm_android/kokoro/aosp-master-plus-llvm_build.sh"
-timeout_mins: 480
-
-env_vars {
-  key: "AOSP_BUILD_TARGET"
-  value: "aosp_cf_arm64_phone"
-}
-
diff --git a/kokoro/aosp_riscv64.cfg b/kokoro/aosp_riscv64.cfg
deleted file mode 100644
index 00853b6..0000000
--- a/kokoro/aosp_riscv64.cfg
+++ /dev/null
@@ -1,10 +0,0 @@
-# Format: //devtools/kokoro/config/proto/build.proto
-
-build_file: "git/toolchain/llvm_android/kokoro/aosp-master-plus-llvm_build.sh"
-timeout_mins: 480
-
-env_vars {
-  key: "AOSP_BUILD_TARGET"
-  value: "aosp_cf_riscv64_phone"
-}
-
diff --git a/kokoro/aosp_x86_64.cfg b/kokoro/aosp_x86_64.cfg
deleted file mode 100644
index 7f790a8..0000000
--- a/kokoro/aosp_x86_64.cfg
+++ /dev/null
@@ -1,10 +0,0 @@
-# Format: //devtools/kokoro/config/proto/build.proto
-
-build_file: "git/toolchain/llvm_android/kokoro/aosp-master-plus-llvm_build.sh"
-timeout_mins: 480
-
-env_vars {
-  key: "AOSP_BUILD_TARGET"
-  value: "aosp_cf_x86_64_phone"
-}
-
diff --git a/kokoro/common.cfg b/kokoro/common.cfg
old mode 100644
new mode 100755
index 6633d3f..227da93
--- a/kokoro/common.cfg
+++ b/kokoro/common.cfg
@@ -1,9 +1,23 @@
 # Format: //devtools/kokoro/config/proto/build.proto
 
-# These are artifacts we want to save once the build is done.
-action {
-  define_artifacts {
-    regex: "git/dist/*"
-  }
+env_vars {
+  key: "LLVM_BUILD_TYPE"
+  value: "linux-master"
 }
 
+env_vars {
+  key: "SCCACHE_GCS_BUCKET"
+  value: "android-llvm-sccache"
+}
+
+env_vars {
+  key: "SCCACHE_GCS_RW_MODE"
+  value: "READ_WRITE"
+}
+
+env_vars {
+  key: "SCCACHE_GCS_KEY_PATH"
+  value: "/tmpfs/src/gfile/google.com_android-llvm-kokoro-b72d0a7b8cf9.json"
+}
+
+gfile_resources: "/bigstore/android-llvm-sccache-key/google.com_android-llvm-kokoro-b72d0a7b8cf9.json"
diff --git a/kokoro/darwin.cfg b/kokoro/darwin.cfg
new file mode 100755
index 0000000..1ce40f6
--- /dev/null
+++ b/kokoro/darwin.cfg
@@ -0,0 +1,18 @@
+# Format: //devtools/kokoro/config/proto/build.proto
+
+build_file: "git/toolchain/llvm_android/kokoro/llvm_build.sh"
+
+# These are artifacts we want to save once the build is done.
+action {
+  define_artifacts {
+    regex: "git/dist/*"
+  }
+}
+
+env_vars {
+  key: "LLVM_BUILD_TYPE"
+  value: "darwin-master"
+}
+
+# Kokoro's Darwin builders are tiny, allow up to 8 hours for the build to finish.
+timeout_mins: 960
diff --git a/kokoro/linux-tot.cfg b/kokoro/linux-tot.cfg
old mode 100644
new mode 100755
index 887377b..efbed40
--- a/kokoro/linux-tot.cfg
+++ b/kokoro/linux-tot.cfg
@@ -2,6 +2,13 @@
 
 build_file: "git/toolchain/llvm_android/kokoro/llvm_build.sh"
 
+# These are artifacts we want to save once the build is done.
+action {
+  define_artifacts {
+    regex: "git/dist/*"
+  }
+}
+
 env_vars {
   key: "LLVM_BUILD_TYPE"
   value: "linux-TOT"
diff --git a/kokoro/linux.cfg b/kokoro/linux.cfg
old mode 100644
new mode 100755
index 6a5922f..9816a92
--- a/kokoro/linux.cfg
+++ b/kokoro/linux.cfg
@@ -2,6 +2,13 @@
 
 build_file: "git/toolchain/llvm_android/kokoro/llvm_build.sh"
 
+# These are artifacts we want to save once the build is done.
+action {
+  define_artifacts {
+    regex: "git/dist/*"
+  }
+}
+
 env_vars {
   key: "LLVM_BUILD_TYPE"
   value: "linux-master"
diff --git a/kokoro/llvm_build.sh b/kokoro/llvm_build.sh
index f405247..e0c7b69 100755
--- a/kokoro/llvm_build.sh
+++ b/kokoro/llvm_build.sh
@@ -26,16 +26,21 @@ trap cleanup EXIT
 mkdir -p "${DIST}"
 if [ $LLVM_BUILD_TYPE == "linux-TOT" ]; then
   OUT_DIR="${OUT}" DIST_DIR="${DIST}" $TOP/prebuilts/python/linux-x86/bin/python3 \
-  $python_src/build.py --build-llvm-next --create-tar \
-  --mlgo \
+  $python_src/build.py --build-llvm-next --mlgo --create-tar \
   --build-name "${KOKORO_BUILD_NUMBER}" \
-  --no-build=windows,lldb
+  --no-build=windows --sccache
 elif [ $LLVM_BUILD_TYPE == "linux-master" ]; then
   OUT_DIR="${OUT}" DIST_DIR="${DIST}" $TOP/prebuilts/python/linux-x86/bin/python3 \
-  $python_src/build.py --create-tar \
-  --mlgo \
+  $python_src/build.py --lto --pgo --bolt --mlgo --create-tar \
   --build-name "${KOKORO_BUILD_NUMBER}" \
-  --no-build=windows,lldb
+  --no-build=windows --sccache
+elif [ $LLVM_BUILD_TYPE == "darwin-master" ]; then
+  OUT_DIR="${OUT}" DIST_DIR="${DIST}" $TOP/prebuilts/python/darwin-x86/bin/python3 \
+  $python_src/build.py --lto --pgo --create-tar --build-name "${KOKORO_BUILD_NUMBER}"
+elif [ $LLVM_BUILD_TYPE == "windows-master" ]; then
+  OUT_DIR="${OUT}" DIST_DIR="${DIST}" $TOP/prebuilts/python/linux-x86/bin/python3 \
+  $python_src/build.py --mlgo --create-tar --build-name "${KOKORO_BUILD_NUMBER}" \
+  --no-build=linux --sccache
 else
   echo "Error: requires LLVM_BUILD_TYPE"
 fi
diff --git a/kokoro/windows.cfg b/kokoro/windows.cfg
new file mode 100755
index 0000000..38ba929
--- /dev/null
+++ b/kokoro/windows.cfg
@@ -0,0 +1,15 @@
+# Format: //devtools/kokoro/config/proto/build.proto
+
+build_file: "git/toolchain/llvm_android/kokoro/llvm_build.sh"
+
+# These are artifacts we want to save once the build is done.
+action {
+  define_artifacts {
+    regex: "git/dist/*"
+  }
+}
+
+env_vars {
+  key: "LLVM_BUILD_TYPE"
+  value: "windows-master"
+}
diff --git a/kythe_vnames.json b/kythe_vnames.json
old mode 100644
new mode 100755
diff --git a/orderfiles/README.md b/orderfiles/README.md
deleted file mode 100644
index 252d4fe..0000000
--- a/orderfiles/README.md
+++ /dev/null
@@ -1,150 +0,0 @@
-Android Order Files
-====================
-
-For the latest version of this doc, please make sure to visit:
-[Android Order Files](https://android.googlesource.com/toolchain/llvm_android/+/refs/heads/main/orderfiles/README.md)
-
-Getting started with Order files
-----------------------------------
-Order files are text files containing symbols representing functions names.
-Linker (lld) uses order files to layout functions in a specific order.
-These ordered binaries in Android will reduce page faults and improve a program's launch time due to the efficient loading of symbols during program’s cold-start.
-Order files have two stages: generating and loading.
-We provide the steps below for generating and loading. In addition, we have an example (dex2oat) that we used for experimenting.
-
-Generate Order files
-----------------------------------
-1. Add the orderfile property in its Android.bp to enable instrumentation. The `cflags` is used to pass the mapping flag and optimization flag (`-O1` or higher) to generate the mapping file. If no optimization flag is passed, it will not generate any mapping file. There is an example below:
-```
-orderfile: {
-    instrumentation: true,
-    load_order_file: false,
-    order_file_path: "",
-    cflags: [
-        "-O1",
-        "-mllvm",
-        "-orderfile-write-mapping=<filename>-mapping.txt",
-    ],
-}
-```
-
-2. Add `__llvm_profile_initialize_file` and `__llvm_orderfile_dump` functions and make sure they are called at the end of the code (End of `main` or `atexit`). You can also rename the output file using `__llvmprofile_set_filename` function.
-```
-extern "C" void __llvm_profile_initialize_file(void);
-extern "C" int __llvm_orderfile_dump(void);
-extern "C" void __llvm_profile_set_filename(const char *Name);
-
-int main () {
-    ...
-    __llvm_profile_set_filename("<filename>.output");
-    __llvm_profile_initialize_file();
-    __llvm_orderfile_dump();
-}
-```
-
-3. Run the binary on either a device or cuttlefish to create `<filename>.output.order` then pull it from it.
-
-4. Convert the profraw file into hexadecimal format
-```
-# Convert to hexadecimal format on Linux, Mac, and ChromeOS
-hexdump -C <filename>.output.order > <filename>.prof
-
-# Convert to hexadecimal format on Windows
-certutil -f -encodeHex <filename>.output.order <filename>.prof
-```
-
-5. Use [toolchain/llvm_android/orderfiles/scripts/create_orderfile.py](https://android.googlesource.com/toolchain/llvm_android/+/refs/heads/main/orderfiles/scripts/create_orderfile.py) to create an order file based on the profile and mapping files
-```
-python3 create-orderfile.py --profile-file <filename>.prof --mapping-file <filename>-mapping.txt --output <filename>.orderfile
-```
-
-6. (Optional) We also provide [toolchain/llvm_android/orderfiles/scripts/validate_orderfile.py](https://android.googlesource.com/toolchain/llvm_android/+/refs/heads/main/orderfiles/scripts/validate_orderfile.py) to validate the order file based on your criteria
-```
-python3 validate-orderfile.py --order-file <filename>.orderfile
-```
-
-Load Order file
-----------------------------------
-1. Make sure your order file (`<filename>.orderfile`) is saved in toolchain/pgo-profiles/orderfiles. We use this folder to find the order files in the build system.
-
-2. Just change the orderfile property in Android.bp and it will automatically load the order file to layout the symbols
-```
-orderfile: {
-    instrumentation: true,
-    load_order_file: true,
-    order_file_path: "<filename>.orderfile",
-}
-```
-
-Example (Dex2oat)
-----------------------------------
-Dex2oat is an ART application used to verify the byte-code of an Android application and used to create an optimized native binary. We ran the build steps on dex2oat and provided the exact steps.
-
-1. Create an orderfile property in [art/dex2oat/Android.bp](https://android.googlesource.com/platform/art/+/refs/heads/main/dex2oat/Android.bp). You should put it in the `art_cc_binary` for dex2oat. Since all of ART including dex2oat is already compiled with optimizations enabled e.g.,`-O2`, we do not need to pass an extra optimization flag to `cflags`.
-```
-orderfile: {
-    instrumentation: true,
-    load_order_file: false,
-    order_file_path: "",
-    cflags: [
-        # Profilings increase frame sizes and ART requires specific sizes.
-        # The below flag ignores the frame sizes defined.
-        "-Wno-frame-larger-than=",
-        # ART sometimes does a fast exit and we want to avoid it to get
-        # correct symbols so we use the below macro
-        "-DART_PGO_INSTRUMENTATION",
-        "-mllvm",
-        "-orderfile-write-mapping=dex2oat-mapping.txt",
-    ],
-}
-```
-
-2. Add the llvm order files functions to [art/dex2oat/dex2oat.cc](https://android.googlesource.com/platform/art/+/refs/heads/main/dex2oat/dex2oat.cc). We only want to build this for Android so we encapsulate the order file functions within the macro that is only defined for non-host builds.
-```
-#if defined(ART_PGO_INSTRUMENTATION)
-  extern "C" void __llvm_profile_initialize_file(void);
-  extern "C" int __llvm_orderfile_dump(void);
-  extern "C" void __llvm_profile_set_filename(const char *Name);
-#endif
-
-int main(int argc, char** argv) {
-  int result = static_cast<int>(art::Dex2oat(argc, argv));
-  if (!art::kIsDebugBuild && !art::kIsPGOInstrumentation && !art::kRunningOnMemoryTool) {
-    art::FastExit(result);
-  }
-
-  #if defined(ART_PGO_INSTRUMENTATION)
-    __llvm_profile_set_filename("dex2oat.output");
-    __llvm_profile_initialize_file();
-    __llvm_orderfile_dump();
-  #endif
-  return result;
-}
-```
-
-3. Build the ART test system by following the steps in [art/test/README.chroot.md](https://android.googlesource.com/platform/art/+/refs/heads/main/test/README.chroot.md). These steps are specifically for running ART tests on a device.
-
-4. Run the dex2oat specific tests on the device and pull the output file and convert it to hexadecimal format
-```
-adb pull /data/local/art-test-chroot/dex2oat.output.order .
-
-# Convert to hexadecimal format on Linux, Mac, and ChromeOS
-hexdump -C dex2oat.output.order > dex2oat.prof
-
-# Convert to hexadecimal format on Windows
-certutil -f -encodeHex dex2oat.output.order dex2oat.prof
-```
-
-5. Create the order file from both the profile file and mapping file
-```
-python3 create-orderfile.py --profile-file dex2oat.prof --mapping-file dex2oat-mapping.txt --output toolchain/pgo-profiles/orderfiles/dex2oat.orderfile
-```
-
-6. Change the order file property in [art/dex2oat/Android.bp](https://android.googlesource.com/platform/art/+/refs/heads/main/dex2oat/Android.bp).
-```
-orderfile: {
-    instrumentation: true,
-    load_order_file: true,
-    order_file_path: "dex2oat.orderfile",
-}
-```
\ No newline at end of file
diff --git a/orderfiles/scripts/README.md b/orderfiles/scripts/README.md
deleted file mode 100644
index cd48435..0000000
--- a/orderfiles/scripts/README.md
+++ /dev/null
@@ -1,127 +0,0 @@
-Android Order Files Scripts
-============================
-
-For the latest version of this doc, please make sure to visit:
-[Android Order Files Scripts](https://android.googlesource.com/toolchain/llvm_android/+/refs/heads/main/orderfiles/scripts/README.md)
-
-Getting started with Order files
-----------------------------------
-Order files are text files containing symbols representing functions names.
-Linker (lld) uses order files to layout functions in a specific order.
-These ordered binaries in Android will reduce page faults and improve a program's launch time due to the efficient loading of symbols during program’s cold-start.
-
-The scripts described here are used to create and validate order files. You can learn how and when they are used by looking at [Android Order Files](https://android.googlesource.com/toolchain/llvm_android/+/refs/heads/main/orderfiles/README.md).
-
-File/CSV Format
-----------------------------------
-Some arguments in the script allows three formats (File, CSV, or Folder) based on the first character.
-All formats represent a list of values, which is symbols or files in our case.
-- File format: The file will have one value per line.
-               Add @ before the filename to show it is a file.
-               If the values are files, the format is (file, weight).
-               Example: @example.txt
-- CSV format: Use “” (Quotation) around the comma-separated values.
-              Example: “main,foo,bar”
-- Folder format: Add ^ before the path to the folder.
-              We assume every file in the folder ends with ".orderfile".
-              Example: ^path/to/folder
-
-Orderfile scripts
-----------------------------------
-Following scripts are provided:
-- [create_orderfile](create_orderfile.py)
-- [validate_orderfile](validate_orderfile.py)
-- [merge_orderfile](merge_orderfile.py)
-
-In order to run the scripts you may need to install the following python3 dependencies:
-- bitarray
-- graphviz
-
-Create Order file
-----------------------------------
-You can create an orderfile from a mapping file and profile file.
-
-```
-python3 create_orderfile [-h] --profile-file PROFILE_FILE --mapping-file MAPPING_FILE [--output OUTPUT] [--denylist DENYLIST] [--last-symbol LAST_SYMBOL] [--leftover]
-```
-
-Flags:
-- Profile file (--profile-file):
-    - Description: The profile file generated by running a binary compiled with forder-file-instrumentation
-    - Type: String
-    - Required
-- Mapping file (--mapping-file):
-    - Description: The mapping file generated during compilation that maps MD5 hashes to symbol names
-    - Type: String
-    - Required
-- Output file (--output):
-    - Description: The output file name for the order file. Default Name: default.orderfile
-    - Type: String
-- Deny List (--denylist):
-    - Description: Symbols that you want to exclude from the order file
-    - Type: String (File/CSV)
-- Last symbol (--last-symbol):
-    - Description: The order file will end at the passed last symbol and ignore the symbols after it.
-                   If you want an order file only for startup, you should pass the last startup symbol.
-                   Last-symbol has priority over leftover so we will output until the last symbol and ignore the leftover flag.
-    - Type: String
-- Leftover symbols (--leftover):
-    - Description: Some symbols (functions) might not have executed so they will not appear in the profile file.
-                   If you want these symbols in your orderfile, you can use this flag and it will add them at the end.
-    - Type: Bool
-
-Validate Order file
-----------------------------------
-Once we get an order file for a library or binary, we need to check if it is valid based on each team’s criteria.
-To automate this process, we wrote a python script to check the criteria.
-The current criteria that we allow:
-- Defining an order priority that needs to be in the orderfile
-- Symbols that have to be present in orderfile
-- Symbols that should not be present in orderfile
-- Minimum number of symbols to make an orderfile good for page layout purposes
-
-```
-python3 validate_orderfile [-h] --order-file ORDER_FILE [--partial PARTIAL] [--allowlist ALLOWLIST] [--denylist DENYLIST] [--min MIN]
-```
-
-Flags:
-- Order file (--order-file):
-    - Description: The order file that is being validated on the below criteria
-    - Type: String
-    - Required
-- Partial Order (--partial):
-    - Description: A partial order of symbols that must be correct in the order file.
-    - Type: String (File/CSV)
-- Allow List (--allowlist):
-    - Description: Symbols that have to be present in orderfile
-    - Type: String (File/CSV)
-- Deny List (--denylist):
-    - Description: Symbols that should not be present in orderfile. Denylist flag has priority over allowlist.
-    - Type: String (File/CSV)
-- Minimum Number of Entries (--min):
-    - Description: Minimum number of symbols to make an orderfile good for page layout purposes
-    - Type: Int
-
-Merge Order File
-----------------------------------
-Any executable running on different devices might not create the same order file due to threads, OS, side effects, etc.
-As a result, our script will take all the order files and merge them into one order file while trying to maintain locality.
-As lower end device require better layout for performance boost, you can assign weights to order files and provide lower
-end device order files with higher weight. You can only assign weights if you use File format and an example can be found
-in test/merge-test/merge.txt.
-
-```
-python3 merge_orderfile [-h] --order-files ORDER_FILES [--output OUTPUT] [--graph-image GRAPH_IMAGE]
-```
-
-Flags:
-- Files (--order-files):
-    - Description: A collection of order files that need to be merged together
-    - Type: String (File/CSV/Folder)
-    - Required
-- Output (--output):
-    - Description: Provide the output file name for the order file. Default Name: default.orderfile
-    - Type: String
-- Graph Image (--graph-image):
-    - Description: Provide the output image name for the graph representation of the order files
-    - Type: String
diff --git a/orderfiles/scripts/create_orderfile.py b/orderfiles/scripts/create_orderfile.py
deleted file mode 100644
index cb9d2c7..0000000
--- a/orderfiles/scripts/create_orderfile.py
+++ /dev/null
@@ -1,128 +0,0 @@
-#!/usr/bin/env python3
-#
-# Copyright (C) 2023 The Android Open Source Project
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# Sample Usage:
-# $ python3 create_orderfile.py --profile-file ../orderfiles/test/example.prof --mapping-file ../orderfiles/test/example-mapping.txt
-#
-# Try '-h' for a full list of command line arguments.
-
-import argparse
-import orderfile_utils
-
-def parse_args():
-    """Parses and returns command line arguments."""
-    parser = argparse.ArgumentParser(prog="create_orderfile",
-                                    description="Create orderfile from profile file and mapping file")
-
-    parser.add_argument(
-        "--profile-file",
-        required=True,
-        help="Parsed profile file that represents the order of the symbol execution")
-
-    parser.add_argument(
-        "--mapping-file",
-        required=True,
-        help="Mapped file that provides the mapping between MD5 hash and symbol name")
-
-    parser.add_argument(
-        "--output",
-        default="default.orderfile",
-        help="Provide the output file name for the order file. Default Name: default.orderfile")
-
-    parser.add_argument(
-        "--denylist",
-        default="",
-        help=f"Exclude symbols based on a symbol-per-line file with @ or comma separarted values within a quotation."
-             f"For example, you can say @file.txt or 'main,bar,foo'")
-
-    parser.add_argument(
-        "--last-symbol",
-        help=f"Create an order file until the passed last symbol and ignore the symbols after it."
-             f"Useful if you want an order file only for startup so you should pass the last startup symbol."
-             f"Last-symbol has priority over leftover so we will output until the last symbol and ignore the leftover flag.")
-
-    parser.add_argument(
-        "--leftover",
-        action='store_true',
-        default=False,
-        help="Add the symbols seen in mapping file but not in profile file at the end")
-
-    return parser.parse_args()
-
-def main():
-    args = parse_args()
-
-    symbols = []
-    mapping = {}
-    seen = set()
-    denylist = orderfile_utils.parse_set(args.denylist)
-
-    # Load the MD5 hash mappings of the symbols.
-    with open(args.mapping_file, "r") as f:
-        for line in f:
-            line = line.strip().split()
-            mapping[line[1]] = line[2]
-
-    # Parse the profile file
-    with open(args.profile_file, "r") as f:
-        for line in f:
-            line = line.strip().split()
-
-            # Every line should have 2 MD5 hashes in reverse order (little Endian)
-            # so we need to reverse them to get the actual md5 hashes
-            if len(line) >= 8:
-                md5_1_b_list = line[1:9]
-                md5_2_b_list = line[9:17]
-
-                md5_1_b_list.reverse()
-                md5_2_b_list.reverse()
-
-                md5_1 = "".join(md5_1_b_list)
-                md5_2 = "".join(md5_2_b_list)
-
-                if(md5_1 in mapping):
-                    symbol_1 = mapping[md5_1]
-                    seen.add(symbol_1)
-
-                    if symbol_1 not in denylist:
-                        symbols.append(symbol_1)
-
-                if(md5_2 in mapping):
-                    symbol_2 = mapping[md5_2]
-                    seen.add(symbol_2)
-
-                    if symbol_2 not in denylist:
-                        symbols.append(symbol_2)
-
-    # Functions in the mapping but not seen in the partial order.
-    # If you want to add them, you can use the leftover flag.
-    # Note: You can only use the leftover flag if the last-symbol flag was not passed
-    if args.leftover and args.last_symbol != None:
-        for md5 in mapping:
-            if mapping[md5] not in seen:
-                symbols.append(mapping[md5])
-
-    # Write it to output file
-    with open(args.output, "w") as f:
-        for symbol in symbols:
-            f.write(symbol+"\n")
-
-            # If we are at the last-symbol, we do not write the rest of the symbols
-            if symbol == args.last_symbol:
-                break
-
-if __name__ == '__main__':
-    main()
diff --git a/orderfiles/scripts/merge_orderfile.py b/orderfiles/scripts/merge_orderfile.py
deleted file mode 100644
index 5794480..0000000
--- a/orderfiles/scripts/merge_orderfile.py
+++ /dev/null
@@ -1,408 +0,0 @@
-#!/usr/bin/env python3
-#
-# Copyright (C) 2023 The Android Open Source Project
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# Sample Usage:
-# $ python3 merge_orderfile.py --order-files %../orderfiles/test
-#
-# Try '-h' for a full list of command line arguments.
-#
-# Note: We allow three formats: Folder, File, and CSV
-# As lower end devices require the most help, you can give
-# their order files a higher weight.
-# You can only provide weights if you choose File format.
-# For example, an order file weight of 4 means the edges
-# in the graph will be multiplied by 4.
-# CSV and Folder assume all files have a weight of 1.
-# An example file can be found at ../test/merge-test/merge.txt
-
-from bitarray import bitarray
-import argparse
-import graphviz
-
-import orderfile_utils
-
-class Vertex(object):
-    """Vertex (symbol) in the graph."""
-    def __init__(self, name: str) -> None:
-        self.name = name
-        self.count = 0
-
-    def __eq__(self, other: object) -> bool:
-        if isinstance(other, Vertex):
-            return self.name == other.name
-        return False
-
-    def __hash__(self) -> int:
-        return hash(self.name)
-
-    def __str__(self) -> str:
-        return f'{self.name}({self.count})'
-
-    def appears(self) -> None:
-        self.count += 1
-
-class Graph(object):
-    """Graph representation of the order files."""
-    def __init__(self) -> None:
-        self.graph = {}
-        self.reverse = {}
-        self.vertices = {}
-
-    def __str__(self) -> str:
-        string = ""
-        for (f_symbol, value) in self.graph.items():
-            for (t_symbol, weight) in self.graph[f_symbol].items():
-                string += f'{f_symbol} --{weight}--> {t_symbol}\n'
-        return string
-
-    def addVertex(self, symbol: str) -> None:
-        if symbol not in self.vertices:
-            v = Vertex(symbol)
-            self.vertices[symbol] = v
-            self.graph[v] = {}
-            self.reverse[v] = {}
-
-        self.vertices[symbol].appears()
-
-    def addEdge(self, from_symbol: str, to_symbol: str) -> None:
-        """Add an edge (it represents two symbols are consecutive)."""
-        from_vertex = self.vertices.get(from_symbol)
-        to_vertex = self.vertices.get(to_symbol)
-
-        if from_vertex is None:
-            raise RuntimeError(f"Symbol {from_symbol} is not in graph")
-
-        if to_vertex is None:
-            raise RuntimeError(f"Symbol {to_symbol} is not in graph")
-
-        if to_vertex not in self.graph[from_vertex]:
-            self.graph[from_vertex][to_vertex] = 0
-            self.reverse[to_vertex][from_vertex] = 0
-
-        self.graph[from_vertex][to_vertex] += 1
-        self.reverse[to_vertex][from_vertex] += 1
-
-    def removeEdgeCompletely(self, from_symbol: str, to_symbol: str) -> None:
-        """Remove the edge from the graph"""
-        from_vertex = self.vertices.get(from_symbol)
-        to_vertex =  self.vertices.get(to_symbol)
-
-        if from_vertex is None:
-            raise RuntimeError(f"Symbol {from_symbol} is not in graph")
-
-        if to_vertex is None:
-            raise RuntimeError(f"Symbol {to_symbol} is not in graph")
-
-        del self.graph[from_vertex][to_vertex]
-        del self.reverse[to_vertex][from_vertex]
-
-        to_vertex.count -= 1
-
-    def checkVertex(self, symbol: str) -> bool:
-        return symbol in self.vertices
-
-    def checkEdge(self, from_symbol: str, to_symbol: str) -> bool:
-        if not self.checkVertex(from_symbol):
-            return False
-
-        if not self.checkVertex(to_symbol):
-            return False
-
-        from_vertex = self.vertices.get(from_symbol)
-        to_vertex =  self.vertices.get(to_symbol)
-
-        if from_vertex not in self.graph:
-            return False
-
-        return to_vertex in self.graph[from_vertex]
-
-    def checkEdgeWeight(self, from_symbol: str, to_symbol: str, weight: str) -> bool:
-        if not self.checkEdge(from_symbol, to_symbol):
-            return False
-
-        from_vertex = self.vertices.get(from_symbol)
-        to_vertex =  self.vertices.get(to_symbol)
-
-        return self.graph[from_vertex][to_vertex] == weight
-
-    def getOutEdges(self, symbol: str):
-        """Graph the out edges for a vertex."""
-        out_edges = []
-        vertex = self.vertices.get(symbol)
-        if vertex is None:
-            raise RuntimeError(f"Symbol {symbol} is not in graph")
-
-        for (key, value) in self.graph[vertex].items():
-            out_edges.append((key, value))
-
-        return out_edges
-
-    def getInEdges(self, symbol: str):
-        """Graph the in edges for a vertex."""
-        in_edges = []
-        vertex = self.vertices.get(symbol)
-        if vertex is None:
-            raise RuntimeError(f"Symbol {symbol} is not in graph")
-
-        for (key, value) in self.reverse[vertex].items():
-            in_edges.append((key, value))
-
-        return in_edges
-
-    def getRoots(self, reverse=False) -> list[str]:
-        """Get the roots of the graph (Vertex with no in edges)."""
-        roots = []
-        for (symbol,_) in self.vertices.items():
-            if not reverse:
-                if len(self.getInEdges(symbol)) == 0:
-                    roots.append(symbol)
-            else:
-                # If you want the reverse (vertex with no out edges)
-                if len(self.getOutEdges(symbol)) == 0:
-                    roots.append(symbol)
-
-        return roots
-
-    def __cyclesUtil(self, vertex: Vertex) -> None:
-        self.visited.add(vertex)
-        self.curr_search.append(vertex)
-
-        for (out_vertex, _) in self.graph[vertex].items():
-            # If vertex already appeared in current depth search, we have a backedge
-            if out_vertex in self.curr_search:
-                # We save save all vertices in the cycle because an edge from the cycle will be removed
-                index = self.curr_search.index(out_vertex)
-                temp_lst = self.curr_search[index:]
-                self.cycles.append(temp_lst)
-            # If vertex visited before in a previous search, we do not need to search from it again
-            elif out_vertex not in self.visited:
-                self.__cyclesUtil(out_vertex)
-
-        self.curr_search.pop()
-
-    def getCycles(self) -> list[list[tuple[str]]]:
-        self.visited = set()
-        self.curr_search = []
-        self.cycles = []
-        lst = []
-
-        for (_, vertex) in self.vertices.items():
-            if vertex not in self.visited:
-                self.__cyclesUtil(vertex)
-
-        return self.cycles
-
-    # Get immediate dominator for each vertex
-    def getDominators(self, post=False):
-        # Create a bitarray for each vertex to showcase which vertices
-        # are dominators
-        num_vertices = len(self.vertices)
-        dominators = {}
-        mapping = []
-        for (_, vertex) in self.vertices.items():
-            mapping.append(vertex)
-            ba = bitarray(num_vertices)
-            ba.setall(True)
-            dominators[vertex] = ba
-
-        # Add the root vertices
-        stack = []
-        roots = self.getRoots(post)
-        for root in roots:
-            stack.append((None, self.vertices[root]))
-
-        while len(stack) != 0:
-            (parent, child) = stack.pop()
-
-            # If no parent, you have no dominators from above
-            # If you have a parent, your dominations is the common dominators
-            # between all parents
-            if parent is None:
-                dominators[child].setall(False)
-            else:
-                dominators[child] &= dominators[parent]
-
-            # You are dominator of yourself
-            index = mapping.index(child)
-            dominators[child][index] = True
-            if not post:
-                for (out_vertex,_) in self.graph[child].items():
-                    stack.append((child, out_vertex))
-            else:
-                for (out_vertex,_) in self.reverse[child].items():
-                    stack.append((child, out_vertex))
-
-        for (vertex, ba) in dominators.items():
-            # If no Trues in bitarray, you have no immediate dominator
-            # because you are a root vertex. Else, you can find the
-            # most left True vertex excluding yourself
-            index = mapping.index(vertex)
-            ba[index] = False
-            if True not in ba:
-                dominators[vertex] = None
-            else:
-                # Due to reverse, this is the actual index in the initial bitarray
-                dominator_index = ba.index(True)
-                dominators[vertex] = mapping[dominator_index]
-
-        return dominators
-
-    def __printOrderUtil(self, vertex):
-        # If already visit, we do not need to get order
-        if vertex in self.visited:
-            return
-
-        self.order.append(vertex)
-        self.visited.add(vertex)
-
-        # Get out edges and sort them based on their weightage
-        out_edges = self.getOutEdges(vertex.name)
-        out_edges.sort(key = lambda x: x[1], reverse=True)
-
-        # We continue dfs based on the largest weight
-        for (out, _) in out_edges:
-            self.__printOrderUtil(out)
-
-    def printOrder(self, output):
-        self.order = []
-        self.visited = set()
-        stack = []
-
-        # Create an order using DFS from the root
-        for root in self.getRoots():
-            self.__printOrderUtil(self.vertices[root])
-
-        # Write the order to a file
-        with open(output, "w") as f:
-            for vertex in self.order:
-                f.write(f"{vertex.name}\n")
-
-    def exportGraph(self, output: str) -> None:
-        """Export graph as a dot file and pdf file."""
-        dot = graphviz.Digraph(comment='Graph Representation of Orderfile')
-
-        for (from_vertex, to_vertices) in self.graph.items():
-            for (to_vertex, weight) in to_vertices.items():
-                dot.edge(from_vertex.__str__(), to_vertex.__str__(), label=str(weight))
-
-        dot.render(filename=output)
-
-
-def parse_args() -> argparse.Namespace:
-    """Parses and returns command line arguments."""
-
-    parser = argparse.ArgumentParser(prog="merge_orderfile",
-                                    description="Merge Order Files")
-
-    parser.add_argument(
-        "--order-files",
-        required=True,
-        help="A collection of order files that need to be merged together."
-             "Format: A file-per-line file with @, a folder with ^, or comma separated values within a quotation."
-             "For example, you can say @file.txt, ^path/to/folder or '1.orderfile,2.orderfile'.")
-
-    parser.add_argument(
-        "--output",
-        default="default.orderfile",
-        help="Provide the output file name for the order file. Default Name: default.orderfile")
-
-    parser.add_argument(
-        "--graph-image",
-        help="Provide the output image name for the graph representation of the order files.")
-
-    return parser.parse_args()
-
-def removeCycles(graph: Graph) -> None:
-    # Remove cycles created by combining order files
-    for cycleList in graph.getCycles():
-        # Get the sum of in edge weights for all vertices in the cycle
-        # We exclude the cycle edges from the calculation
-        # For example, cycle = [a,b,c] where cycle_edges=[a->b, b->c, c->a]
-        # in_edges(a) = [main, c]
-        # in_edges(b) = [a]
-        # in_edges(c) = [b]
-        #
-        # Excluding cycle edges:
-        # in_edges(a) = [main] = 1
-        # in_edges(b) = [] = 0
-        # in_edges(c) = [] = 0
-        inner_edges = [graph.getInEdges(vertex.name) for vertex in cycleList]
-        inner_weights = []
-        for inner_edge in inner_edges:
-            total = 0
-            for edge in inner_edge:
-                if edge[0] not in cycleList:
-                    total += edge[1]
-            inner_weights.append(total)
-
-        # We remove the cycle edge that leads to the highest sum of in-edges for a vertex
-        # because the vertex has other options for ordering.
-        # In the above example, we remove c->a
-        max_inner_weight = max(inner_weights)
-        index = inner_weights.index(max_inner_weight)
-        prev = index - 1
-        if prev < 0:
-            prev = len(inner_weights) - 1
-        to_vertex = cycleList[index]
-        from_vertex = cycleList[prev]
-
-        graph.removeEdgeCompletely(from_vertex.name, to_vertex.name)
-
-def addSymbolsToGraph(graph: Graph, order: list[str], weight: int = 1) -> None:
-    prev_symbol = None
-    for symbol in order:
-        graph.addVertex(symbol)
-
-        if prev_symbol is not None:
-            for i in range(weight):
-                graph.addEdge(prev_symbol, symbol)
-
-        prev_symbol = symbol
-
-def createGraph(files: list[str]) -> Graph:
-    graph = Graph()
-
-    # Create graph representation based on combining the order files
-    for (orderfile, weight) in files:
-        with open(orderfile, "r", encoding="utf-8") as f:
-            lst = []
-            for line in f:
-                line = line.strip()
-                lst.append(line)
-
-            addSymbolsToGraph(graph, lst, weight)
-
-    return graph
-
-def main() -> None:
-    args = parse_args()
-
-    files = orderfile_utils.parse_merge_list(args.order_files)
-    graph = createGraph(files)
-
-    # Assert no cycles after removing them
-    removeCycles(graph)
-    assert(len(graph.getCycles()) == 0)
-
-    # Create an image of the graph representation
-    if args.graph_image:
-        graph.exportGraph(args.graph_image)
-
-    # Create order file from the graph structure
-    graph.printOrder(args.output)
-
-if __name__ == '__main__':
-    main()
diff --git a/orderfiles/scripts/orderfile_unittest.py b/orderfiles/scripts/orderfile_unittest.py
deleted file mode 100644
index 5fdeba6..0000000
--- a/orderfiles/scripts/orderfile_unittest.py
+++ /dev/null
@@ -1,763 +0,0 @@
-#!/usr/bin/env python3
-#
-# Copyright (C) 2023 The Android Open Source Project
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# Sample Usage:
-# $ python3 -m unittest orderfile_unittest.py
-#
-# For more verbose test information:
-# $ python3 -m unittest -v orderfile_unittest.py
-
-import os
-import unittest
-import subprocess
-
-import merge_orderfile
-import orderfile_utils as utils
-
-class TestCreateOrderfile(unittest.TestCase):
-
-    def setUp(self):
-        top = utils.android_build_top()
-        THIS_DIR = os.getcwd()
-        self.create_script = top+"/toolchain/llvm_android/orderfiles/scripts/create_orderfile.py"
-        self.validate_script = top+"/toolchain/llvm_android/orderfiles/scripts/validate_orderfile.py"
-        self.profile_file = top+"/toolchain/llvm_android/orderfiles/test/example.prof"
-        self.mapping_file = top+"/toolchain/llvm_android/orderfiles/test/example-mapping.txt"
-        self.order_file = top+"/toolchain/llvm_android/orderfiles/test/example.orderfile"
-        self.denylist_file = top+"/toolchain/llvm_android/orderfiles/test/denylist.txt"
-        self.output_file = THIS_DIR+"/default.orderfile"
-        self.temp_file = THIS_DIR+"/temp.orderfile"
-
-    # Test if the script creates an orderfile
-    def test_create_orderfile_normal(self):
-        utils.check_call(["python3", self.create_script,
-                            "--profile-file", self.profile_file,
-                            "--mapping-file", self.mapping_file])
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        # Clean up at the end
-        os.remove(self.output_file)
-
-    # Test if no mapping/profile file isn't passed then the script errors
-    def test_create_orderfile_missing_mapping_argument(self):
-        with self.assertRaises(subprocess.CalledProcessError) as context:
-            utils.check_error(["python3", self.create_script,
-                                "--profile-file", self.profile_file])
-
-        # Check error output that flag is required
-        last_line = context.exception.output.split("\n")[-2]
-        self.assertEqual(last_line,
-                        "create_orderfile: error: the following arguments are required: --mapping-file")
-
-    # Test if the script creates an orderfile named temp.orderfile not default.orderfile
-    def test_create_orderfile_output_name(self):
-        utils.check_call(["python3", self.create_script,
-                            "--profile-file", self.profile_file,
-                            "--mapping-file", self.mapping_file,
-                            "--output", "temp.orderfile"])
-        self.assertTrue(os.path.isfile(self.temp_file))
-        self.assertFalse(os.path.isfile(self.output_file))
-
-        # Clean up at the end
-        os.remove(self.temp_file)
-
-    # Test if the script creates an orderfile by adding the leftover mapping symbols at the end of the orderfile
-    def test_create_orderfile_leftover(self):
-        utils.check_call(["python3", self.create_script,
-                            "--profile-file", self.profile_file,
-                            "--mapping-file", self.mapping_file])
-        utils.check_call(["python3", self.create_script,
-                            "--profile-file", self.profile_file,
-                            "--mapping-file", self.mapping_file,
-                            "--leftover",
-                            "--output", "temp.orderfile"])
-        self.assertTrue(os.path.isfile(self.temp_file))
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        first  = []
-        second = []
-        with open(self.output_file, "r") as f:
-            for line in f:
-                first.append(line.strip())
-
-        with open(self.temp_file, "r") as f:
-            for line in f:
-                second.append(line.strip())
-
-        # Leftover flag will make the second orderfile either have the same
-        # number of symbols or more than the first orderfile
-        self.assertGreaterEqual(len(second), len(first))
-
-        # Both orderfiles should have the same first few symbols
-        for i in range(len(first)):
-            self.assertEqual(first[i], second[i])
-
-        # Clean up at the end
-        os.remove(self.temp_file)
-        os.remove(self.output_file)
-
-    # Test if the script creates an orderfile without part based on both formats
-    def test_create_orderfile_denylist(self):
-        # Test with CSV format
-        utils.check_call(["python3", self.create_script,
-                            "--profile-file", self.profile_file,
-                            "--mapping-file", self.mapping_file,
-                            "--denylist", "_Z4partPiii"])
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        with open(self.output_file, "r") as f:
-            for line in f:
-                line = line.strip()
-                self.assertNotEqual(line, "_Z4partPiii")
-
-        # Clean up at the end
-        os.remove(self.output_file)
-
-        # Test with file format
-        utils.check_call(["python3", self.create_script,
-                            "--profile-file", self.profile_file,
-                            "--mapping-file", self.mapping_file,
-                            "--denylist", "@"+self.denylist_file])
-
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        with open(self.output_file, "r") as f:
-            for line in f:
-                line = line.strip()
-                self.assertNotEqual(line, "_Z4partPiii")
-
-        # Clean up at the end
-        os.remove(self.output_file)
-
-    # Test if the script creates an orderfile until the last symbol
-    def test_create_orderfile_last_symbol(self):
-        # Test an example where main is the last symbol
-        utils.check_call(["python3", self.create_script,
-                            "--profile-file", self.profile_file,
-                            "--mapping-file", self.mapping_file,
-                            "--last-symbol", "main"])
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        # Only main symbols should be in the file
-        output = utils.check_output(["python3", self.validate_script,
-                                    "--order-file", self.output_file,
-                                    "--allowlist", "_GLOBAL__sub_I_main.cpp,main",
-                                    "--denylist", "_Z5mergePiiii,_Z9mergeSortPiii,_Z4partPiii,_Z9quickSortPiii"])
-        self.assertTrue(output, "Order file is valid")
-
-        # Clean up at the end
-        os.remove(self.output_file)
-
-        # Test last-symbol has higher priority over leftover
-        utils.check_call(["python3", self.create_script,
-                            "--profile-file", self.profile_file,
-                            "--mapping-file", self.mapping_file,
-                            "--last-symbol", "main",
-                            "--leftover"])
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        # Only main symbols should be in the file because leftover was ignored
-        output = utils.check_output(["python3", self.validate_script,
-                                    "--order-file", self.output_file,
-                                    "--allowlist", "_GLOBAL__sub_I_main.cpp,main",
-                                    "--denylist", "_Z5mergePiiii,_Z9mergeSortPiii,_Z4partPiii,_Z9quickSortPiii"])
-        self.assertTrue(output, "Order file is valid")
-
-        # Clean up at the end
-        os.remove(self.output_file)
-
-class TestValidateOrderfile(unittest.TestCase):
-
-    def setUp(self):
-        top = utils.android_build_top()
-        THIS_DIR = os.getcwd()
-        self.validate_script = top+"/toolchain/llvm_android/orderfiles/scripts/validate_orderfile.py"
-        self.order_file = top+"/toolchain/llvm_android/orderfiles/test/example.orderfile"
-        self.denylist_file = top+"/toolchain/llvm_android/orderfiles/test/denylist.txt"
-        self.partial_file = top+"/toolchain/llvm_android/orderfiles/test/partial.txt"
-        self.partialb_file = top+"/toolchain/llvm_android/orderfiles/test/partial_bad.txt"
-        self.allowlistv_file = top+"/toolchain/llvm_android/orderfiles/test/allowlistv.txt"
-        self.output_file = THIS_DIR+"/default.orderfile"
-
-    # Test the validate script works correctly
-    def test_validate_orderfile_normal(self):
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.order_file])
-        self.assertTrue(output, "Order file is valid")
-
-    # Test errors in vaidate script like bad type mismatch or no orderfile passed
-    def test_validate_orderfile_argument_errors(self):
-        with self.assertRaises(subprocess.CalledProcessError) as context:
-            utils.check_error(["python3", self.validate_script])
-
-        # Check error output that flag is required
-        last_line = context.exception.output.split("\n")[-2]
-        self.assertEqual(last_line,
-                        "validate_orderfile: error: the following arguments are required: --order-file")
-
-    # Test if the validate script checks partial order based on both formats
-    def test_validate_orderfile_partial_flag(self):
-        # Test a correct partial order in CSV format
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.order_file,
-                                        "--partial", "_Z9mergeSortPiii,_Z5mergePiiii"])
-
-        self.assertTrue(output, "Order file is valid")
-
-        # Test a correct partial order in file format
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.order_file,
-                                        "--partial", "@"+self.partial_file])
-        self.assertTrue(output, "Order file is valid")
-
-        # Test a partial order with only one symbol (We allow this case)
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.order_file,
-                                        "--partial", "_Z9mergeSortPiii"])
-        self.assertTrue(output, "Order file is valid")
-
-        # Test a partial order with one symbol not in orderfile
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.order_file,
-                                        "--partial", "_Z9mergeSortPiii,temp"])
-        self.assertTrue(output, "Order file is valid")
-
-        # Test a bad partial order in CSV format
-        with self.assertRaises(subprocess.CalledProcessError) as context:
-            utils.check_error(["python3", self.validate_script,
-                                "--order-file", self.order_file,
-                                "--partial", "_Z5mergePiiii,_Z9mergeSortPiii"])
-
-        # Check the last non-empty to see if gives a RuntimeError
-        # and has a message saying _Z5mergePiiii must be before _Z9mergeSortPiii in orderfile
-        last_line = context.exception.output.split("\n")[-2]
-        self.assertEqual(last_line,
-                        "RuntimeError: `_Z5mergePiiii` must be before `_Z9mergeSortPiii` in orderfile")
-
-        # Test a bad partial order in file format
-        with self.assertRaises(subprocess.CalledProcessError) as context:
-            utils.check_error(["python3", self.validate_script,
-                                "--order-file", self.order_file,
-                                "--partial", "@"+self.partialb_file])
-
-        # Check the last non-empty to see if gives a RuntimeError
-        # and has a message saying _Z5mergePiiii must be before _Z9mergeSortPiii in orderfile
-        last_line = context.exception.output.split("\n")[-2]
-        self.assertEqual(last_line,
-                        "RuntimeError: `_Z5mergePiiii` must be before `_Z9mergeSortPiii` in orderfile")
-
-    # Test if the validate script checks if symbols are present in orderfile based on both format
-    def test_validate_orderfile_allowlist_flag(self):
-        # Test a correct allowlist in CSV format
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.order_file,
-                                        "--allowlist", "main"])
-        self.assertTrue(output, "Order file is valid")
-
-        # Test a correct allowlist in file format
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.order_file,
-                                        "--allowlist", "@"+self.allowlistv_file])
-        self.assertTrue(output, "Order file is valid")
-
-        # Test a bad allowlist in CSV format
-        with self.assertRaises(subprocess.CalledProcessError) as context:
-            utils.check_error(["python3", self.validate_script,
-                                "--order-file", self.order_file,
-                                "--allowlist", "_Z4partPiii"])
-
-        # Check the last non-empty to see if gives a RuntimeError
-        # and has a message saying symbols in allow-list are not in orderfile
-        last_line = context.exception.output.split("\n")[-2]
-        self.assertEqual(last_line,
-                        "RuntimeError: Some symbols in allow-list are not in the orderfile")
-
-        # Test a bad allowlist in file format
-        with self.assertRaises(subprocess.CalledProcessError) as context:
-            utils.check_error(["python3", self.validate_script,
-                                "--order-file", self.order_file,
-                                "--allowlist", "@"+self.denylist_file])
-
-        # Check the last non-empty to see if gives a RuntimeError
-        # and has a message saying symbols in allow-list are not in orderfile
-        last_line = context.exception.output.split("\n")[-2]
-        self.assertEqual(last_line,
-                        "RuntimeError: Some symbols in allow-list are not in the orderfile")
-
-    # Test if the validate script checks if symbols are not present in orderfile based on both format
-    def test_validate_orderfile_denylist_flag(self):
-        # Test a correct denylist in CSV format
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.order_file,
-                                        "--denylist", "_Z4partPiii"])
-        self.assertTrue(output, "Order file is valid")
-
-        # Test a correct denylist in file format
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.order_file,
-                                        "--denylist", "@"+self.denylist_file])
-        self.assertTrue(output, "Order file is valid")
-
-        # Test a bad denylist in CSV format
-        with self.assertRaises(subprocess.CalledProcessError) as context:
-            utils.check_error(["python3", self.validate_script,
-                                "--order-file", self.order_file,
-                                "--denylist", "main"])
-
-        # Check the last non-empty to see if gives a RuntimeError
-        # and has a message saying "main" should not be in orderfile
-        last_line = context.exception.output.split("\n")[-2]
-        self.assertEqual(last_line,
-                        "RuntimeError: Orderfile should not contain main")
-
-        # Test a bad denylist in file format
-        with self.assertRaises(subprocess.CalledProcessError) as context:
-            utils.check_error(["python3", self.validate_script,
-                                "--order-file", self.order_file,
-                                "--denylist", "@"+self.allowlistv_file])
-
-        # Check the last non-empty to see if gives a RuntimeError
-        # and has a message saying "main" should not be in orderfile
-        last_line = context.exception.output.split("\n")[-2]
-        self.assertEqual(last_line,
-                        "RuntimeError: Orderfile should not contain main")
-
-    # Test if the validate script checks if there are a minimum number of symbols
-    def test_validate_orderfile_min_flag(self):
-        # Test a correct minimum number of symbols
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.order_file,
-                                        "--min", "3"])
-        self.assertTrue(output, "Order file is valid")
-
-        # Test a bad minimum number of symbols
-        with self.assertRaises(subprocess.CalledProcessError) as context:
-            utils.check_error(["python3", self.validate_script,
-                                "--order-file", self.order_file,
-                                "--min", "10"])
-
-        # Check the last non-empty to see if gives a RuntimeError
-        # and has a message saying it needs at least 10 symbols
-        last_line = context.exception.output.split("\n")[-2]
-        self.assertEqual(last_line,
-                        "RuntimeError: The orderfile has 5 symbols but it needs at least 10 symbols")
-
-        # Test a bad minimum number of symbols
-        with self.assertRaises(subprocess.CalledProcessError) as context:
-            utils.check_error(["python3", self.validate_script,
-                                "--order-file", self.order_file,
-                                "--min", "three"])
-
-        # Check error output that flag has invalid type
-        last_line = context.exception.output.split("\n")[-2]
-        self.assertEqual(last_line,
-                        "validate_orderfile: error: argument --min: invalid int value: 'three'")
-
-    # Test if the validate script gives priority to denylist flag over other flags
-    def test_validate_orderfile_denylist_priority(self):
-        # Test the denylist has more priority over allowlist and should not give error
-        # here because the symbol is not in the orderfile
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.order_file,
-                                        "--allowlist", "_Z4partPiii",
-                                        "--denylist", "_Z4partPiii"])
-        self.assertTrue(output, "Order file is valid")
-
-        # Test the denylist has more priority over allowlist and should give error
-        # here because the symbol is in the orderfile
-        with self.assertRaises(subprocess.CalledProcessError) as context:
-            utils.check_error(["python3", self.validate_script,
-                                "--order-file", self.order_file,
-                                "--allowlist", "_Z5mergePiiii",
-                                "--denylist", "_Z5mergePiiii"])
-
-        # Check the last non-empty to see if gives a RuntimeError
-        # and has a message saying _Z5mergePiiii should not be in orderfile
-        last_line = context.exception.output.split("\n")[-2]
-        self.assertEqual(last_line,
-                        "RuntimeError: Orderfile should not contain _Z5mergePiiii")
-
-class TestMergeOrderfile(unittest.TestCase):
-
-    def setUp(self):
-        top = utils.android_build_top()
-        THIS_DIR = os.path.realpath(os.path.dirname(__file__))
-        self.validate_script = top+"/toolchain/llvm_android/orderfiles//scripts/validate_orderfile.py"
-        self.merge_script = top+"/toolchain/llvm_android/orderfiles/scripts/merge_orderfile.py"
-        self.output_file = THIS_DIR+"/merged-normal.orderfile"
-        self.merge_test_folder = top+"/toolchain/llvm_android/orderfiles/test/merge-test"
-        self.file = top+"/toolchain/llvm_android/orderfiles/test/merge-test/merge.txt"
-
-    # Test if the order files are merged correctly
-    def test_merge_orderfile_normal(self):
-        # Test a folder input
-        utils.check_call(["python3", self.merge_script,
-                            "--order-files", f"^{self.merge_test_folder}",
-                            f"--output={self.output_file}"])
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.output_file,
-                                        "--partial", "main,b,c,d,a,e,f"])
-        self.assertTrue(output, "Order file is valid")
-
-        # Clean up at the end
-        os.remove(self.output_file)
-
-        # Test the file format with different weights
-        utils.check_call(["python3", self.merge_script,
-                            "--order-files", f"@{self.file}",
-                            f"--output={self.output_file}"])
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.output_file,
-                                        "--partial", "main,b,c,d,e,f,a"])
-        self.assertTrue(output, "Order file is valid")
-
-        # Clean up at the end
-        os.remove(self.output_file)
-
-        # Test with CSV format
-        lst = ["1.orderfile", "2.orderfile"]
-        lst = [self.merge_test_folder + "/" + orderfile for orderfile in lst]
-        param = ",".join(lst)
-        utils.check_call(["python3", self.merge_script,
-                            "--order-files", param,
-                            f"--output={self.output_file}"])
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.output_file,
-                                        "--partial", "main,b,c,d,e,f"])
-        self.assertTrue(output, "Order file is valid")
-
-        # Clean up at the end
-        os.remove(self.output_file)
-
-    # Test if the simple graph functions work correctly
-    def test_graph_simple_function(self):
-        graph = merge_orderfile.Graph()
-
-        # Add 0-9 to the graph
-        prev = None
-        for i in range(10):
-            name = str(i)
-
-            graph.addVertex(name)
-            self.assertTrue(graph.checkVertex(name))
-
-            if prev != None:
-                graph.addEdge(prev, name)
-                self.assertTrue(graph.checkEdgeWeight(prev, name, 1))
-
-            prev = name
-
-        # Numbers were never added so they should be no edge to or from them
-        # and they should not be a vertex
-        self.assertFalse(graph.checkVertex("100"))
-        self.assertFalse(graph.checkEdge("0", "10"))
-        self.assertFalse(graph.checkEdge("100", "10"))
-        self.assertFalse(graph.checkEdge("0", "100"))
-
-        # As we add an edge for two vertices, the edge weight increases
-        graph.addVertex("11")
-        graph.addVertex("12")
-        self.assertTrue(graph.checkVertex("11"))
-        self.assertTrue(graph.checkVertex("12"))
-        for i in range(5):
-            graph.addEdge("11","12")
-            self.assertTrue(graph.checkEdgeWeight("11","12",i+1))
-
-        # Edge got deleted so it should not be in the graph anymore
-        graph.removeEdgeCompletely("11", "12")
-        self.assertFalse(graph.checkEdge("11", "12"))
-
-        # Check if you add many successors for a vertex then getOutEdges
-        # will return the correct number of successor and has the edge weight
-        for i in range(20, 26):
-            graph.addVertex(str(i))
-            self.assertTrue(graph.checkVertex(str(i)))
-
-        for j in range(21, 26):
-            graph.addEdge("20", str(j))
-            self.assertTrue(graph.checkEdge("20",str(j)))
-
-        out_edges = graph.getOutEdges("20")
-        self.assertTrue(len(out_edges) == 5)
-        out_vertices = [x[0].name for x in out_edges]
-        out_weights = [x[1] for x in out_edges]
-        for j in range(21, 26):
-            self.assertTrue(str(j) in out_vertices)
-            index = out_vertices.index(str(j))
-            self.assertTrue(out_weights[index] == 1)
-
-        # Check if you add many predecessors for a vertex then getOutEdges
-        # will return the correct number of predecessor and has the edge weight
-        for i in range(30, 33):
-            graph.addVertex(str(i))
-            self.assertTrue(graph.checkVertex(str(i)))
-
-        for j in range(31, 33):
-            graph.addEdge(str(j), "30")
-            self.assertTrue(graph.checkEdge(str(j), "30"))
-
-        in_edges = graph.getInEdges("30")
-        self.assertTrue(len(in_edges) == 2)
-        in_vertices = [x[0].name for x in in_edges]
-        in_weights = [x[1] for x in in_edges]
-        for j in range(31, 33):
-            self.assertTrue(str(j) in in_vertices)
-            index = in_vertices.index(str(j))
-            self.assertTrue(in_weights[index] == 1)
-
-        # Check if the roots are correct
-        roots = graph.getRoots()
-        self.assertTrue(len(roots) == 6)
-        for v in roots:
-            if v in ["0", "11", "12", "20", "31", "32"]:
-                continue
-
-            self.assertTrue(False)
-
-        # Check if the endings (0 out-edges) are corrects
-        endings = graph.getRoots(True)
-        self.assertTrue(len(endings) == 9)
-        for v in endings:
-            if v in ["9", "11", "12", "21", "22", "23", "24", "25", "30"]:
-                continue
-
-            self.assertTrue(False)
-
-    # Test if the graphs correctly removes cycles
-    def test_graph_remove_cycles(self):
-        simple_cycle = merge_orderfile.Graph()
-        long_cycle = merge_orderfile.Graph()
-        many_cycles = merge_orderfile.Graph()
-
-        ############## Example 1 ###############
-        # Add vertices to make sure no cycles
-        simple_cycle.addVertex("a")
-        simple_cycle.addVertex("b")
-        simple_cycle.addVertex("c")
-        simple_cycle.addVertex("d")
-
-        simple_cycle.addEdge("a","b")
-        simple_cycle.addEdge("a","b")
-        simple_cycle.addEdge("a","c")
-        simple_cycle.addEdge("b","c")
-        simple_cycle.addEdge("b","d")
-        simple_cycle.addEdge("c","d")
-        self.assertTrue(len(simple_cycle.getCycles()) == 0)
-
-        # Added a cycle (b,c)
-        simple_cycle.addEdge("c","b")
-        self.assertTrue(len(simple_cycle.getCycles()) == 1)
-
-        # Since b has a higher in-edge weights than c, the edge c->b
-        # gets removed
-        self.assertTrue(simple_cycle.checkEdgeWeight("a","b",2))
-        self.assertTrue(simple_cycle.checkEdgeWeight("a","c",1))
-
-        merge_orderfile.removeCycles(simple_cycle)
-        self.assertTrue(len(simple_cycle.getCycles()) == 0)
-        self.assertFalse(simple_cycle.checkEdge("c","b"))
-
-        ############## Example 2 ###############
-        # Add vertices to make sure no cycles
-        long_cycle.addVertex("a")
-        long_cycle.addVertex("b")
-        long_cycle.addVertex("c")
-        long_cycle.addVertex("d")
-        long_cycle.addVertex("e")
-        long_cycle.addVertex("f")
-        long_cycle.addVertex("g")
-
-        long_cycle.addEdge("a","b")
-        long_cycle.addEdge("a","c")
-        long_cycle.addEdge("c","e")
-        long_cycle.addEdge("c","f")
-        long_cycle.addEdge("b","e")
-        long_cycle.addEdge("b","g")
-        long_cycle.addEdge("a","d")
-        long_cycle.addEdge("d","e")
-        long_cycle.addEdge("e","f")
-        long_cycle.addEdge("f","g")
-        self.assertTrue(len(long_cycle.getCycles()) == 0)
-
-        # Added a cycle (d, e, f, g)
-        long_cycle.addEdge("g","d")
-        self.assertTrue(len(long_cycle.getCycles()) == 1)
-
-        # Since e has a higher in-edge weights than the other vertices in the cycle,
-        # the edge d->e gets removed.
-        # d's in-edges excluding cycle edges are (a->d): 1
-        # e's in-edges excluding cycle edges are (b->e) and (c->e): 2
-        # f's in-edges excluding cycle edges are (c->f): 1
-        # g's in-edges excluding cycle edges are (b->g): 1
-        merge_orderfile.removeCycles(long_cycle)
-        self.assertTrue(len(long_cycle.getCycles()) == 0)
-        self.assertFalse(long_cycle.checkEdge("d","e"))
-
-        ############## Example 3 ###############
-        # Add vertices to make sure no cycles
-        many_cycles.addVertex("a")
-        many_cycles.addVertex("b")
-        many_cycles.addVertex("c")
-        many_cycles.addVertex("d")
-        many_cycles.addVertex("e")
-        many_cycles.addVertex("f")
-        many_cycles.addVertex("g")
-
-        many_cycles.addEdge("a","b")
-        many_cycles.addEdge("a","b")
-        many_cycles.addEdge("a","b")
-        many_cycles.addEdge("a","d")
-        many_cycles.addEdge("b","c")
-        many_cycles.addEdge("c","d")
-        many_cycles.addEdge("d","e")
-        many_cycles.addEdge("e","f")
-        many_cycles.addEdge("f","g")
-        self.assertTrue(len(many_cycles.getCycles()) == 0)
-
-        # Added cycles (b,c,d), (d, e, f), (b,c,d,e,f,g)
-        many_cycles.addEdge("f","d")
-        many_cycles.addEdge("g","b")
-        many_cycles.addEdge("d","b")
-
-        self.assertTrue(len(many_cycles.getCycles()) == 3)
-
-        # Since in_edges(b) > in_edges(d) > the other vertices in the cycle,
-        # the edge g->b gets removed in the big cycle, d->b from the first small cycle,
-        # and edge f->d from the second small cycle
-        # b's in-edges excluding cycle edges is (a->d) with weight 3: 1
-        # c's in-edges excluding cycle edges: 0
-        # d's in-edges excluding cycle edges is (a->d) with weight 1: 1
-        # e's in-edges excluding cycle edges: 0
-        # f's in-edges excluding cycle edges: 0
-        # g's in-edges excluding cycle edges: 0
-        merge_orderfile.removeCycles(many_cycles)
-        self.assertTrue(len(many_cycles.getCycles()) == 0)
-        self.assertFalse(many_cycles.checkEdge("g","b"))
-        self.assertFalse(many_cycles.checkEdge("d","b"))
-        self.assertFalse(many_cycles.checkEdge("f","d"))
-
-    # Test if the graphs correctly orders based on our algorithm.
-    # Assume we have no cycles because the script will remove cycles
-    # before creating the order file.
-    def test_graph_order(self):
-        linear_graph = merge_orderfile.Graph()
-        merge_to_postdominator = merge_orderfile.Graph()
-        fernando_example = merge_orderfile.Graph()
-
-        ############## Example 1 ###############
-        # You only have a simple order file that have one successor
-        # along the way.
-        linear_graph.addVertex("a")
-        linear_graph.addVertex("b")
-        linear_graph.addVertex("c")
-        linear_graph.addVertex("d")
-
-        linear_graph.addEdge("a","b")
-        linear_graph.addEdge("b","c")
-        linear_graph.addEdge("c","d")
-
-        linear_graph.printOrder(self.output_file)
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.output_file,
-                                        "--partial", "a,b,c,d"])
-        self.assertTrue(output, "Order file is valid")
-
-        linear_graph.exportGraph("example1.dot")
-        self.assertTrue(os.path.isfile("example1.dot"))
-
-        # Clean up at the end
-        os.remove(self.output_file)
-        os.remove("example1.dot")
-        os.remove("example1.dot.pdf")
-
-        ############## Example 2 ###############
-        order1 = ["a","b"]
-        order2 = ["a","b","e","h"]
-        order3 = ["a","b","e","h"]
-        order4 = ["a","b","e"]
-        order5 = ["a","b","c","d","h"]
-        order6 = ["a","b","c"]
-        order7 = ["a","b","f","g","h"]
-
-        merge_orderfile.addSymbolsToGraph(merge_to_postdominator, order1)
-        merge_orderfile.addSymbolsToGraph(merge_to_postdominator, order2)
-        merge_orderfile.addSymbolsToGraph(merge_to_postdominator, order3)
-        merge_orderfile.addSymbolsToGraph(merge_to_postdominator, order4)
-        merge_orderfile.addSymbolsToGraph(merge_to_postdominator, order5)
-        merge_orderfile.addSymbolsToGraph(merge_to_postdominator, order6)
-        merge_orderfile.addSymbolsToGraph(merge_to_postdominator, order7)
-
-        merge_to_postdominator.printOrder(self.output_file)
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.output_file,
-                                        "--partial", "a,b,e,h,c,d,f,g"])
-        self.assertTrue(output, "Order file is valid")
-
-        merge_to_postdominator.exportGraph("example2.dot")
-        self.assertTrue(os.path.isfile("example2.dot"))
-
-        # Clean up at the end
-        os.remove(self.output_file)
-        os.remove("example2.dot")
-        os.remove("example2.dot.pdf")
-
-        ############## Example 3 ###############
-        order1 = ["main","a","b","c","d"]
-        order2 = ["main","a","b","c","e","f"]
-        order3 = ["main","f"]
-        order4 = ["main","a","b","c","i"]
-        order5 = ["main","g", "i", "c"]
-        order6 = ["main","g", "i", "j"]
-        order7 = ["main","h", "i"]
-        order8 = ["main","a","b","c","e","f"]
-
-        merge_orderfile.addSymbolsToGraph(fernando_example, order1)
-        merge_orderfile.addSymbolsToGraph(fernando_example, order2)
-        merge_orderfile.addSymbolsToGraph(fernando_example, order3)
-        merge_orderfile.addSymbolsToGraph(fernando_example, order4)
-        merge_orderfile.addSymbolsToGraph(fernando_example, order5)
-        merge_orderfile.addSymbolsToGraph(fernando_example, order6)
-        merge_orderfile.addSymbolsToGraph(fernando_example, order7)
-        merge_orderfile.addSymbolsToGraph(fernando_example, order8)
-
-        fernando_example.printOrder(self.output_file)
-        self.assertTrue(os.path.isfile(self.output_file))
-
-        output = utils.check_output(["python3", self.validate_script,
-                                        "--order-file", self.output_file,
-                                        "--partial", "main,a,b,c,e,f,d,i,j,g,h"])
-        self.assertTrue(output, "Order file is valid")
-
-        fernando_example.exportGraph("example3.dot")
-        self.assertTrue(os.path.isfile("example3.dot"))
-
-        # Clean up at the end
-        os.remove(self.output_file)
-        os.remove("example3.dot")
-        os.remove("example3.dot.pdf")
-
-if __name__ == '__main__':
-    unittest.main()
diff --git a/orderfiles/scripts/orderfile_utils.py b/orderfiles/scripts/orderfile_utils.py
deleted file mode 100644
index 76067d4..0000000
--- a/orderfiles/scripts/orderfile_utils.py
+++ /dev/null
@@ -1,103 +0,0 @@
-#!/usr/bin/env python
-#
-# Copyright (C) 2023 The Android Open Source Project
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-from pathlib import Path
-import glob
-import os
-import subprocess
-
-def parse_set(param : str) -> set[str]:
-    """Parse symbol set based on a file or comma-separate symbols."""
-    symbol_set = set()
-    if len(param) == 0:
-        return symbol_set
-
-    if param[0] == "@":
-        with open(param[1:], "r") as f:
-            for line in f:
-                line = line.strip()
-                symbol_set.add(line)
-        return symbol_set
-
-    list_symbols = param.split(",")
-    symbol_set.update(list_symbols)
-    return symbol_set
-
-def parse_list(param : str) -> list[str]:
-    """Parse partial order based on a file or comma-separate symbols."""
-    symbol_order = []
-    if len(param) == 0:
-        return symbol_order
-
-    if param[0] == "@":
-        with open(param[1:], "r") as f:
-            for line in f:
-                line = line.strip()
-                symbol_order.append(line)
-        return symbol_order
-
-    symbol_order = param.split(",")
-    return symbol_order
-
-def parse_merge_list(param : str) -> list[tuple[str,int]]:
-    """Parse partial order based on a file, folder, or comma-separate symbols."""
-    file_list = []
-    if len(param) == 0:
-        return file_list
-
-    if param[0] == "@":
-        file_dir = Path(param[1:]).resolve().parent
-        with open(param[1:], "r") as f:
-            for line in f:
-                line = line.strip()
-                line_list = line.split(",")
-                # Name, Weight
-                file_list.append((file_dir / line_list[0], int(line_list[1])))
-        return file_list
-
-    if param[0] == "^":
-        file_lst = glob.glob(param[1:]+"/*.orderfile")
-        # Assumig weight of 1 for all the files. Sorting of files provides
-        # a deterministic order of orderfile.
-        file_list = sorted([(orderfile, 1) for orderfile in file_lst])
-        return file_list
-
-    file_lst = param.split(",")
-    file_list = [(orderfile, 1) for orderfile in file_lst]
-    return file_list
-
-def check_call(cmd, *args, **kwargs):
-    """subprocess.check_call."""
-    subprocess.check_call(cmd, *args, **kwargs)
-
-
-def check_output(cmd, *args, **kwargs):
-    """subprocess.check_output."""
-    return subprocess.run(
-        cmd, *args, **kwargs, check=True, text=True,
-        stdout=subprocess.PIPE).stdout
-
-def check_error(cmd, *args, **kwargs):
-    """subprocess.check_error."""
-    return subprocess.run(
-        cmd, *args, **kwargs, check=True, text=True,
-        stdout=subprocess.PIPE, stderr=subprocess.STDOUT).stdout
-
-def android_build_top():
-    """Get top directory to find files."""
-    THIS_DIR = os.path.realpath(os.path.dirname(__file__))
-    return os.path.realpath(os.path.join(THIS_DIR, '../../../..'))
\ No newline at end of file
diff --git a/orderfiles/scripts/validate_orderfile.py b/orderfiles/scripts/validate_orderfile.py
deleted file mode 100644
index 83faa23..0000000
--- a/orderfiles/scripts/validate_orderfile.py
+++ /dev/null
@@ -1,128 +0,0 @@
-#!/usr/bin/env python3
-#
-# Copyright (C) 2023 The Android Open Source Project
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-# Sample Usage:
-# $ python3 validate_orderfile.py --order-file ../orderfiles/test/example.orderfile
-#
-# Try '-h' for a full list of command line arguments.
-#
-# Currently, we check four things in an orderfile:
-#   - A partial order is maintained in the orderfile
-#   - All symbols in allowlist must be present in the orderfile
-#   - No symbol in denylist should be present in the orderfile
-#   - The orderfile has a minimum number of symbols
-
-import argparse
-import orderfile_utils
-
-def parse_args():
-    """Parses and returns command line arguments."""
-    parser = argparse.ArgumentParser(prog="validate_orderfile",
-                                    description="Validates the orderfile is correct and useful based on flag conditions")
-
-    parser.add_argument(
-        "--order-file",
-        required=True,
-        help="Orderfile that needs to be validated")
-
-    parser.add_argument(
-        "--partial",
-        default="",
-        help=f"A partial order of symbols that need to hold in the orderfile."
-             f"Format: A symbol-per-line file with @ or comma separarted values within a quotation."
-             f"For example, you can say @file.txt or 'main,bar,foo'.")
-
-    parser.add_argument(
-        "--allowlist",
-        default="",
-        help=f"Symbols that have to be present in the orderfile."
-             f"Format: A symbol-per-line file with @ or comma separarted values within a quotation."
-             f"For example, you can say @file.txt or 'main,bar,foo'.")
-
-    parser.add_argument(
-        "--denylist",
-        default="",
-        help=f"Symbols that should not be in orderfile. Denylist flag has priority over allowlist."
-             f"Format: A symbol-per-line file with @ or comma separarted values within a quotation."
-             f"For example, you can say @file.txt or 'main,bar,foo'.")
-
-    parser.add_argument(
-        "--min",
-        type=int,
-        default=0,
-        help="Minimum number of entires needed for an orderfile")
-
-    return parser.parse_args()
-
-def main():
-    args = parse_args()
-
-    allowlist = orderfile_utils.parse_set(args.allowlist)
-    partial = orderfile_utils.parse_list(args.partial)
-    denylist = orderfile_utils.parse_set(args.denylist)
-
-    # Check if there are symbols common to both allowlist and denylist
-    # We give priority to denylist so the symbols in the intersection
-    # will be removed from allowlist
-    inter = allowlist.intersection(denylist)
-    allowlist = allowlist.difference(inter)
-
-    num_entries = 0
-    file_indices = {}
-    file_present = set()
-
-    # Read the orderfile
-    with open(args.order_file, "r") as f:
-        for line in f:
-            line = line.strip()
-
-            # Check if a symbol not allowed is within the orderfile
-            if line in denylist:
-                raise RuntimeError(f"Orderfile should not contain {line}")
-
-            if line in allowlist:
-                file_present.add(line)
-
-            file_indices[line] = num_entries
-            num_entries += 1
-
-    # Check if there are not a minimum number of symbols in orderfile
-    if num_entries < args.min:
-        raise RuntimeError(f"The orderfile has {num_entries} symbols but it "
-                           f"needs at least {args.min} symbols")
-
-    # Check if all symbols allowed must be allowlist
-    if len(allowlist) != len(file_present):
-        raise RuntimeError("Some symbols in allow-list are not in the orderfile")
-
-    # Check if partial order passed with flag is maintained within orderfile
-    # The partial order might contain symbols not in the orderfile which we allow
-    # because the order is still maintained.
-    old_index = None
-    curr_symbol = None
-    for symbol in partial:
-        new_index = file_indices.get(symbol)
-        if new_index is not None:
-            if old_index is not None:
-                if new_index < old_index:
-                    raise RuntimeError(f"`{curr_symbol}` must be before `{symbol}` in orderfile")
-            old_index = new_index
-            curr_symbol = symbol
-
-    print("Order file is valid")
-
-if __name__ == '__main__':
-    main()
diff --git a/orderfiles/test/allowlistv.txt b/orderfiles/test/allowlistv.txt
deleted file mode 100644
index 88d050b..0000000
--- a/orderfiles/test/allowlistv.txt
+++ /dev/null
@@ -1 +0,0 @@
-main
\ No newline at end of file
diff --git a/orderfiles/test/denylist.txt b/orderfiles/test/denylist.txt
deleted file mode 100644
index 3fb6092..0000000
--- a/orderfiles/test/denylist.txt
+++ /dev/null
@@ -1 +0,0 @@
-_Z4partPiii
\ No newline at end of file
diff --git a/orderfiles/test/example-mapping.txt b/orderfiles/test/example-mapping.txt
deleted file mode 100644
index 9c6bd76..0000000
--- a/orderfiles/test/example-mapping.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-MD5 db956436e78dd5fa main
-MD5 83bff1e88ac48f32 _GLOBAL__sub_I_main.cpp
-MD5 c943255f95351375 _Z5mergePiiii
-MD5 d2d2238cf08db816 _Z9mergeSortPiii
-MD5 11ed18006e729e73 _Z4partPiii
-MD5 3e897b5ee8bebbd1 _Z9quickSortPiii
diff --git a/orderfiles/test/example.orderfile b/orderfiles/test/example.orderfile
deleted file mode 100644
index 2648f79..0000000
--- a/orderfiles/test/example.orderfile
+++ /dev/null
@@ -1,5 +0,0 @@
-_GLOBAL__sub_I_main.cpp
-main
-_Z9mergeSortPiii
-_Z5mergePiiii
-_Z9quickSortPiii
diff --git a/orderfiles/test/example.prof b/orderfiles/test/example.prof
deleted file mode 100644
index 8b8cf3d..0000000
--- a/orderfiles/test/example.prof
+++ /dev/null
@@ -1,6 +0,0 @@
-00000000  32 8f c4 8a e8 f1 bf 83  fa d5 8d e7 36 64 95 db  |2...........6d..|
-00000010  16 b8 8d f0 8c 23 d2 d2  75 13 35 95 5f 25 43 c9  |.....#..u.5._%C.|
-00000020  d1 bb be e8 5e 7b 89 3e  00 00 00 00 00 00 00 00  |....^{.>........|
-00000030  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|
-*
-00080000
diff --git a/orderfiles/test/merge-test/1.orderfile b/orderfiles/test/merge-test/1.orderfile
deleted file mode 100644
index 0f14c35..0000000
--- a/orderfiles/test/merge-test/1.orderfile
+++ /dev/null
@@ -1,4 +0,0 @@
-main
-b
-c
-d
\ No newline at end of file
diff --git a/orderfiles/test/merge-test/2.orderfile b/orderfiles/test/merge-test/2.orderfile
deleted file mode 100644
index c4de398..0000000
--- a/orderfiles/test/merge-test/2.orderfile
+++ /dev/null
@@ -1,3 +0,0 @@
-main
-e
-f
\ No newline at end of file
diff --git a/orderfiles/test/merge-test/3.orderfile b/orderfiles/test/merge-test/3.orderfile
deleted file mode 100644
index 487fe82..0000000
--- a/orderfiles/test/merge-test/3.orderfile
+++ /dev/null
@@ -1,3 +0,0 @@
-main
-a
-c
\ No newline at end of file
diff --git a/orderfiles/test/merge-test/4.orderfile b/orderfiles/test/merge-test/4.orderfile
deleted file mode 100644
index 1f82a38..0000000
--- a/orderfiles/test/merge-test/4.orderfile
+++ /dev/null
@@ -1,2 +0,0 @@
-main
-b
\ No newline at end of file
diff --git a/orderfiles/test/merge-test/5.orderfile b/orderfiles/test/merge-test/5.orderfile
deleted file mode 100644
index 1f82a38..0000000
--- a/orderfiles/test/merge-test/5.orderfile
+++ /dev/null
@@ -1,2 +0,0 @@
-main
-b
\ No newline at end of file
diff --git a/orderfiles/test/merge-test/6.orderfile b/orderfiles/test/merge-test/6.orderfile
deleted file mode 100644
index 370c695..0000000
--- a/orderfiles/test/merge-test/6.orderfile
+++ /dev/null
@@ -1,4 +0,0 @@
-main
-a
-c
-b
\ No newline at end of file
diff --git a/orderfiles/test/merge-test/merge.txt b/orderfiles/test/merge-test/merge.txt
deleted file mode 100644
index acde6b2..0000000
--- a/orderfiles/test/merge-test/merge.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-1.orderfile,4
-2.orderfile,2
-3.orderfile,1
-6.orderfile,1
\ No newline at end of file
diff --git a/orderfiles/test/partial.txt b/orderfiles/test/partial.txt
deleted file mode 100644
index 9f8f7eb..0000000
--- a/orderfiles/test/partial.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-_Z9mergeSortPiii
-_Z5mergePiiii
\ No newline at end of file
diff --git a/orderfiles/test/partial_bad.txt b/orderfiles/test/partial_bad.txt
deleted file mode 100644
index d0120d6..0000000
--- a/orderfiles/test/partial_bad.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-_Z5mergePiiii
-_Z9mergeSortPiii
\ No newline at end of file
diff --git a/patches/Revert-Driver-Allow-target-override-containing-.-in-executable-name.patch b/patches/Revert-Driver-Allow-target-override-containing-.-in-executable-name.patch
index 1dca1d7..be27dca 100644
--- a/patches/Revert-Driver-Allow-target-override-containing-.-in-executable-name.patch
+++ b/patches/Revert-Driver-Allow-target-override-containing-.-in-executable-name.patch
@@ -23,20 +23,6 @@ index 2dba975a5a8f..b10d5e098a4e 100644
    if (is_style_windows(llvm::sys::path::Style::native)) {
      // Transform to lowercase for case insensitive file systems.
      std::transform(ProgName.begin(), ProgName.end(), ProgName.begin(),
-@@ -245,13 +245,6 @@ static const DriverSuffix *parseDriverSuffix(StringRef ProgName, size_t &Pos) {
-   // added via -target as implicit first argument.
-   const DriverSuffix *DS = FindDriverSuffix(ProgName, Pos);
- 
--  if (!DS && ProgName.endswith(".exe")) {
--    // Try again after stripping the executable suffix:
--    // clang++.exe -> clang++
--    ProgName = ProgName.drop_back(StringRef(".exe").size());
--    DS = FindDriverSuffix(ProgName, Pos);
--  }
--
-   if (!DS) {
-     // Try again after stripping any trailing version number:
-     // clang++3.5 -> clang++
 diff --git a/clang/test/Driver/target-override.c b/clang/test/Driver/target-override.c
 index 2c605ac9a03d..aef89cc9a9dd 100644
 --- a/clang/test/Driver/target-override.c
diff --git a/paths.py b/paths.py
old mode 100644
new mode 100755
index c4e1411..76afa60
--- a/paths.py
+++ b/paths.py
@@ -15,7 +15,6 @@
 #
 """Helpers for paths."""
 
-import glob
 import os
 from pathlib import Path
 import string
@@ -28,7 +27,6 @@ import hosts
 SCRIPTS_DIR: Path = Path(__file__).resolve().parent
 ANDROID_DIR: Path = SCRIPTS_DIR.parents[1]
 OUT_DIR: Path = Path(os.environ.get('OUT_DIR', ANDROID_DIR / 'out')).resolve()
-DIST_DIR = Path(os.environ.get('DIST_DIR', OUT_DIR)).resolve()
 SYSROOTS: Path = OUT_DIR / 'sysroots'
 LLVM_PATH: Path = OUT_DIR / 'llvm-project'
 PREBUILTS_DIR: Path = ANDROID_DIR / 'prebuilts'
@@ -37,8 +35,6 @@ TOOLCHAIN_DIR: Path = ANDROID_DIR / 'toolchain'
 TOOLCHAIN_UTILS_DIR: Path = EXTERNAL_DIR / 'toolchain-utils'
 TOOLCHAIN_LLVM_PATH: Path = TOOLCHAIN_DIR / 'llvm-project'
 
-KLEAF_VERSIONS_BZL: Path = PREBUILTS_DIR / 'clang' / 'host' / 'linux-x86' / 'kleaf' / 'versions.bzl'
-
 CLANG_PREBUILT_DIR: Path = (PREBUILTS_DIR / 'clang' / 'host' / hosts.build_host().os_tag
                             / constants.CLANG_PREBUILT_VERSION)
 CLANG_PREBUILT_LIBCXX_HEADERS: Path = CLANG_PREBUILT_DIR / 'include' / 'c++' / 'v1'
@@ -50,14 +46,12 @@ BIONIC_HEADERS: Path = ANDROID_DIR / 'bionic' / 'libc' / 'include'
 BIONIC_KERNEL_HEADERS: Path = ANDROID_DIR / 'bionic' / 'libc' / 'kernel' / 'uapi'
 
 GO_BIN_PATH: Path = PREBUILTS_DIR / 'go' / hosts.build_host().os_tag / 'bin'
-CMAKE_BIN_PATH: Path = PREBUILTS_DIR / 'cmake' / hosts.build_host().os_tag / 'bin' / 'cmake'
+CMAKE_BIN_PATH: Path = Path('/bin/cmake')
 BUILD_TOOLS_DIR: Path = PREBUILTS_DIR / 'build-tools'
-BISON_BIN_PATH: Path = BUILD_TOOLS_DIR / hosts.build_host().os_tag / 'bin' / 'bison'
-M4_BIN_PATH: Path = BUILD_TOOLS_DIR / hosts.build_host().os_tag / 'bin' / 'm4'
-MAKE_BIN_PATH: Path = BUILD_TOOLS_DIR / hosts.build_host().os_tag / 'bin' / 'make'
+MAKE_BIN_PATH: Path = Path('/bin/make')
 # Use the musl version of ninja on Linux, it is statically linked and avoids
 # problems with LD_LIBRARY_PATH causing ninja to use the wrong libc++.so.
-NINJA_BIN_PATH: Path = BUILD_TOOLS_DIR / hosts.build_host().os_tag_musl / 'bin' / 'ninja'
+NINJA_BIN_PATH: Path = Path('/bin/ninja')
 
 LIBEDIT_SRC_DIR: Path = EXTERNAL_DIR / 'libedit'
 LIBNCURSES_SRC_DIR: Path = EXTERNAL_DIR / 'libncurses'
@@ -66,7 +60,10 @@ SWIG_SRC_DIR: Path = EXTERNAL_DIR / 'swig'
 XZ_SRC_DIR: Path = TOOLCHAIN_DIR / 'xz'
 ZSTD_SRC_DIR: Path = EXTERNAL_DIR / 'zstd'
 
-NDK_BASE: Path = TOOLCHAIN_DIR / 'prebuilts' / 'ndk' / 'releases' / constants.NDK_VERSION
+NDK_BASE: Path = TOOLCHAIN_DIR / 'prebuilts' /'ndk' / constants.NDK_VERSION
+NDK_LIBCXX_HEADERS: Path = NDK_BASE / 'sources' / 'cxx-stl' / 'llvm-libc++'/ 'include'
+NDK_LIBCXXABI_HEADERS: Path = NDK_BASE / 'sources' / 'cxx-stl' / 'llvm-libc++abi' / 'include'
+NDK_SUPPORT_HEADERS: Path = NDK_BASE / 'sources' / 'android' / 'support' / 'include'
 
 RISCV64_ANDROID_SYSROOT: Path = TOOLCHAIN_DIR / 'prebuilts' / 'sysroot' / 'platform' / 'riscv64-linux-android'
 
@@ -86,7 +83,8 @@ KYTHE_CXX_EXTRACTOR = (PREBUILTS_DIR / 'clang-tools' / hosts.build_host().os_tag
 KYTHE_OUTPUT_DIR = OUT_DIR / 'kythe-files'
 KYTHE_VNAMES_JSON = SCRIPTS_DIR / 'kythe_vnames.json'
 
-ORDERFILE_SCRIPTS_DIR: Path = TOOLCHAIN_DIR / "llvm_android" / "orderfiles" / "scripts"
+_PYTHON_VER = '3'
+_PYTHON_VER_SHORT = '3'
 
 def pgo_profdata_filename() -> str:
     svn_revision = android_version.get_svn_revision_number()
@@ -95,31 +93,23 @@ def pgo_profdata_filename() -> str:
 
 def pgo_profdata_tarname() -> str:
     svn_revision = android_version.get_svn_revision_number()
-    return f'pgo-r{svn_revision}.tar.xz'
+    return f'pgo-r{svn_revision}.tar.bz2'
 
 
 def pgo_profdata_tar() -> Optional[Path]:
-    profile_env = os.getenv('LLVM_PGO_PROFILE')
-    if profile_env is not None:
-        profile = Path(glob.glob(profile_env)[0])
-    else:
-        profile = (PREBUILTS_DIR / 'clang' / 'host' / 'linux-x86' / 'profiles' /
-                   pgo_profdata_tarname())
+    profile = (PREBUILTS_DIR / 'clang' / 'host' / 'linux-x86' / 'profiles' /
+               pgo_profdata_tarname())
     return profile if profile.exists() else None
 
 
 def bolt_fdata_tarname() -> str:
     svn_revision = android_version.get_svn_revision_number()
-    return f'bolt-r{svn_revision}.tar.xz'
+    return f'bolt-r{svn_revision}.tar.bz2'
 
 
 def bolt_fdata_tar() -> Optional[Path]:
-    profile_env = os.getenv('LLVM_BOLT_PROFILE')
-    if profile_env is not None:
-        profile = Path(glob.glob(profile_env)[0])
-    else:
-        profile = (PREBUILTS_DIR / 'clang' / 'host' / 'linux-x86' / 'profiles' /
-                   bolt_fdata_tarname())
+    profile = (PREBUILTS_DIR / 'clang' / 'host' / 'linux-x86' / 'profiles' /
+               bolt_fdata_tarname())
     return profile if profile.exists() else None
 
 
@@ -136,22 +126,6 @@ def get_python_dir(host: hosts.Host) -> Path:
     """Returns the path to python for a host."""
     return PREBUILTS_DIR / 'python' / host.os_tag
 
-
-def determine_python_ver() -> str:
-    python_path = get_python_dir(hosts.Host.Linux)
-    versions = list(python_path.glob("include/python*"))
-    if len(versions) != 1:
-        raise RuntimeError(
-            "Could not determine unique Python version from "
-            f"{python_path / 'include'}"
-        )
-    return versions[0].name.removeprefix("python")
-
-
-_PYTHON_VER = determine_python_ver()
-_PYTHON_VER_SHORT = _PYTHON_VER.replace('.', '')
-
-
 def get_python_executable(host: hosts.Host) -> Path:
     """Returns the path to python executable for a host."""
     python_root = get_python_dir(host)
@@ -187,9 +161,3 @@ def get_python_dynamic_lib(host: hosts.Host) -> Path:
         hosts.Host.Darwin: python_root / 'lib' / f'libpython{_PYTHON_VER}.dylib',
         hosts.Host.Windows: python_root / f'python{_PYTHON_VER_SHORT}.dll',
     }[host]
-
-def get_tensorflow_path() -> Optional[Path]:
-    path = os.getenv('TENSORFLOW_INSTALL')
-    if path is not None and Path(path).is_dir():
-        return path
-    return None
diff --git a/py3_utils.py b/py3_utils.py
old mode 100644
new mode 100755
index 0f0e9c0..ad456c6
--- a/py3_utils.py
+++ b/py3_utils.py
@@ -20,17 +20,8 @@ import subprocess
 import sys
 
 THIS_DIR = os.path.realpath(os.path.dirname(__file__))
-def get_host_tag():
-    if sys.platform.startswith('linux'):
-        return 'linux-x86'
-    if sys.platform.startswith('darwin'):
-        return 'darwin-x86'
-    raise RuntimeError('Unsupported host: {}'.format(sys.platform))
 
 
 def run_with_py3(script_name):
-    python_bin = os.path.join(THIS_DIR, '..', '..', 'prebuilts', 'build-tools',
-                              'path', get_host_tag(), 'python3')
-    python_bin = os.path.abspath(python_bin)
     subprocess.check_call(
-        [python_bin, os.path.join(THIS_DIR, script_name)] + sys.argv[1:])
+        ["/bin/python3", os.path.join(THIS_DIR, script_name)] + sys.argv[1:])
diff --git a/pylintrc b/pylintrc
old mode 100644
new mode 100755
diff --git a/remove-prebuilts.py b/remove-prebuilts.py
index b375806..1a8bc57 100755
--- a/remove-prebuilts.py
+++ b/remove-prebuilts.py
@@ -50,16 +50,6 @@ def remove_prebuilt(prebuilt_dir: Path, version: str, use_cbr: bool) -> None:
     shutil.rmtree(clang_dir)
 
 
-def remove_kleaf_version(prebuilt_dir: Path, version: str) -> None:
-    version_line = '    "{}",'.format(version)
-    with open(paths.KLEAF_VERSIONS_BZL) as f:
-        kleaf_versions_lines = f.read().splitlines()
-    kleaf_versions_lines.remove(version_line)
-    with open(paths.KLEAF_VERSIONS_BZL, "w") as f:
-        f.write("\n".join(kleaf_versions_lines))
-    utils.check_call(['git', 'add', paths.KLEAF_VERSIONS_BZL], cwd=prebuilt_dir)
-
-
 def do_commit(prebuilt_dir: Path, version: str, bug_id: Optional[str]) -> None:
     utils.check_call(['git','rm', '-r', f'clang-{version}'], cwd=prebuilt_dir)
 
@@ -83,8 +73,6 @@ def main():
     for host in hosts:
         prebuilt_dir = paths.PREBUILTS_DIR / 'clang' / 'host' / host
         remove_prebuilt(prebuilt_dir, args.version, args.use_current_branch)
-        if host == 'linux-x86':
-            remove_kleaf_version(prebuilt_dir, args.version)
         do_commit(prebuilt_dir, args.version, args.bug)
 
     if args.repo_upload:
diff --git a/source_manager.py b/source_manager.py
old mode 100644
new mode 100755
index 15b282f..8027794
--- a/source_manager.py
+++ b/source_manager.py
@@ -19,7 +19,6 @@ Package to manage LLVM sources when building a toolchain.
 
 import logging
 from pathlib import Path
-from typing import List, Optional
 import os
 import re
 import shutil
@@ -28,7 +27,6 @@ import subprocess
 import sys
 
 import android_version
-from toolchain_errors import ToolchainErrorCode, ToolchainError
 import hosts
 import paths
 import utils
@@ -39,14 +37,14 @@ def logger():
     return logging.getLogger(__name__)
 
 
-def apply_patches(source_dir, svn_version, patch_json, patch_dir, git_am,
-                  failure_mode):
+def apply_patches(source_dir, svn_version, patch_json, patch_dir,
+                  failure_mode='fail'):
     """Apply patches in $patch_dir/$patch_json to $source_dir.
 
     Invokes external/toolchain-utils/llvm_tools/patch_manager.py to apply the
     patches.
     """
-    assert failure_mode, "Invalid failure_mode"
+
     patch_manager_cmd = [
         sys.executable,
         str(paths.TOOLCHAIN_UTILS_DIR / 'llvm_tools' / 'patch_manager.py'),
@@ -56,23 +54,14 @@ def apply_patches(source_dir, svn_version, patch_json, patch_dir, git_am,
         '--failure_mode', failure_mode
     ]
 
-    patch_dir = os.getcwd()
-    if git_am:
-      patch_manager_cmd.append('--git_am')
-      """Run git am in the source directory"""
-      patch_dir=source_dir
+    return utils.check_output(patch_manager_cmd)
 
-    return utils.check_output(patch_manager_cmd, cwd=patch_dir)
 
-class PatchInfo:
-    """Holds info for a round of patch applications."""
-    def __init__(self) -> None:
-        self.applied_patches = []
-        self.failed_patches = []
-        self.inapplicable_patches = []
+def write_source_info(source_dir: str, patch_output: str) -> None:
+    url_prefix = 'https://android.googlesource.com/toolchain/llvm_android/+/' +\
+        '{{scripts_sha}}'
 
-    @staticmethod
-    def get_subject(patch_file: Path) -> str:
+    def _get_subject(patch_file: Path) -> str:
         contents = patch_file.read_text()
         # Parse patch generated by `git format-patch`.
         matches = re.search('Subject: (.*)\n', contents)
@@ -88,77 +77,45 @@ class PatchInfo:
         subject = matches.group(1)
         return subject
 
-    @staticmethod
-    def format_patch_line(patch_file: Path) -> str:
-        url_prefix = 'https://android.googlesource.com/toolchain/llvm_android/+/' +\
-            '{{scripts_sha}}'
+
+    def _format_patch_line(patch_file: Path) -> str:
         assert patch_file.is_file(), f"patch file doesn't exist: {patch_file}"
         patch_name = patch_file.name
         if re.match('([0-9a-f]+)(_v[0-9]+)?\.patch$', patch_name):
             url_suffix = '/patches/cherry/' + patch_name
-            link_text = PatchInfo.get_subject(patch_file)
+            link_text = _get_subject(patch_file)
         else:
             url_suffix = '/patches/' + patch_name
             link_text = patch_name
         return f'- [{link_text}]({url_prefix}{url_suffix})'
 
-def write_source_info(source_dir: str, pi: PatchInfo) -> None:
     output = []
     base_revision = android_version.get_git_sha()
     github_url = 'https://github.com/llvm/llvm-project/commits/' + base_revision
     output.append(f'Base revision: [{base_revision}]({github_url})')
     output.append('')
 
-    applied_patches = []
-    for patch in pi.applied_patches:
-        applied_patches.append(pi.format_patch_line(Path(patch)))
-
-    output.extend(sorted(applied_patches))
-
-    with open(paths.OUT_DIR / 'clang_source_info.md', 'w') as outfile:
-        outfile.write('\n'.join(output))
-
-
-def get_source_info(source_dir: str, patch_output: str) -> PatchInfo:
-    pi = PatchInfo()
     patches = patch_output.strip().splitlines()
     patches_iter = iter(patches)
     assert next(patches_iter) == 'The following patches applied successfully:'
-
-    applied = True # applied/successful patches
-    failed = False # failed patches
-    inapplicable = False # inapplicable patches
-
     while True:
         patch = next(patches_iter, None)
         # We may optionally have an empty line followed by patches that were not
         # applicable.
         if patch == '':
-            ni = next(patches_iter)
-            if ni == 'The following patches were not applicable:':
-                applied = False
-                failed = False
-                inapplicable = True
-                continue
-            if ni == 'The following patches failed to apply:':
-                applied = False
-                failed = True
-                inapplicable = False
-                continue
+            assert next(
+                patches_iter) == 'The following patches were not applicable:'
+            break
         elif patch is None:
             break
-        print("patch={},{},{}$$".format(patch, type(patch), len(patch)))
         assert patch.endswith('.patch')
-        if applied:
-            pi.applied_patches.append(patch)
-        if failed:
-            pi.failed_patches.append(patch)
-        if inapplicable:
-            pi.inapplicable_patches.append(patch)
+        output.append(_format_patch_line(Path(patch)))
 
-    return pi
+    with open(paths.OUT_DIR / 'clang_source_info.md', 'w') as outfile:
+        outfile.write('\n'.join(output))
 
-def setup_sources(git_am=False, llvm_rev=None, skip_apply_patches=False, continue_on_patch_errors=False) -> Optional[ToolchainError]:
+
+def setup_sources(llvm_rev=None, skip_apply_patches=False):
     """Setup toolchain sources into paths.LLVM_PATH.
 
     Copy toolchain/llvm-project into paths.LLVM_PATH or clone from upstream.
@@ -166,8 +123,7 @@ def setup_sources(git_am=False, llvm_rev=None, skip_apply_patches=False, continu
     toolchain/llvm_android/patches/PATCHES.json.  The function overwrites
     paths.LLVM_PATH only if necessary to avoid recompiles during incremental builds.
     """
-    # Return the error messages upon failure.
-    ret: Optional[ToolchainError] = []
+
     source_dir = paths.LLVM_PATH
     tmp_source_dir = source_dir.parent / (source_dir.name + '.tmp')
     if os.path.exists(tmp_source_dir):
@@ -181,7 +137,6 @@ def setup_sources(git_am=False, llvm_rev=None, skip_apply_patches=False, continu
     if not llvm_rev:
         # Copy llvm source tree to a temporary directory.
         copy_from = paths.TOOLCHAIN_LLVM_PATH
-        logger().info(f'No llvm revision provided, copying from {copy_from}')
         # Use 'cp' instead of shutil.copytree.  The latter uses copystat and retains
         # timestamps from the source.  We instead use rsync below to only update
         # changed files into source_dir.  Using 'cp' will ensure all changed files
@@ -195,20 +150,7 @@ def setup_sources(git_am=False, llvm_rev=None, skip_apply_patches=False, continu
           # Fallback to normal copy.
           cmd = ['cp', '-Rf', copy_from, tmp_source_dir]
           subprocess.check_call(cmd)
-
-        if git_am:
-          # To avoid clobbering the source tree's git objects, remove
-          # out/llvm-project/.git and copy .repo/projects/toolchain/llvm-project.git there
-          tmp_out_git_dir = tmp_source_dir / '.git'
-          copy_from_git = os.path.abspath(os.path.join(tmp_source_dir, os.readlink(tmp_out_git_dir)))
-
-          cmd = ['rm', '-rf', tmp_out_git_dir]
-          subprocess.check_call(cmd)
-
-          cmd = ['cp', '-Rf', '-L', copy_from_git, tmp_out_git_dir]
-          subprocess.check_call(cmd)
     else:
-        logger().info(f'Fetching {llvm_rev} from https://github.com/llvm/llvm-project.git')
         if not os.path.exists(tmp_source_dir):
             os.makedirs(tmp_source_dir)
         with utils.chdir_context(tmp_source_dir):
@@ -227,13 +169,10 @@ def setup_sources(git_am=False, llvm_rev=None, skip_apply_patches=False, continu
     svn_version = android_version.get_svn_revision_number()
 
     if not skip_apply_patches:
-      failure_mode = 'continue' if continue_on_patch_errors else 'fail'
       patch_output = apply_patches(tmp_source_dir, svn_version, patch_json,
-                                   patch_dir, git_am, failure_mode)
+                                   patch_dir)
       logger().info(patch_output)
-      pi = get_source_info(tmp_source_dir, patch_output)
-      write_source_info(tmp_source_dir, pi)
-      ret = ToolchainError(ToolchainErrorCode.PATCH_ERROR, str(pi.failed_patches))
+      write_source_info(tmp_source_dir, patch_output)
 
     # Copy tmp_source_dir to source_dir if they are different.  This avoids
     # invalidating prior build outputs.
@@ -244,17 +183,16 @@ def setup_sources(git_am=False, llvm_rev=None, skip_apply_patches=False, continu
         # $DST/BASENAME($SRC) instead of $DST.
         tmp_source_dir_str = str(tmp_source_dir) + '/'
 
-        # rsync to update only changed files.  Use '-c' to use checksums to find
-        # if files have changed instead of only modification time and size -
-        # which could have inconsistencies.  Use '--delete' to ensure files not
-        # in tmp_source_dir are deleted from $source_dir.
-        subprocess.check_call(['rsync', '-r', '--delete', '--links', '-c',
-                               tmp_source_dir_str, source_dir])
+        # # rsync to update only changed files.  Use '-c' to use checksums to find
+        # # if files have changed instead of only modification time and size -
+        # # which could have inconsistencies.  Use '--delete' to ensure files not
+        # # in tmp_source_dir are deleted from $source_dir.
+        # subprocess.check_call(['rsync', '-r', '--delete', '--links', '-c',
+        #                        tmp_source_dir_str, source_dir])
 
         shutil.rmtree(tmp_source_dir)
     remote, url = try_set_git_remote(source_dir)
     logger().info(f'git remote url: remote: {remote} url: {url}')
-    return ret
 
 
 def try_set_git_remote(source_dir):
diff --git a/test/scripts/ab_client.py b/test/scripts/ab_client.py
old mode 100644
new mode 100755
diff --git a/test/scripts/cluster_info.yaml b/test/scripts/cluster_info.yaml
old mode 100644
new mode 100755
diff --git a/test/scripts/data.py b/test/scripts/data.py
old mode 100644
new mode 100755
diff --git a/test/scripts/forrest.py b/test/scripts/forrest.py
old mode 100644
new mode 100755
diff --git a/test/scripts/gerrit.py b/test/scripts/gerrit.py
old mode 100644
new mode 100755
diff --git a/test/scripts/test_configs.yaml b/test/scripts/test_configs.yaml
old mode 100644
new mode 100755
diff --git a/test/scripts/test_paths.py b/test/scripts/test_paths.py
old mode 100644
new mode 100755
diff --git a/timer.py b/timer.py
old mode 100644
new mode 100755
index 09aca06..f06702c
--- a/timer.py
+++ b/timer.py
@@ -34,14 +34,9 @@ class Timer:
     @classmethod
     def report(cls):
         """Return list of '<duration> <description>' entries."""
-        num_cores = os.cpu_count()
-        report = f'Cores: {num_cores}\n\n'
-
         pretty_print = lambda t: str(timedelta(seconds=int(t)))
         result = sorted(cls.times.items(), key=lambda item: item[1], reverse=True)
-        report = report + '\n'.join(f'{pretty_print(t)} {d}' for d, t in result)
-
-        return report
+        return '\n'.join(f'{pretty_print(t)} {d}' for d, t in result)
 
     @classmethod
     def report_to_file(cls, outfile):
diff --git a/toolchain_errors.py b/toolchain_errors.py
deleted file mode 100644
index 2700a0f..0000000
--- a/toolchain_errors.py
+++ /dev/null
@@ -1,41 +0,0 @@
-#
-# Copyright (C) 2024 The Android Open Source Project
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-"""Report toolchain build errors"""
-
-from enum import Enum, auto
-from typing import List
-
-class ToolchainErrorCode(Enum):
-    UNKNOWN_ERROR = auto()
-    PATCH_ERROR = auto()
-    STAGE1_BUILD_ERROR = auto()
-    STAGE1_TEST_ERROR = auto()
-    STAGE2_BUILD_ERROR = auto()
-    STAGE2_TEST_ERROR = auto()
-    RUNTIME_BUILD_ERROR = auto()
-
-class ToolchainError:
-    code: ToolchainErrorCode
-    msg: str
-    def __init__(self, code: ToolchainErrorCode, msg: str) -> None:
-        self.code = code
-        self.msg = msg
-
-    def __repr__(self):
-        return repr(self.code.name + ':' + self.msg)
-
-def combine_toolchain_errors(errs: List[ToolchainError]) -> str:
-    return ',\n'.join(map(str, errs))
diff --git a/toolchains.py b/toolchains.py
old mode 100644
new mode 100755
index 7cc65d5..d30c863
--- a/toolchains.py
+++ b/toolchains.py
@@ -116,7 +116,7 @@ class Toolchain:
     @property
     def lib_dirs(self) -> List[Path]:
         """Returns the paths to lib dirs."""
-        return [self.path / 'lib', self.path / 'lib' / 'x86_64-unknown-linux-gnu', self.path / 'lib' / 'x86_64-unknown-linux-musl']
+        return [self.path / 'lib', self.path / 'lib' / 'aarch64-linux-gnu', self.path / 'lib' / 'x86_64-unknown-linux-musl']
 
     @property
     def _version_file(self) -> Path:
@@ -136,10 +136,10 @@ class Toolchain:
 
     @property
     def libcxx_headers(self) -> Path:
-        return self.path / 'include' / 'c++' / 'v1'
+        return self.path / 'include' / 'c++' / '12'
 
 
 def get_prebuilt_toolchain() -> Toolchain:
     """Returns the prebuilt toolchain."""
     # Prebuilt toolchain doesn't have a build path. Use a temp path instead.
-    return Toolchain(paths.CLANG_PREBUILT_DIR, Path('.'))
+    return Toolchain(Path('/usr'), Path('.'))
diff --git a/trim_patch_data.py b/trim_patch_data.py
index 4bbc140..da0f849 100755
--- a/trim_patch_data.py
+++ b/trim_patch_data.py
@@ -16,7 +16,6 @@
 #
 
 import os
-from pathlib import Path
 import string
 import subprocess
 
@@ -28,38 +27,20 @@ import source_manager
 import sys
 
 _LLVM_ANDROID_PATH = paths.SCRIPTS_DIR
-_PATCH_DIR = _LLVM_ANDROID_PATH / 'patches'
-_PATCH_JSON = _PATCH_DIR / 'PATCHES.json'
+_PATCH_DIR = os.path.join(_LLVM_ANDROID_PATH, 'patches')
+_PATCH_JSON = os.path.join(_PATCH_DIR, 'PATCHES.json')
 
 _SVN_REVISION = (android_version.get_svn_revision_number())
 
 
 def get_removed_patches(output):
-    """
-    Parse the list of removed patches from patch_manager.py's output.
-
-    The output is of the form:
-    Removed <n> old patches:
-    - <patch_path1>: <patch_title1>
-    - <patch_path2>: <patch_title2>
-    ...
-    """
-
-    def _get_file_from_line(line):
-        # each line is '- <patch_path>: patch_title\n'
-        line = line[2:]
-        return line[:line.find(':')]
-
-    marker = ' old patches:\n'
+    """Parse the list of removed patches from patch_manager.py's output."""
+    marker = 'removed from the patch metadata file:\n'
     marker_start = output.find(marker)
     if marker_start == -1:
         return None
     removed = output[marker_start + len(marker):].splitlines()
-    rmfiles = [_PATCH_DIR / _get_file_from_line(p) for p in removed if p]
-    for rmfile in rmfiles:
-        if not rmfile.exists():
-            raise RuntimeError(f'Removed file {rmfile} doesn\'t exist')
-    return rmfiles
+    return [p.strip() for p in removed]
 
 
 def trim_patches_json():
@@ -78,6 +59,17 @@ def main():
               'Android LLVM version.')
         return
 
+    def _get_patch_path(patch):
+        # Find whether the basename printed by patch_manager.py is a cherry-pick
+        # (patch/cherry/<PATCH>) or a local patch (patch/<PATCH>).
+        cherry = os.path.join(_PATCH_DIR, 'cherry', patch)
+        local = os.path.join(_PATCH_DIR, patch)
+        if os.path.exists(cherry):
+            return cherry
+        elif os.path.exists(local):
+            return local
+        raise RuntimeError(f'Cannot find patch file {patch}')
+
     # Start a new repo branch before trimming patches.
     os.chdir(_LLVM_ANDROID_PATH)
     branch_name = f'trim-patches-before-{_SVN_REVISION}'
@@ -89,10 +81,12 @@ def main():
         print('No patches to remove')
         return
 
+    removed_patch_paths = [_get_patch_path(p) for p in removed_patches]
+
     # Apply the changes to git and commit.
     utils.check_call(['git', 'add', _PATCH_JSON])
-    for patch in removed_patches:
-        utils.check_call(['git', 'rm', str(patch)])
+    for patch in removed_patch_paths:
+        utils.check_call(['git', 'rm', patch])
 
     message_lines = [
         f'Remove patch entries older than {_SVN_REVISION}.',
diff --git a/update-clang-stable.py b/update-clang-stable.py
index 3072f57..21969d6 100755
--- a/update-clang-stable.py
+++ b/update-clang-stable.py
@@ -57,7 +57,6 @@ class ClangStableBuilder:
         self.stable_dir.mkdir()
         (self.stable_dir / 'bin').mkdir()
         (self.stable_dir / 'lib').mkdir()
-        (self.stable_dir / 'lib' / 'x86_64-unknown-linux-gnu').mkdir()
         (self.stable_dir / 'share').mkdir()
 
         self.copy_file(self.clang_dir / 'bin' / 'clang-format', self.stable_dir / 'bin')
@@ -65,8 +64,7 @@ class ClangStableBuilder:
 
         self.copy_files((self.clang_dir / 'lib').glob('libclang.*'), self.stable_dir / 'lib')
         self.copy_dir(self.clang_dir / 'lib' / 'python3', self.stable_dir / 'lib')
-        self.copy_files((self.clang_dir / 'lib' / 'x86_64-unknown-linux-gnu').glob('libc++*'),
-                        self.stable_dir / 'lib' / 'x86_64-unknown-linux-gnu')
+        self.copy_dir(self.clang_dir / 'lib' / 'x86_64-unknown-linux-gnu', self.stable_dir / 'lib')
         self.copy_dir(self.clang_dir / 'share' / 'clang', self.stable_dir / 'share')
 
         (self.stable_dir / 'README.md').write_text(
diff --git a/update-prebuilts.py b/update-prebuilts.py
index aa587b1..23918b3 100755
--- a/update-prebuilts.py
+++ b/update-prebuilts.py
@@ -19,12 +19,10 @@
 """Update the prebuilt clang from the build server."""
 
 import argparse
-import contextlib
-import glob
 import inspect
 import logging
 import os
-from pathlib import Path
+import pathlib
 import shutil
 import subprocess
 import sys
@@ -32,9 +30,6 @@ import utils
 
 import paths
 
-PGO_PROFILE_PATTERN = 'pgo-*.tar.xz'
-BOLT_PROFILE_PATTERN = 'bolt-*.tar.xz'
-
 def logger():
     """Returns the module level logger."""
     return logging.getLogger(__name__)
@@ -73,13 +68,6 @@ class ArgParser(argparse.ArgumentParser):
             default=False,
             help='Skip the cleanup, and leave intermediate files')
 
-        self.add_argument(
-            '--skip-update-profiles',
-            '-sp',
-            action='store_true',
-            default=False,
-            help='Skip updating PGO and BOLT profiles')
-
         self.add_argument(
             '--overwrite', action='store_true',
             help='Remove/overwrite any existing prebuilt directories.')
@@ -110,15 +98,19 @@ def fetch_artifact(branch, target, build, pattern):
     utils.check_call(cmd)
 
 
+def extract_package(package, install_dir, args=[]):
+    cmd = ['tar', 'xf', package, '-C', install_dir] + args
+    utils.check_call(cmd)
+
+
 def extract_clang_info(clang_dir):
     version_file_path = os.path.join(clang_dir, 'AndroidVersion.txt')
     with open(version_file_path) as version_file:
         # e.g. for contents: ['7.0.1', 'based on r326829']
         contents = [l.strip() for l in version_file.readlines()]
-        full_version = contents[0]
-        major_version = full_version.split('.')[0]
+        version = contents[0]
         revision = contents[1].split()[-1]
-        return full_version, major_version, revision
+        return version, revision
 
 
 def symlink_to_linux_resource_dir(install_dir):
@@ -147,26 +139,18 @@ def validity_check(host, install_dir, clang_version_major):
     # profiles.
     if host == 'linux-x86':
       realClangPath = os.path.join(install_dir, 'bin', 'clang-' + clang_version_major)
-      strings = utils.check_output([realClangPath, '--version'])
+      strings = utils.check_output(['strings', realClangPath])
       llvm_next = strings.find('ANDROID_LLVM_NEXT') != -1
 
-      if not llvm_next:
-        has_pgo = ('+pgo' in strings) and ('-pgo' not in strings)
-        if not has_pgo:
+      no_pgo_profile = strings.find('NO PGO PROFILE') != -1
+      if no_pgo_profile and not llvm_next:
           logger().error('The Clang binary is not built with PGO profiles.')
           return False
-        has_bolt = ('+bolt' in strings) and ('-bolt' not in strings)
-        if not has_bolt:
+
+      no_bolt_profile = strings.find('NO BOLT PROFILE') != -1
+      if no_bolt_profile and not llvm_next:
           logger().error('The Clang binary is not built with BOLT profiles.')
           return False
-        has_lto = ('+lto' in strings) and ('-lto' not in strings)
-        if not has_lto:
-          logger().error('The Clang binary is not built with LTO.')
-          return False
-        has_mlgo = ('+mlgo' in strings) and ('-mlgo' not in strings)
-        if not has_mlgo:
-          logger().error('The Clang binary is not built with MLGO support.')
-          return False
 
     # Check that all the files listed in remote_toolchain_inputs are valid
     if host == 'linux-x86':
@@ -207,19 +191,17 @@ def update_clang(host, build_number, use_current_branch, download_dir, bug,
         utils.check_call(
             ['repo', 'start', branch_name, '.'])
 
-    package = f'{download_dir}/clang-{build_number}-{host}.tar.xz'
+    package = f'{download_dir}/clang-{build_number}-{host}.tar.bz2'
 
     # Handle legacy versions of packages (like those from aosp/llvm-r365631).
     if not os.path.exists(package) and host == 'windows-x86':
-        package = f'{download_dir}/clang-{build_number}-windows-x86-64.tar.xz'
-
-    build_info_file = f'{download_dir}/BUILD_INFO-{host}'
+        package = f'{download_dir}/clang-{build_number}-windows-x86-64.tar.bz2'
     manifest_file = f'{download_dir}/{manifest}'
 
-    utils.extract_tarball(prebuilt_dir, package)
+    extract_package(package, prebuilt_dir)
 
     extract_subdir = 'clang-' + build_number
-    clang_version_full, clang_version_major, svn_revision = extract_clang_info(extract_subdir)
+    clang_version, svn_revision = extract_clang_info(extract_subdir)
 
     # Install into clang-<svn_revision>.  Suffixes ('a', 'b', 'c' etc.), if any,
     # are included in the svn_revision.
@@ -229,38 +211,25 @@ def update_clang(host, build_number, use_current_branch, download_dir, bug,
     # Linux prebuilts need to include a few libraries from the linux_musl artifacts
     if host == 'linux-x86':
         musl_install_subdir = install_subdir + '/musl'
-        musl_package = f'{download_dir}/clang-{build_number}-linux_musl-x86.tar.xz'
+        musl_package = f'{download_dir}/clang-{build_number}-linux_musl-x86.tar.bz2'
         if os.path.exists(extract_subdir):
             shutil.rmtree(extract_subdir)
-        utils.extract_tarball(prebuilt_dir, musl_package, [
+        extract_package(musl_package, prebuilt_dir, [
             "--wildcards",
             "*/lib/libclang.so*",
             "*/lib/*/libc++.so*",
-            "*/lib/libc_musl.so",
-            "*/lib/aarch64-unknown-linux-musl/libc++.a",
-            "*/lib/aarch64-unknown-linux-musl/libc++abi.a",
-            "*/lib/x86_64-unknown-linux-musl/libc++.a",
-            "*/lib/x86_64-unknown-linux-musl/libc++abi.a",
-            ])
+            "*/lib/libc_musl.so"])
         install_clang_directory(extract_subdir, musl_install_subdir, overwrite)
 
-        for triple in ('aarch64-unknown-linux-musl', 'x86_64-unknown-linux-musl'):
-            # Move archives.
-            src_dir = Path(musl_install_subdir) / 'lib' / triple
-            dest_dir = Path(install_subdir) / 'lib' / 'clang' / clang_version_major / 'lib' / triple
-            dest_dir.mkdir(exist_ok=True)  # The x86_64 triple will already exist.
-            for name in ('libc++.a', 'libc++abi.a'):
-                shutil.move(src_dir / name, dest_dir / name)
-
-        with open(paths.KLEAF_VERSIONS_BZL) as f:
-            kleaf_versions_lines = f.read().splitlines()
-        new_version_line = '    "{}",'.format(svn_revision)
-        list_end_idx = kleaf_versions_lines.index("]")
-        if new_version_line not in kleaf_versions_lines:
-            kleaf_versions_lines.insert(list_end_idx, new_version_line)
-        with open(paths.KLEAF_VERSIONS_BZL, "w") as f:
-            f.write("\n".join(kleaf_versions_lines))
-        utils.check_call(['git', 'add', paths.KLEAF_VERSIONS_BZL])
+        kleaf_parent = pathlib.Path("kleaf") / "parent"
+        os.makedirs(kleaf_parent, exist_ok=True)
+        try:
+            os.unlink(kleaf_parent / install_subdir)
+        except FileNotFoundError:
+            pass
+        os.symlink(
+            os.path.relpath(install_subdir, kleaf_parent),
+            kleaf_parent / install_subdir)
 
     # Some platform tests (e.g. system/bt/profile/sdp) build directly with
     # coverage instrumentation and rely on the driver to pick the correct
@@ -271,10 +240,9 @@ def update_clang(host, build_number, use_current_branch, download_dir, bug,
         symlink_to_linux_resource_dir(install_subdir)
 
     if do_validity_check:
-        if not validity_check(host, install_subdir, clang_version_major):
+        if not validity_check(host, install_subdir, clang_version.split('.')[0]):
             sys.exit(1)
 
-    shutil.copy(build_info_file, str(prebuilt_dir / install_subdir / 'BUILD_INFO'))
     shutil.copy(manifest_file, str(prebuilt_dir / install_subdir))
 
     utils.check_call(['git', 'add', install_subdir])
@@ -286,9 +254,9 @@ def update_clang(host, build_number, use_current_branch, download_dir, bug,
         return
 
     message_lines = [
-        f'Update prebuilt Clang to {svn_revision} ({clang_version_full}).',
+        f'Update prebuilt Clang to {svn_revision} ({clang_version}).',
         '',
-        f'clang {clang_version_full} (based on {svn_revision}) from build {build_number}.'
+        f'clang {clang_version} (based on {svn_revision}) from build {build_number}.'
     ]
     if is_testing:
         message_lines.append('Note: This prebuilt is from testing branch.')
@@ -313,30 +281,6 @@ def install_clang_directory(extract_subdir: str, install_subdir: str, overwrite:
     os.rename(extract_subdir, install_subdir)
 
 
-def update_profiles(download_dir, build_number, bug):
-    profiles_dir = paths.PREBUILTS_DIR / 'clang' / 'host' / 'linux-x86' / 'profiles'
-
-    with contextlib.chdir(profiles_dir):
-        # First, delete the old profiles.
-        for f in glob.glob(PGO_PROFILE_PATTERN):
-            os.remove(f)
-        for f in glob.glob(BOLT_PROFILE_PATTERN):
-            os.remove(f)
-
-        # Replace with the downloaded new profiles.
-        shutil.copy(glob.glob(f'{download_dir}/{PGO_PROFILE_PATTERN}')[0], '.')
-        shutil.copy(glob.glob(f'{download_dir}/{BOLT_PROFILE_PATTERN}')[0], '.')
-
-        utils.check_call(['git', 'add', '.'])
-        message_lines = [f'Check in profiles from build {build_number}']
-        if bug is not None:
-            message_lines.append('')
-            message_lines.append(f'Bug: {format_bug(bug)}')
-        message_lines.append('Test: N/A')
-        message = '\n'.join(message_lines)
-        utils.check_call(['git', 'commit', '-m', message])
-
-
 def main():
     args = ArgParser().parse_args()
     logging.basicConfig(level=logging.DEBUG)
@@ -362,14 +306,12 @@ def main():
     targets = [targets_map[h] for h  in hosts]
     if 'linux-x86' in hosts:
         targets.append('linux_musl')
-
-    build_info = 'BUILD_INFO'
-    clang_pattern = 'clang-*.tar.xz'
+    clang_pattern = 'clang-*.tar.bz2'
     manifest = f'manifest_{args.build}.xml'
 
     branch = args.branch
     if branch is None:
-        output = utils.check_output(['/google/bin/releases/android/ab/ab.par',
+        output = utils.check_output(['/google/data/ro/projects/android/ab',
                                      'get',
                                      '--raw', # prevent color text
                                      '--bid', args.build,
@@ -384,25 +326,14 @@ def main():
     try:
         if do_fetch:
             fetch_artifact(branch, targets[0], args.build, manifest)
-            for host in hosts:
-                target = targets_map[host]
-                fetch_artifact(branch, target, args.build, build_info)
-                os.rename(f'{download_dir}/{build_info}', f'{download_dir}/{build_info}-{host}')
             for target in targets:
                 fetch_artifact(branch, target, args.build, clang_pattern)
 
-            if not args.skip_update_profiles and 'linux-x86' in hosts:
-                fetch_artifact(branch, 'linux', args.build, PGO_PROFILE_PATTERN)
-                fetch_artifact(branch, 'linux', args.build, BOLT_PROFILE_PATTERN)
-
         for host in hosts:
             update_clang(host, args.build, args.use_current_branch,
                          download_dir, args.bug, manifest, args.overwrite,
                          not args.no_validity_check, is_testing)
 
-        if not args.skip_update_profiles and 'linux-x86' in hosts:
-            update_profiles(download_dir, args.build, args.bug)
-
         if args.repo_upload:
             topic = f'clang-prebuilt-{args.build}'
             if is_testing:
diff --git a/update-kernel-toolchain.py b/update_kernel_toolchain.py
similarity index 100%
rename from update-kernel-toolchain.py
rename to update_kernel_toolchain.py
diff --git a/utils.py b/utils.py
old mode 100644
new mode 100755
index c3dd0fb..bd6b4fa
--- a/utils.py
+++ b/utils.py
@@ -21,9 +21,7 @@ import logging
 import os
 from pathlib import Path
 import shlex
-import shutil
 import subprocess
-import sys
 from typing import Dict, List
 
 import constants
@@ -41,7 +39,7 @@ def subprocess_run(cmd, *args, **kwargs):
     """subprocess.run with logging."""
     logger().debug('subprocess.run:%s %s',
                   datetime.datetime.now().strftime("%H:%M:%S"),
-                  cmd if isinstance(cmd, str) else list2cmdline(cmd))
+                  list2cmdline(cmd))
     if kwargs.pop('dry_run', None):
         return None
     return subprocess.run(cmd, *args, **kwargs, text=True)
@@ -62,26 +60,6 @@ def check_output(cmd, *args, **kwargs):
     return subprocess_run(cmd, *args, **kwargs, check=True, stdout=subprocess.PIPE).stdout
 
 
-def create_tarball(source_dir, input, output):
-    xz_env = os.environ.copy()
-    xz_env["XZ_OPT"] = "-T0"
-    check_call([
-        'tar', '-cJC', str(source_dir),
-        '-f', str(output),
-        *map(str, input)
-    ], env=xz_env)
-    # print sha of tarball for debugging
-    check_call(['shasum', str(output)])
-
-
-def extract_tarball(output_dir, input, args=[]):
-    xz_env = os.environ.copy()
-    xz_env["XZ_OPT"] = "-T0"
-    # print sha of tarball for debugging
-    check_call(['shasum', str(input)])
-    check_call(['tar', '-xC', str(output_dir), '-f', str(input)] + args, env=xz_env)
-
-
 def is_available_mac_ver(ver: str) -> bool:
     """Returns whether a version string is equal to or under MAC_MIN_VERSION."""
     _parse_version = lambda ver: list(int(v) for v in ver.split('.'))
@@ -107,7 +85,7 @@ def create_script(script_path: Path, cmd: List[str], env: Dict[str, str]) -> Non
         for k, v in env.items():
             if v != ORIG_ENV.get(k):
                 outf.write(f'export {k}="{v}"\n')
-        outf.write(list2cmdline(cmd) + ' "$@"\n')
+        outf.write(list2cmdline(cmd) + ' $@\n')
     script_path.chmod(0o755)
 
 
@@ -150,59 +128,3 @@ def prebuilt_repo_upload(host: str, topic: str, hashtag: str, is_testing: bool):
         # -2 a testing prebuilt so we don't accidentally submit it.
         cmd.append('--label=Code-Review-2')
     check_output(cmd, cwd=prebuilt_dir)
-
-
-def clean_out_dir():
-    """Delete files from older build (paths.OUT_DIR) but retain paths.OUT_DIR /
-    prebuilt_cached, which is input for a chained build.
-    """
-
-    for child in paths.OUT_DIR.iterdir():
-        if child.name == 'prebuilt_cached':
-            continue
-        logger().info(f'removing {child} in {paths.OUT_DIR}')
-        if child.is_dir():
-            shutil.rmtree(child)
-        else:
-            child.unlink()
-
-
-def check_gsutil():
-    cmd = ['gsutil', 'version']
-    try:
-        subprocess.Popen(
-            cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
-        )
-        return True
-    except FileNotFoundError:
-        return False
-
-
-def check_stubby():
-    cmd = ['stubby', '--version']
-    try:
-        subprocess.Popen(
-            cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
-        )
-        return True
-    except FileNotFoundError:
-        return False
-
-
-def check_tools(use_sha: bool):
-    if not check_gsutil():
-        print(
-            'Fatal: gsutil not installed! Please go to'
-            ' https://cloud.google.com/storage/docs/gsutil_install to install'
-            ' gsutil',
-            file=sys.stderr,
-        )
-        sys.exit(1)
-
-    if use_sha and not check_stubby():
-        print(
-            'Fatal: stubby not found. This is only available on gLinux'
-            ' (Googlers only). Use --build_id instead',
-            file=sys.stderr,
-        )
-        sys.exit(1)
diff --git a/version.py b/version.py
old mode 100644
new mode 100755
diff --git a/win_sdk.py b/win_sdk.py
old mode 100644
new mode 100755
-- 
2.25.1

